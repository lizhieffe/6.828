{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJNj4Wd5ynuCT2k3gWi/7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5d2542da8ae46f1ad39e6fbc69fdd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97219d6fa5ed4a658f16b680e5224d83",
              "IPY_MODEL_a6514b188a424f73803677254f1ab983",
              "IPY_MODEL_79466e6201014fd8a4cfad76a415ce39"
            ],
            "layout": "IPY_MODEL_8634a384088a4f3491c8fd78a6f4b48a"
          }
        },
        "97219d6fa5ed4a658f16b680e5224d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad58170a56940469916f9f1de90a031",
            "placeholder": "​",
            "style": "IPY_MODEL_9a4c98f31eee4c63b74d7fb38051576d",
            "value": "tokenizing the splits (num_proc=24): 100%"
          }
        },
        "a6514b188a424f73803677254f1ab983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b08be4f1ff4585802d85543ed2d36f",
            "max": 7212392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccce48e38dfc4fa48ff85b572df237df",
            "value": 7212392
          }
        },
        "79466e6201014fd8a4cfad76a415ce39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9b40c5a3e504fed80388dc783ee91c1",
            "placeholder": "​",
            "style": "IPY_MODEL_6493dafee2274868b603cd801801248e",
            "value": " 7212392/7212392 [14:10&lt;00:00, 637.84 examples/s]"
          }
        },
        "8634a384088a4f3491c8fd78a6f4b48a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad58170a56940469916f9f1de90a031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a4c98f31eee4c63b74d7fb38051576d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20b08be4f1ff4585802d85543ed2d36f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccce48e38dfc4fa48ff85b572df237df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9b40c5a3e504fed80388dc783ee91c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6493dafee2274868b603cd801801248e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72ea15479a514c9b8421b5b433ac6014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a19a8095f9d241b88443cc1fc0e2bff6",
              "IPY_MODEL_419aed6914fa4310b4c494daeb9ace9c",
              "IPY_MODEL_4947e5bd914e4a8599497b91de610d96"
            ],
            "layout": "IPY_MODEL_c042d2954a59453592e5d9ac165bceef"
          }
        },
        "a19a8095f9d241b88443cc1fc0e2bff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034d40327bd74719ae669a628c630c92",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e7fc3c47444f8393e6c13bb0e08a96",
            "value": "tokenizing the splits (num_proc=24): 100%"
          }
        },
        "419aed6914fa4310b4c494daeb9ace9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b96a911c7384c1191a45a786457bf6c",
            "max": 801377,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebc752460c2146ddbd8e5913ad5ec782",
            "value": 801377
          }
        },
        "4947e5bd914e4a8599497b91de610d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eada107d1fbf4bc0ac2e3f6a280a01b7",
            "placeholder": "​",
            "style": "IPY_MODEL_aadce6e45f254597b0a7ecdc3f63f89d",
            "value": " 801377/801377 [01:41&lt;00:00, 604.49 examples/s]"
          }
        },
        "c042d2954a59453592e5d9ac165bceef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034d40327bd74719ae669a628c630c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e7fc3c47444f8393e6c13bb0e08a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b96a911c7384c1191a45a786457bf6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc752460c2146ddbd8e5913ad5ec782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eada107d1fbf4bc0ac2e3f6a280a01b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aadce6e45f254597b0a7ecdc3f63f89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/6.828/blob/master/Shakespeare_LM_v6_v5%2BOpenWebText_Dataset%2BTokenizer_BPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRMxsDXmpbMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "BLOCK_SIZE = 96 # Context length: how many chars do we take to predict the next one?\n",
        "\n",
        "# number of workers in .map() call\n",
        "# good number to use is ~order number of cpu cores // 2\n",
        "NUM_PROC = 24"
      ],
      "metadata": {
        "id": "vDHUDjtsph4G"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup GPU"
      ],
      "metadata": {
        "id": "NZIvnPdcps3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GPU:\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  assert device.type != 'cpu', \"GPU is not available\"\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd95wthQptwk",
        "outputId": "a7471c64-5876-413a-b216-27362f9e3397"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_cpu = torch.Generator(device='cpu').manual_seed(2147483647) # for reproducibility\n",
        "g_device = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "FDyu_mmdxQNC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "hHD6RNbQpv4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "9hrzlbSZpf9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEm7qKNcphMJ",
        "outputId": "2ab4e552-6bca-4440-d8da-e28f5a4ddbe6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = enc.encode(\"how are you little yo I don't know \\n man\")\n",
        "\n",
        "back = enc.decode(test + [enc.eot_token])\n",
        "\n",
        "\n",
        "test, back"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpimbYK9wgF0",
        "outputId": "8a1793f3-d4e8-467f-89c3-7f12f6ab4e99"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4919, 389, 345, 1310, 27406, 314, 836, 470, 760, 220, 198, 582],\n",
              " \"how are you little yo I don't know \\n man<|endoftext|>\")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = enc.n_vocab\n",
        "print(f'{vocab_size=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIrD8sXNxeII",
        "outputId": "c2a95813-f00e-489f-90f4-f5ac5de8e310"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size=50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "CXSgUDhjdCxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open Web Text DS\n",
        "\n",
        "- See nanoGPT impl: https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py"
      ],
      "metadata": {
        "id": "Ywme562nmnL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data - openwebtext\n",
        "\n",
        "!pip install datasets # Since we are running in colab docker image, install it here.\n",
        "\n",
        "from datasets import load_dataset # huggingface datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtR7MTsV6SpB",
        "outputId": "05ead1ea-fc1a-42d3-836d-9172b26e6eed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"openwebtext\", num_proc=NUM_PROC)"
      ],
      "metadata": {
        "id": "EBWxTlER6jbT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By default it only contains the 'train' split, so create a test split\n",
        "\n",
        "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
        "split_dataset['dev'] = split_dataset.pop('test')  # rename the test split to val"
      ],
      "metadata": {
        "id": "3gnn63lrmHDf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "id": "DFVha5xrnfHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfb29a5-b0e5-4fc2-bec6-0f842c1694eb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 7212392\n",
              "    })\n",
              "    dev: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 801377\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "\n",
        "for it in split_dataset.get('train'):\n",
        "  print(it)\n",
        "  if i > 3:\n",
        "    break\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mvNz-RCp-nC",
        "outputId": "f56ddda4-5121-4e1e-c26f-64195efc931a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Expecting DOT Street Safety Projects to Deliver More Than the Minimum\\n\\nA DOT plan to add painted bike lanes and sharrows to Spring Street [PDF] doesn’t go far enough to prioritize walking and biking, says Community Board 2 transportation vice-chair Maury Schott.\\n\\nLast Thursday, DOT presented the proposal to the CB 2 transportation committee. Two-thirds of the audience supported the plan, meeting attendees said, and neighborhood NIMBY ringleader Sean Sweeney was a no-show. In the end, the plan received a unanimous 10-0 vote.\\n\\nThe lack of opposition, however, may be a sign of DOT timidity more than anything else. “The proposal by DOT was, to say the least, minimally intrusive,” Schott told Streetsblog. “It was as much as you could hope to do without making the commitment to remove parking on at least one side of the street.”\\n\\nAlthough DOT has been on a roll this year with proposals for road diets and protected bike lanes, the agency’s designs usually don’t subtract much parking. Avoiding the removal of car storage may head off small-scale political fights, but it also limits the impact of the city’s street safety projects.\\n\\nSchott said he wants to see DOT prioritize safe walking and biking over on-street parking, rather than the other way around. In Lower Manhattan, where about 80 percent of households are car-free, the politics should be especially favorable for major changes. Many people at last week’s meeting, Schott said, were also frustrated by “half-measures” from the agency.\\n\\n“So far, many people feel that Vision Zero is a lot more talk than it is action,” Schott said. “The whole rhetoric of Vision Zero is that New York is a pedestrian-friendly or a pedestrian-dominated city. If you want to say that, then the first thing you have to realistically do is say that supporting the private ownership of private automobiles should not be a priority in any way.”\\n\\nEven if some people must own cars to, for example, reach jobs in the suburbs every day, Schott said, most Manhattanites can make do with car-share or car rental. And people who come to do business in Manhattan — if they drive at all — don’t usually expect parking right in front of their destination.\\n\\n“I guarantee you there are tens of thousands of privately owned automobiles that do little more than sit on the street and get moved for street cleaning,” Schott said. “You can’t do both things. You can’t have a pedestrian-friendly city and subsidize the ownership of private automobiles for a substantial amount of the population.”\\n\\nWhile Schott might feel more strongly about the issue than some of his fellow board members, he knows he isn’t alone. “There are a lot of people who feel the same way as I do on the board but aren’t willing to listen to the griping of people who have convinced themselves that they have to have a car,” he said. That’s how the gripers can set the agenda for the city’s street safety plans, even in an area like Community District 2 where the vast majority of residents don’t own cars.\\n\\n“We’re dealing with a city that for 70 years has been built and managed for cars,” Schott said. “You can’t just make little changes around the edges and expect any substantial improvement in the problems that has created.”\\n\\nThe plan to paint bike lanes and sharrows on Spring Street will come up for a vote at the next CB 2 full board meeting, scheduled for April 23 at 6:30 pm in the auditorium of the Scholastic Building, 557 Broadway.'}\n",
            "{'text': \"Malawi Goodwill Ambassador for Child Welfare, Madonna, has raised $5.5 million (MK2.4 billion) to support people displaced by the devastating floods which continue to wreck havoc across the country.\\n\\nThe US pop mogul, joined by rapper Gucci Mane, raised the funds from ticket sales and live auction during an event dubbed A Night to Benefit Malawi and UNICEF held in New York.\\n\\nAccording to a statement from Madonna's Raising Malawi, the proceeds will be split equally between UNICEF and Madonna's organisation Raising Malawi.\\n\\nMadonna, who is acknowledged by the Guinness World Records as the best-selling female recording artist and the fourth best-selling act of all time behind The Beatles, Elvis Presley, and Michael Jackson, was Madonna, was appointed the Goodwill Ambassador for Child Welfare by President Peter Mutharika in December last year.\\n\\nThe superstar came under heavy fire from former Malawi president Joyce Banda after scrapping plan for girls' academy. Madonna who holds the record for the most number-ones on all Billboard charts with a total of 156 number-ones opted instead for 10 classroom blocks with one housing about 50 students.\"}\n",
            "{'text': 'News\\n\\nIn America debutta la pasta (Barilla) che si cuoce senza bollire l’acqua\\n\\n08 aprile 2015\\n\\ndi Elmar Burchia\\n\\nSiete pigri? Non siete bravi a cucinare? Non sapete nemmeno far bollire l’acqua? Ecco la pasta che fa per voi. Barilla ha recentemente lanciato «Pronto», la pasta «facile da preparare» che si cuoce senza far bollire l’acqua. Per risparmiare acqua, gas ma soprattutto tempo. Tuttavia, non la troveremo sugli scaffali dei supermercati in Italia. Gli spaghetti, le penne, le linguine, i fusilli «Pronto» sono stati pensati esclusivamente per il mercato americano, a quanto pare insofferente ai tempi troppo lunghi (davvero?) che occorrono per preparare un piatto di pasta. Cucinarli è semplicissimo: si butta la pasta in una padella o in una pentola, si versa un po’ d’acqua fredda sopra e la si lascia cuocere a fuoco alto per una decina di minuti. Il tutto, senza la noia di dover aspettare che l’acqua bolla. Inoltre, non dovrete nemmeno scolarla. La pasta assorbe tutta l’acqua, come per il riso. Alla fine, la si condisce a piacimento. «Al dente - Perfezione in pochi minuti», si legge sulle confezioni della linea «Pronto». Tuttavia, notano alcuni blog culinari americani (che non sembrano molto entusiasti del prodotto), il risultato di questa nuova «tecnica» è una pasta piuttosto scotta. «La scienza ha finalmente reso più semplice cucinare la pasta», scrive - con un tono d’ironia - The Daily Meal. «È un buon inizio, Barilla, ma avvisaci quando inizi a metterci i gamberetti essiccati».\\n\\n© RIPRODUZIONE RISERVATA'}\n",
            "{'text': \"They’re easy to spot, the neo-Nazi and his driver, strolling side by side up Main Street beneath a noonday sun, flanked by the two-story brick and limestone buildings of Beatrice, Nebraska. But it’s not the way they’re dressed.\\n\\nGerhard Lauck, the man they call the “Farm Belt führer” doesn’t draw attention to himself heedlessly, doesn’t swaddle himself in swastikas – not any more, anyway, and not out here in south-east Nebraska, where he finally retreated after serving a four-year stint in a German prison for distributing neo-Nazi propaganda. He doesn’t bark “Heil Hitler!” He doesn’t shoot his right arm into the air.\\n\\nNo, it’s not the way they’re dressed, though Lauck is wearing a military brown shirt, and his driver, who won’t identify himself, is over-buttoned for a warm spring day. It’s something about their posture, a self-seriousness reflected in their stride, as if they’re not conducting business so much as playing at it.\\n\\nIf you're talking about US Nazis that have had an impact in the last 50 years, Gerhard is probably number one Bob Wolfson, Anti-Defamation League\\n\\nAt 6ft 4in and roughly 240lb, Lauck towers above his driver. Were it not for his illicit activity abroad, his tireless promotion of “racial purity”, his worship of Adolf Hitler (whom he likes to call “too humane” just to revel in the outrage), the pair would seem almost comical, a Laurel and Hardy, or closer still, a Pinky and the Brain.\\n\\n“A joke? He was never a joke,” said Bob Wolfson, former director of the Anti-Defamation League’s Plains States Region, when I questioned Lauck’s significance. “If you’re talking about American Nazis that have had an impact internationally in the last 50 years, Gerhard is probably number one.”\\n\\nFor Wolfson, Lauck’s trajectory is a valuable case study, a narrative worth repeating to recognize the signs of a budding hatemonger.\\n\\nAccording to the Southern Poverty Law Center, “the radical right was more successful in entering the political mainstream last year (2016) than in half a century”. This extremism has reared its head in forms both violent and purely ideological, from the exploding popularity of websites like the Daily Stormer, founded by an avowed Hitlerite, to the rash of hate crimes recorded immediately following Donald Trump’s election.\\n\\nLauck fits squarely into a succession of far right ideologues pushing an anti-globalist agenda. But unlike so many shouting their hate today, he has been peddling his particular brand in Europe for decades: before, during and after the internet disrupted traditional propaganda channels.\\n\\n“His methodology and effect have been changed by technology,” Wolfson says, “but the fact that he’s still running a hosting site, that he hasn’t lost his ability to connect all these groups, that he has very good relationships with thousands and thousands of neo-Nazis all over the world – that’s not insignificant.”\\n\\nI watch from across the street as they enter the restaurant. Above them, a crew of Hispanic workers dangle their legs from the scaffolding as they re-mortar the exterior. He’s not concerned. It’s not the individuals that bother him – it’s the groups, slowly “mongrelizing” the planet, he believes, playing the long con of “white genocide”.\\n\\nIs there a neo-Nazi storm brewing in Trump country? Read more\\n\\nThe room is banquet sized and bare. The lights are dim, and the driver, still in his boxy jacket, stands beside our table. He doesn’t greet me when I step inside. I ask if he’s with Gerhard. He nods toward the restroom.\\n\\nSeconds later, Lauck barrels out, belly forward, thumbs tucked in his waistband, mustache thick, hair peppered and neatly trimmed.\\n\\nThe world’s top supplier of printed neo-Nazi propaganda is ready for his first mainstream media interview in years.\\n\\n“You do a great public service by saying these folks are around,” says Wolfson, who spent years of his career tracking Lauck’s work. “It’s healthy for people to get close to darkness so they can triangulate what darkness looks like.”\\n\\nI first contacted Lauck last February, after studying the Southern Poverty Law Center’s newly published “Hate Map”.\\n\\nOf the 917 active hate groups shown nationwide, 99 were Nazi groups, and five fell within the borders of my home state of Nebraska. Two of them in the neo-Nazi category: the NDSAP/AO, the German acronym for the National Socialist German Workers Party/Overseas Organization and Third Reich Books, both spearheaded by Gary “Gerhard” Lauck. The former was listed in the capital city of Lincoln, the latter in Fairbury, population 3,800.\\n\\nAfter finding the website for Third Reich Books, I quickly emailed the only address listed, astonished to find someone with Lauck’s history living quietly among us. I didn’t think he would write back, but just 20 minutes later, his response was sitting in my mailbox.\\n\\n“I’m semi-retired,” he wrote, “and routinely decline interviews from the local and state media.” If I were writing for a national or international publication, however, he’d be willing to make an exception. Though corresponding with a neo-Nazi fried my nerves – the very thought of his name in my inbox keeping me up at night – we stayed in touch for several months until we finally locked in a time and place to meet.\\n\\nIn person, Lauck is nothing if not self-aware.\\n\\nHe jokes about his “excessive modesty” as often as he calls himself a “sex symbol”, which is to say: frequently. He is fine with being called a Nazi propagandist. He doesn’t argue otherwise, though he insists he fabricates “a lot less than most propagandists” and certainly less than “the mass media”. The more appropriate question, he later tells me, isn’t whether or not what he says is true, but how much he’s omitted. He says these things frequently, purposely throwing himself in doubt, relishing in the grey zone between fact and fiction.\\n\\nHis fluency in German has led many in the past to confuse a very real speech impediment with an affected German accent. He struggles with the “r” sound, substituting a “w” or deleting it entirely. And he often crutches on the phrase “type of thing”, even when the phrase doesn’t apply: “I’m not say’n we should, type of thing, but the conkest of living space to poetect yuh own wace, yuh own culcha is OK.”\\n\\nBorn in Milwaukee in 1953, Lauck grew up in a bubble of pro-German sentiment. Once nicknamed the “German Athens of America”, by the 1930s Milwaukee hosted the largest German-born population in the country outside of Chicago and New York City. But a host of factors shattered the myth of a monolithic German American community, not the least of which was the political approach to preserving its heritage in the wake of the first world war – an era of fierce anti-German sentiment – and the looming shadow of the Third Reich.\\n\\nLauck’s parents, both from Wisconsin, grew up in this polarizing atmosphere, though he claims they were non-political. “Of course, there were comments about the United States fighting on the wrong side,” he says. “But, after all, that’s pretty obvious.”\\n\\nCocooned by his heritage, Lauck developed an acute sense of family and ethnic identity – something he’d fetishize for the rest of his life.\\n\\n“I’ve been an American for over 60 years,” he says. “I’ve been a German for over 4,000.”\\n\\nI’ve been an American for over 60 years. I’ve been a German for over 4,000 Gerhard Lauck\\n\\nWhen Lauck was 11, his father Francis, formerly an engineer with the AO Smith Corporation, accepted a professorship at the University of Nebraska, transplanting the family to Lincoln. They moved into a modest home on a quiet, tree-lined street on the eastern edge of the city: middle class, upwardly mobile and overwhelmingly white.\\n\\nThough Nebraska, too, boasted significant German heritage, the Germans here were more Americanized, Lauck says. Few of them retained the language, but more than that, they’d lost what he now viewed as the hallmarks of German culture.\\n\\nThis sudden departure from his nationalistic upbringing in Milwaukee, combined with a burgeoning American counterculture, pushed him over the edge. He considered Lincoln “a spiritually foreign country”.\\n\\nIs there a neo-Nazi storm brewing in Trump country? Read more\\n\\nHe withdrew into books and filled his room with German war regalia. When he finally asked for Mein Kampf, at just 14, it didn’t seem that peculiar for the same precocious kid who’d asked for The Communist Manifesto just the week before. He hated the former, of course. But Mein Kampf “made perfect sense”, crystallizing the pro-German values he was reared on, and ushering forth the “master race” myth he still preaches today.\\n\\nHe graduated from Lincoln East high school in just three years, sliding further into the national socialist philosophy, submitting essays to Nazi publications and converting both his father and oldest brother Robert to the party.\\n\\nIn an act of “ethnic consciousness”, he changed his name from Gary to “Gerhard”, and neighbors say he’d often correct them on it. “If Cassius Clay can be Muhammad Ali,” he says, “then Gary Lauck can be Gerhard Lauck.”\\n\\nNeighbors say his mother, Laura, was “the sanest one over there”. They remember her with sadness, pointing out she gave everything to that family and seemed to get so little in return. They say Lauck’s sister Janice struggled for years with what the family thought was a mental disorder, only to find out she had a brain tumor.\\n\\nWhat the neighbors didn’t know – what they couldn’t – was that the youngest son of this peculiar Wisconsin family was soon to establish one of the most important and far-reaching neo-Nazi organizations in history, and that he’d do it all in plain sight, right there in his parents’ basement.\\n\\nAfter two years at university, Lauck dropped out to focus his efforts abroad. He first connected with a fledgling cell of neo-Nazis in New York before flying to “the fatherland” in 1972, where he found swastikas and slogans written in graffiti, though no signs of a unified movement. He was arrested for distributing Nazi literature, specifically outlawed by Germany’s criminal code as part of the denazification program following the second world war, but it gave him an idea.\\n\\nHe describes it in The Education of an Evil Genius, his self-published autobiography:\\n\\n“An overseas organization based in a free country would supply the underground resistance with professionally produced printed matter. It would have a uniform contact address in the free country. Inquirers would receive free sample literature and their own unique ‘ID number’ for use in future correspondence instead of their real name and address. This protected their identity in the event of a later interception of the mail.”\\n\\nThus the National Socialist German Workers Party/Overseas Organization, or NDSAP/AO, was born.\\n\\nThe idea couldn’t have been simpler, but in the pre-internet age, never before had the neo-Nazi resistance in Europe been so connected. Dissident cells who previously knew nothing of one another were now in communication, and Gerhard Lauck, barely into his 20s, once a bookish teenage pariah, served as the conduit from thousands of miles away.\\n\\nFacebook Twitter Pinterest Confiscated material from a far right extremist group on display at a press conference held by Brandenburg’s Ministry of the Interior in 2011. Photograph: Alamy\\n\\nIt started with a run of just 1,000 swastika stickers mailed to Germany. A year later, Lauck writes in his autobiography, the average run was 100,000. Soon, the AO published a German-language newspaper, the NS Kampfruf, and later still, an English version called The National Socialist Report.\\n\\nDuring the next two decades, between 1975 and 1995, he moved around between Lincoln, Chicago and New York, but he always used the same return address on the propaganda. The newspapers expanded to nearly a dozen languages and 30 countries. Bolstered by its own reflection – “We realized we were not just a bunch of old people ready to die,” Lauck says – the resistance movement in Europe, and especially in Germany, exploded.\\n\\nThough numbers varied widely at the time, the Anti-Defamation League estimated in 1993 that nearly 60,000 Germans were involved in neo-Nazi activity. The German government itself guessed that while only 2,000 Germans fit a strict Nazi mold, nearly 43,000 rightwing extremists were active in various hate groups, and as many as 6,400 had been militarized. Either way, all of the estimates had multiplied significantly since Lauck’s arrival in the 1970s.\\n\\nIn November 1976, celebrating the 38th anniversary of Kristallnacht, neo-Nazis in Frankfurt plastered the city with AO posters that read: “We are here again. Red Front perish. Don’t buy Jewish.”\\n\\nWhen a rash of antisemitic crimes broke out in Hanover in February 1978, German sources alleged the demonstrators were financed by the AO through a Swiss bank account.\\n\\nAnd when a 26-year-old construction worker shot and killed himself and three foreigners with a pistol at a nightclub in Nuremberg in August 1982, police found his pockets stuffed with AO stickers.\\n\\nIn 1992, Lauck’s propaganda was found at the crime scene of more than 200 criminal investigations, nearly all of them triggered by violent activities. And in the early days of the Yugoslav wars, Lauck used the AO newspapers to recruit and solicit money for a unit of more than 100 neo-Nazi militants of various nationalities to fight for Croatia, a Nazi ally in the second world war.\\n\\nLauck’s activities were heavily monitored – by the FBI, by the CIA, by the Anti-Defamation League. And yet, Lauck remained virtually untouchable on American soil. Despite his disdain for democracy, he’d armored himself with the First Amendment.\\n\\nFacebook Twitter Pinterest A visitor looks at photographs of SS troops and Nazi propaganda at the Topography of Terror museum in Berlin. Photograph: John Macdougall/AFP/Getty Images\\n\\nThere were hiccups. Caught in March 1976 with 20,000 stickers, a fake passport and a large sum of money, Lauck spent four and a half months in German prison, or as he calls it, a “state-run luxury hotel”. And he once received a cigar-box sized black powder bomb in the mail, powerful enough to kill anyone within a five- to 10ft radius. That could have been his mother, father or his brother Robert, all of whom frequently picked up his mail. Or it could have been his Lithuanian wife, a sympathizer he met in Chicago named Janina Bareisa.\\n\\nHis brother Jerry, however, who had publicly disavowed his brother’s ideology, never picked up the mail.\\n\\nLauck says politics had nothing to do with it, but on 7 February , 1978, he loaded a 12-gauge shotgun, aimed it at Jerry and pulled the trigger, wounding him. At the hearing, Jerry – who hadn’t stepped foot in the home for more than two years – testified that he’d stopped by the home to drop off a package for his ailing twin sister, but his father, weak from lung cancer, rebuffed him. Lauck, who kept a shotgun nearby in case of political blowback, heard the commotion and ran upstairs. When Jerry slapped his father, Lauck took aim and fired. The charges, which carried a penalty of up to 50 years, were dropped after Jerry refused to come forward and the time for a speedy trial elapsed.\\n\\nNone of it slowed Lauck down. Not the deportations. Not the bomb threats. Not the family drama. The mail kept coming. The mail kept going. And by the early 1990s, Lauck was considered “the biggest supplier of neo-Nazi materials in the German scene”, according to Bodo Becker, a spokesman for Germany’s Office for the Protection of the Constitution, established specifically to monitor neo-Nazi activity.\\n\\n“That’s what I do,” Lauck says. “It’s a really good job.”\\n\\nHis sins would catch up with him. In December 1993, after years of frustration with German law enforcement and powerless to prosecute the man perhaps most responsible for their country’s surge in rightwing extremism, then-FBI director Louis Freeh opened an investigation focused on Americans facilitating German neo-Nazis.\\n\\nThough he didn’t identify individuals, those who followed the case knew exactly where he’d start.\\n\\nTwo years later, in March 1995, Lauck was arrested in Denmark on warrants issued from Hamburg via Interpol charging him with 36 separate counts of “distributing propaganda against the German constitution, encouraging racial hatred, inciting criminal acts and participating in a criminal organization.” He was soon extradited to Germany, which had been surveilling his activities and wiretapping his German cohorts. What role, if any, the FBI played in Lauck’s seizure remains unclear, though both Lauck himself and other sources close to the case who asked not to be identified claim US officials were involved.\\n\\nEither way, Lauck was sentenced to four years in German prison. He maintains he knew the arrest was coming, citing the continued publication of the NS Kampfruf after his arrest as proof of the AO’s foresight.\\n\\nWhat he wasn’t prepared for, however, was the internet.\\n\\nFacebook Twitter Pinterest Gary Lauck leaves the Copenhagen supreme court in August 1995. Photograph: Bjarke Oersted/AP\\n\\nBy the time he finished his sentence and returned to the US – Chicago first, and then back to his mother’s basement with his wife in tow – the internet had undercut the importance of his work. Dissident neo-Nazi cells all over the world could now easily connect online, and the print industry had lost nearly all of its currency.\\n\\nHe entered prison as a king among his people, Wolfson says. He left as something less.\\n\\nNeighbors in Lincoln say he stayed quiet after his return. Often times, one neighbor told me, the only way you could tell he was home at all was the strange smell of cigarettes wafting from the backyard.\\n\\nBut Lauck wasn’t completely out of the game. In 2000, just a year after his return, he launched a web-hosting platform called Zensurfrei.com, meaning “censorship free”.\\n\\nAimed primarily at European hate groups, the website boasts: “We … believe there is no such thing as ‘hate speech,’ only free speech that is hated by the established authorities. Zensurfrei.com is the first fully managed and secure web hosting provider for those denied access to mainstream hosting due to ‘offensive’ content.”\\n\\nIn addition to its still expanding web-hosting platform, the AO continues to publish hard-copy translations of original SS literature through its online bookstore, Third Reich Books. In total, they publish over 400 titles in 13 languages, Lauck says, and recently purchased their own binding equipment, moving the entire book-building process in-house.\\n\\n“If it was up to me I’d move back to Germany to be on the front lines,” he says. “But it’s not practical … I can do more good here.”\\n\\nAfter our interview, the photographer and I waited for Lauck and his driver to pull away. I had no interest in following them – not in a clandestine sense, anyway – but I wondered how neighbors felt about an internationally notorious Nazi propagandist camping out quietly among them, so we drove the 30 minutes west, slicing through budding cornfields on Route 136.\\n\\nNevertheless, I hadn’t planned on bumping into Lauck again just five minutes after parking the car.\\n\\nHe quickly appointed himself my liaison, ushering me into various businesses around town. He asked if I’d like to see his hiking path. We left the square and walked toward the Little Blue river, where the city converted an old rail line for recreational use.\\n\\nI questioned him throughout the tour about White House chief strategist Steve Bannon, who was once called a “white supremacist” by Nancy Pelosi. He considers him “watered down” compared to the “Hitler youth and Wehrmacht” he once knew in Germany; about Donald Trump, whom he calls a transitional figure on the path toward revolution, comparable to “Kerensky in Russia”, admitting “the fact that he could be elected on the message he presented … is extremely significant”.\\n\\nI also asked him about the sharp rise in antisemitic crimes in 2017. “The Jews claim that, but a lot of times Jews commit the crimes themselves and fake it to get sympathy,” he says. “You’ll know they’re really making progress if you watch the stock market for a company that manufactures a lot of Zyklon B.”\\n\\nLauck purchased a house some time around 2008. By then, his father had long been dead, and Lauck had let his parents’ home fall into disrepair, termites eating the floors and even the base of his mother’s piano.\\n\\nNeighbors say he rarely took his mother out of the house. He eventually moved her to a nursing home outside of Lincoln, hastily packing up their belongings and what remained of his operations in the basement. A short string of new owners moved in and out. Nobody stuck around.\\n\\nAs long as he stays on his side of the fence I won't have to put a barrel in his face David George, Lauck's neighbor\\n\\nMost residents here in Fairbury don’t recognize his name. Those who do raise their eyebrows, whisper about his activities, laugh them away as if it were a quirk of this avid walker’s past. They don’t condone Nazism, certainly, but they won’t run him off. They respect his privacy, and expect he’ll reciprocate. “As long as he stays outta my yard I don’t care what he does,” says David George, who owns Pla Mor Lanes and Café. “As long as he stays on his side of the fence I won’t have to put a barrel in his face.”\\n\\nLauck asks that I keep Fairbury out of the story, telling me a cautionary tale of blackmailing a journalist who spied on his mother. He tells me it isn’t a threat.\\n\\n“If I wanted to threaten you I’d have just taken you out further into the woods and pulled out my Luger and pointed it at you.”\\n\\nIn 2008, a Baghdadi construction worker named Rafid Al Nada, fearing for his young family’s safety in a radicalized Iraq, began trudging his way through the application process for an American visa.\\n\\nAfter five interviews, they landed in Lincoln, Nebraska. Al Nada found employment with a local construction company, where he could put his considerable skills to work. At first they lived with his cousin, then an apartment of their own, and then a small house. In 2015, they upgraded again, this time into a modest home on a quiet, tree-lined street on the eastern side of the city: middle class, upwardly mobile and overwhelmingly white.\\n\\nThe yard was a mess and the floors were completely rotted through. Luckily for them, they knew a guy who could fix it up cheap, a skilled handyman with decades of experience behind him.\\n\\nThey’re still working on the yard, but the interior today is in impeccable condition. New floors. New paint. Framed photos of family members hanging from the walls, not a single one atilt. He built a new front porch. He paved a new sidewalk to the front door. He hasn’t finished the basement yet, but it’s on the list. When they first moved in, he says, he found something strange down there.\\n\\n“He found a book talking about Hitler and things,” his son Mustafa says in perfect English, translating for his father.\\n\\nI ask if I can see it. Al Nada laughs and gestures towards the trash can.\\n\\n“I’m sorry, my friend. I tossed it.”\"}\n",
            "{'text': 'LAND O\\' LAKES -- A robbery suspect was arrested after he posted comments under his own \"wanted\" photo on the Pasco County Sheriff\\'s Office Facebook page.\\n\\nMatthew Oliver, 23, was arrested Friday on an active warrant for a robbery.\\n\\nOliver\\'s photo was posted Wednesday to the PCSO Facebook page as part of agency\\'s \"Fugitive of the Day\" program. A short while later, Oliver began posting comments under his photo, deputies said.\\n\\nOn Friday, the PCSO Fugitive Team posted the following comment: \"Members of the Fugitive Warrants Unit observed Matt Oliver as he walked into an apartment wearing a T-shirt. Moments later he came back outside where it is 80 degrees, wearing a camouflague jacket with the hood up. He was apprehended without any resistance.\"\\n\\nSign Up and Save Get six months of free digital access to the Bradenton Herald\\n\\nOfficials said that while they have been able to make arrests through the Fugitive of the Day program, this is the first time the suspect has actively and publicly engaged with the agency through the Facebook page.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(example):\n",
        "  # ignores any special tokens\n",
        "  ids = enc.encode_ordinary(example['text'])\n",
        "\n",
        "  # Add the end of text token, e.g. 50256 for gpt2 bpe\n",
        "  ids.append(enc.eot_token)\n",
        "  out = {'ids': ids, 'len': len(ids)}\n",
        "  return out"
      ],
      "metadata": {
        "id": "XqGVvpBSpR1-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "\n",
        "for it in split_dataset.get('train'):\n",
        "  print(process(it))\n",
        "  if i > 3:\n",
        "    break\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xzH6MO1qvyu",
        "outputId": "be5dc7a0-fabe-47b1-d97d-302931a0efba"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': [3109, 35570, 42743, 3530, 11233, 29898, 284, 43371, 3125, 17924, 262, 26265, 198, 198, 32, 42743, 1410, 284, 751, 13055, 7161, 15296, 290, 427, 6018, 82, 284, 8225, 3530, 685, 20456, 60, 1595, 447, 247, 83, 467, 1290, 1576, 284, 32980, 6155, 290, 38088, 11, 1139, 8108, 5926, 362, 9358, 7927, 12, 16337, 6669, 1601, 3059, 1252, 13, 198, 198, 5956, 3635, 11, 42743, 5545, 262, 6961, 284, 262, 10078, 362, 9358, 5583, 13, 4930, 12, 17936, 286, 262, 5386, 4855, 262, 1410, 11, 3249, 21736, 531, 11, 290, 6232, 399, 3955, 17513, 5858, 27940, 11465, 47005, 373, 257, 645, 12, 12860, 13, 554, 262, 886, 11, 262, 1410, 2722, 257, 28085, 838, 12, 15, 3015, 13, 198, 198, 464, 3092, 286, 5471, 11, 2158, 11, 743, 307, 257, 1051, 286, 42743, 4628, 17995, 517, 621, 1997, 2073, 13, 564, 250, 464, 6961, 416, 42743, 373, 11, 284, 910, 262, 1551, 11, 10356, 453, 39930, 11, 447, 251, 3059, 1252, 1297, 27262, 14036, 13, 564, 250, 1026, 373, 355, 881, 355, 345, 714, 2911, 284, 466, 1231, 1642, 262, 7901, 284, 4781, 7647, 319, 379, 1551, 530, 1735, 286, 262, 4675, 13, 447, 251, 198, 198, 7003, 42743, 468, 587, 319, 257, 4836, 428, 614, 351, 11628, 329, 2975, 18977, 290, 6861, 7161, 15296, 11, 262, 4086, 447, 247, 82, 9824, 3221, 836, 447, 247, 83, 34128, 881, 7647, 13, 24390, 278, 262, 9934, 286, 1097, 6143, 743, 1182, 572, 1402, 12, 9888, 1964, 11418, 11, 475, 340, 635, 7095, 262, 2928, 286, 262, 1748, 447, 247, 82, 4675, 3747, 4493, 13, 198, 198, 14874, 1252, 531, 339, 3382, 284, 766, 42743, 32980, 3338, 6155, 290, 38088, 625, 319, 12, 25662, 7647, 11, 2138, 621, 262, 584, 835, 1088, 13, 554, 16048, 13458, 11, 810, 546, 4019, 1411, 286, 12503, 389, 1097, 12, 5787, 11, 262, 4819, 815, 307, 2592, 17070, 329, 1688, 2458, 13, 4650, 661, 379, 938, 1285, 447, 247, 82, 3249, 11, 3059, 1252, 531, 11, 547, 635, 14718, 416, 564, 250, 13959, 12, 47336, 447, 251, 422, 262, 4086, 13, 198, 198, 447, 250, 2396, 1290, 11, 867, 661, 1254, 326, 19009, 12169, 318, 257, 1256, 517, 1561, 621, 340, 318, 2223, 11, 447, 251, 3059, 1252, 531, 13, 564, 250, 464, 2187, 13214, 286, 19009, 12169, 318, 326, 968, 1971, 318, 257, 22382, 12, 13120, 393, 257, 22382, 12, 34475, 1748, 13, 1002, 345, 765, 284, 910, 326, 11, 788, 262, 717, 1517, 345, 423, 284, 39659, 466, 318, 910, 326, 6493, 262, 2839, 9238, 286, 2839, 44820, 815, 407, 307, 257, 8475, 287, 597, 835, 13, 447, 251, 198, 198, 6104, 611, 617, 661, 1276, 898, 5006, 284, 11, 329, 1672, 11, 3151, 3946, 287, 262, 21040, 790, 1110, 11, 3059, 1252, 531, 11, 749, 13458, 2737, 460, 787, 466, 351, 1097, 12, 20077, 393, 1097, 14447, 13, 843, 661, 508, 1282, 284, 466, 1597, 287, 13458, 851, 611, 484, 3708, 379, 477, 851, 836, 447, 247, 83, 3221, 1607, 7647, 826, 287, 2166, 286, 511, 10965, 13, 198, 198, 447, 250, 40, 9149, 345, 612, 389, 11192, 286, 4138, 286, 17092, 6898, 44820, 326, 466, 1310, 517, 621, 1650, 319, 262, 4675, 290, 651, 3888, 329, 4675, 12724, 11, 447, 251, 3059, 1252, 531, 13, 564, 250, 1639, 460, 447, 247, 83, 466, 1111, 1243, 13, 921, 460, 447, 247, 83, 423, 257, 22382, 12, 13120, 1748, 290, 7905, 1096, 262, 9238, 286, 2839, 44820, 329, 257, 8904, 2033, 286, 262, 3265, 13, 447, 251, 198, 198, 3633, 3059, 1252, 1244, 1254, 517, 7634, 546, 262, 2071, 621, 617, 286, 465, 5891, 3096, 1866, 11, 339, 4206, 339, 2125, 447, 247, 83, 3436, 13, 564, 250, 1858, 389, 257, 1256, 286, 661, 508, 1254, 262, 976, 835, 355, 314, 466, 319, 262, 3096, 475, 3588, 447, 247, 83, 4684, 284, 6004, 284, 262, 11762, 278, 286, 661, 508, 423, 9431, 2405, 326, 484, 423, 284, 423, 257, 1097, 11, 447, 251, 339, 531, 13, 1320, 447, 247, 82, 703, 262, 11762, 364, 460, 900, 262, 8666, 329, 262, 1748, 447, 247, 82, 4675, 3747, 3352, 11, 772, 287, 281, 1989, 588, 8108, 5665, 362, 810, 262, 5909, 3741, 286, 5085, 836, 447, 247, 83, 898, 5006, 13, 198, 198, 447, 250, 1135, 447, 247, 260, 7219, 351, 257, 1748, 326, 329, 4317, 812, 468, 587, 3170, 290, 5257, 329, 5006, 11, 447, 251, 3059, 1252, 531, 13, 564, 250, 1639, 460, 447, 247, 83, 655, 787, 1310, 2458, 1088, 262, 13015, 290, 1607, 597, 8904, 9025, 287, 262, 2761, 326, 468, 2727, 13, 447, 251, 198, 198, 464, 1410, 284, 7521, 7161, 15296, 290, 427, 6018, 82, 319, 8225, 3530, 481, 1282, 510, 329, 257, 3015, 379, 262, 1306, 10078, 362, 1336, 3096, 3249, 11, 7530, 329, 3035, 2242, 379, 718, 25, 1270, 9114, 287, 262, 30625, 1505, 286, 262, 3059, 349, 3477, 11819, 11, 642, 3553, 20617, 13, 50256], 'len': 817}\n",
            "{'ids': [15029, 23368, 4599, 10594, 20656, 329, 5932, 32404, 11, 42345, 11, 468, 4376, 720, 20, 13, 20, 1510, 357, 33907, 17, 13, 19, 2997, 8, 284, 1104, 661, 20085, 416, 262, 14101, 27283, 543, 2555, 284, 15228, 29860, 1973, 262, 1499, 13, 198, 198, 464, 1294, 1461, 37690, 11, 5399, 416, 25670, 1962, 35764, 42736, 11, 4376, 262, 5153, 422, 7846, 4200, 290, 2107, 14389, 1141, 281, 1785, 17494, 317, 5265, 284, 38065, 4434, 23368, 290, 4725, 8476, 37, 2714, 287, 968, 1971, 13, 198, 198, 4821, 284, 257, 2643, 422, 42345, 338, 7567, 1710, 4434, 23368, 11, 262, 15740, 481, 307, 6626, 8603, 1022, 4725, 8476, 37, 290, 42345, 338, 12684, 7567, 1710, 4434, 23368, 13, 198, 198, 18454, 6415, 11, 508, 318, 10810, 416, 262, 41559, 2159, 13407, 355, 262, 1266, 12, 16473, 4048, 8296, 6802, 290, 262, 5544, 1266, 12, 16473, 719, 286, 477, 640, 2157, 383, 27330, 11, 35169, 1763, 1636, 11, 290, 3899, 6612, 11, 373, 42345, 11, 373, 9899, 262, 4599, 10594, 20656, 329, 5932, 32404, 416, 1992, 5613, 337, 1071, 283, 9232, 287, 3426, 938, 614, 13, 198, 198, 464, 28422, 1625, 739, 4334, 2046, 422, 1966, 4434, 23368, 1893, 25936, 347, 5282, 706, 19320, 2105, 1410, 329, 4813, 6, 21531, 13, 42345, 508, 6622, 262, 1700, 329, 262, 749, 1271, 12, 1952, 319, 477, 36229, 15907, 351, 257, 2472, 286, 23871, 1271, 12, 1952, 18524, 2427, 329, 838, 15806, 7021, 351, 530, 5627, 546, 2026, 2444, 13, 50256], 'len': 246}\n",
            "{'ids': [9980, 198, 198, 818, 2253, 8886, 8326, 8591, 26296, 357, 10374, 5049, 8, 1125, 33721, 18912, 78, 344, 3308, 4496, 275, 692, 557, 300, 447, 247, 330, 39566, 198, 198, 2919, 2471, 380, 293, 1853, 198, 198, 10989, 2574, 3876, 347, 2575, 544, 198, 198, 50, 1155, 68, 12967, 380, 30, 8504, 264, 1155, 68, 49025, 72, 257, 38421, 259, 533, 30, 8504, 473, 6449, 68, 36945, 3653, 78, 1290, 275, 692, 557, 300, 447, 247, 330, 39566, 30, 38308, 78, 8591, 26296, 1125, 24685, 583, 7608, 72, 13, 2409, 5049, 387, 2274, 972, 68, 26992, 979, 5549, 21110, 47, 4298, 78, 17730, 11, 8591, 26296, 21110, 38942, 576, 12379, 9198, 533, 17730, 1125, 33721, 18912, 78, 344, 3308, 4496, 1290, 275, 692, 557, 300, 447, 247, 330, 39566, 13, 2448, 6106, 79, 1670, 72, 533, 4078, 64, 11, 3623, 17266, 264, 404, 81, 1078, 315, 1462, 28691, 13, 309, 15318, 40543, 11, 1729, 8591, 42377, 2787, 78, 424, 70, 4528, 41498, 7344, 390, 72, 2208, 647, 66, 7246, 287, 632, 9752, 13, 2671, 72, 48241, 11, 443, 3112, 710, 11, 443, 20280, 500, 11, 1312, 277, 385, 50173, 21110, 47, 4298, 78, 17730, 3367, 78, 1185, 72, 29707, 7246, 1658, 2527, 452, 3263, 68, 583, 4229, 11991, 5549, 45630, 5733, 11, 257, 5554, 78, 279, 533, 287, 568, 487, 9100, 68, 257, 72, 2169, 14415, 4161, 16634, 14678, 456, 72, 357, 67, 615, 332, 78, 10091, 1125, 1609, 273, 1313, 78, 583, 9198, 533, 555, 31028, 45807, 2566, 26296, 13, 327, 1229, 22050, 4528, 6184, 101, 5026, 489, 291, 747, 25147, 25, 33721, 475, 8326, 8591, 26296, 287, 555, 64, 279, 671, 8466, 267, 287, 555, 64, 28145, 5708, 11, 33721, 25470, 555, 745, 447, 247, 288, 447, 247, 330, 39566, 277, 445, 6814, 264, 404, 430, 304, 8591, 33721, 300, 3372, 544, 18912, 78, 344, 260, 257, 14035, 25634, 435, 1462, 583, 555, 64, 875, 1437, 2566, 949, 47966, 13, 13778, 9732, 1462, 11, 3308, 4496, 8591, 645, 544, 2566, 466, 332, 355, 79, 3087, 533, 1125, 300, 447, 247, 330, 39566, 275, 33011, 13, 554, 5978, 260, 11, 1729, 466, 85, 8374, 36945, 3653, 78, 629, 6192, 5031, 13, 4689, 26296, 840, 273, 1350, 9732, 8326, 300, 447, 247, 330, 39566, 11, 1282, 583, 4229, 6106, 78, 13, 1439, 64, 3734, 11, 8591, 33721, 1779, 271, 344, 257, 279, 9607, 3681, 78, 13, 21110, 2348, 18794, 68, 532, 2448, 5036, 89, 7935, 287, 745, 11072, 949, 47966, 17730, 11, 33721, 1232, 469, 33154, 293, 1013, 8471, 295, 72, 390, 8466, 1627, 64, 21110, 47, 4298, 78, 17730, 13, 309, 15318, 40543, 11, 407, 5733, 435, 66, 35657, 4130, 10845, 259, 2743, 45630, 3216, 357, 2395, 1729, 5026, 1671, 5733, 18605, 1462, 920, 385, 72, 459, 72, 1619, 40426, 17631, 828, 4229, 6106, 586, 5549, 2566, 1235, 64, 14364, 10071, 21110, 660, 31522, 3970, 17730, 6184, 101, 555, 64, 26296, 31028, 15318, 455, 78, 629, 12375, 13, 21110, 14772, 629, 2013, 4496, 387, 2457, 434, 68, 302, 568, 31028, 127, 117, 5026, 489, 501, 38421, 259, 533, 8591, 26296, 17730, 11, 629, 11590, 532, 369, 555, 5680, 78, 288, 447, 247, 1934, 544, 532, 383, 6714, 49636, 13, 21110, 127, 230, 555, 809, 261, 287, 528, 952, 11, 2409, 5049, 11, 17266, 1196, 4703, 32009, 627, 25440, 287, 528, 72, 257, 1138, 353, 979, 1312, 45756, 567, 35671, 3209, 44240, 7246, 17730, 13, 198, 198, 16224, 44967, 49, 3727, 52, 57, 2849, 36, 45698, 1137, 53, 13563, 50256], 'len': 580}\n",
            "{'ids': [2990, 447, 247, 260, 2562, 284, 4136, 11, 262, 19102, 12, 31343, 290, 465, 4639, 11, 336, 18886, 1735, 416, 1735, 510, 8774, 3530, 11061, 257, 645, 3204, 4252, 11, 49059, 416, 262, 734, 12, 13571, 17214, 290, 48520, 6832, 286, 12568, 20970, 11, 18329, 13, 887, 340, 447, 247, 82, 407, 262, 835, 484, 447, 247, 260, 12049, 13, 198, 198, 38069, 10424, 40014, 694, 11, 262, 582, 484, 869, 262, 564, 250, 48412, 16734, 277, 9116, 71, 11751, 447, 251, 1595, 447, 247, 83, 3197, 3241, 284, 2241, 32420, 8613, 11, 1595, 447, 247, 83, 1509, 37382, 2241, 287, 47923, 1134, 292, 784, 407, 597, 517, 11, 6949, 11, 290, 407, 503, 994, 287, 5366, 12, 23316, 18329, 11, 810, 339, 3443, 35622, 706, 7351, 257, 1440, 12, 1941, 22966, 287, 257, 2679, 3770, 329, 25950, 19102, 12, 31343, 11613, 13, 679, 1595, 447, 247, 83, 21405, 564, 250, 1544, 346, 11908, 0, 447, 251, 679, 1595, 447, 247, 83, 2686, 465, 826, 3211, 656, 262, 1633, 13, 198, 198, 2949, 11, 340, 447, 247, 82, 407, 262, 835, 484, 447, 247, 260, 12049, 11, 996, 40014, 694, 318, 5762, 257, 2422, 7586, 10147, 11, 290, 465, 4639, 11, 508, 1839, 447, 247, 83, 5911, 2241, 11, 318, 625, 12, 16539, 276, 329, 257, 5814, 6076, 1110, 13, 632, 447, 247, 82, 1223, 546, 511, 24521, 11, 257, 2116, 12, 34009, 1108, 12548, 287, 511, 33769, 11, 355, 611, 484, 447, 247, 260, 407, 14523, 1597, 523, 881, 355, 2712, 379, 340, 13, 198, 198, 1532, 345, 821, 3375, 546, 1294, 19147, 326, 423, 550, 281, 2928, 287, 262, 938, 2026, 812, 11, 13573, 10424, 318, 2192, 1271, 530, 5811, 8662, 1559, 11, 12297, 12, 7469, 14755, 4041, 198, 198, 2953, 718, 701, 604, 259, 290, 7323, 14956, 23160, 11, 40014, 694, 18028, 2029, 465, 4639, 13, 15176, 340, 407, 329, 465, 24271, 3842, 10522, 11, 465, 15867, 1203, 12148, 286, 564, 250, 33001, 25590, 447, 251, 11, 465, 11892, 286, 35333, 11908, 357, 1929, 296, 339, 7832, 284, 869, 564, 250, 18820, 31533, 447, 251, 655, 284, 9890, 287, 262, 11616, 828, 262, 5166, 561, 1283, 2048, 401, 605, 11, 257, 43442, 290, 27583, 11, 393, 5699, 991, 11, 257, 14657, 88, 290, 262, 14842, 13, 198, 198, 447, 250, 32, 9707, 30, 679, 373, 1239, 257, 9707, 11, 447, 251, 531, 5811, 8662, 1559, 11, 1966, 3437, 286, 262, 12297, 12, 7469, 14755, 4041, 447, 247, 82, 31981, 1829, 17718, 11, 618, 314, 11434, 40014, 694, 447, 247, 82, 12085, 13, 564, 250, 1532, 345, 447, 247, 260, 3375, 546, 1605, 19147, 326, 423, 550, 281, 2928, 19765, 287, 262, 938, 2026, 812, 11, 13573, 10424, 318, 2192, 1271, 530, 13, 447, 251, 198, 198, 1890, 8662, 1559, 11, 40014, 694, 447, 247, 82, 22942, 318, 257, 8119, 1339, 2050, 11, 257, 8689, 2861, 20394, 284, 7564, 262, 5895, 286, 257, 44194, 6877, 368, 506, 263, 13, 198, 198, 4821, 284, 262, 8050, 30627, 3854, 3337, 11, 564, 250, 1169, 7702, 826, 373, 517, 4388, 287, 8218, 262, 1964, 8661, 938, 614, 357, 5304, 8, 621, 287, 2063, 257, 4289, 447, 251, 13, 770, 25571, 468, 302, 1144, 663, 1182, 287, 5107, 1111, 6590, 290, 14177, 15735, 11, 422, 262, 30990, 11533, 286, 9293, 588, 262, 6714, 8865, 263, 11, 9393, 416, 281, 1196, 6972, 11908, 578, 11, 284, 262, 28509, 286, 5465, 6741, 6264, 3393, 1708, 3759, 1301, 447, 247, 82, 3071, 13, 198, 198, 43, 559, 694, 11414, 38590, 656, 257, 22435, 286, 1290, 826, 1405, 928, 947, 7796, 281, 3098, 12, 20541, 396, 8666, 13, 887, 5023, 523, 867, 19642, 511, 5465, 1909, 11, 339, 468, 587, 36145, 1359, 465, 1948, 4508, 287, 2031, 329, 4647, 25, 878, 11, 1141, 290, 706, 262, 5230, 30067, 4569, 11613, 9619, 13, 198, 198, 447, 250, 6653, 20411, 290, 1245, 423, 587, 3421, 416, 3037, 11, 447, 251, 8662, 1559, 1139, 11, 564, 250, 4360, 262, 1109, 326, 339, 447, 247, 82, 991, 2491, 257, 13662, 2524, 11, 326, 339, 5818, 447, 247, 83, 2626, 465, 2694, 284, 2018, 477, 777, 2628, 11, 326, 339, 468, 845, 922, 6958, 351, 4138, 290, 4138, 286, 19102, 12, 44527, 477, 625, 262, 995, 784, 326, 447, 247, 82, 407, 32081, 13, 447, 251, 198, 198, 40, 2342, 422, 1973, 262, 4675, 355, 484, 3802, 262, 7072, 13, 23302, 606, 11, 257, 5462, 286, 16949, 3259, 288, 9248, 511, 7405, 422, 262, 41498, 33266, 355, 484, 302, 12, 30171, 283, 262, 20897, 13, 679, 447, 247, 82, 407, 5213, 13, 632, 447, 247, 82, 407, 262, 3925, 326, 11393, 683, 784, 340, 447, 247, 82, 262, 2628, 11, 6364, 564, 250, 31059, 2411, 2890, 447, 251, 262, 5440, 11, 339, 5804, 11, 2712, 262, 890, 369, 286, 564, 250, 11186, 20744, 447, 251, 13, 198, 198, 3792, 612, 257, 19102, 12, 31343, 6388, 21778, 287, 1301, 1499, 30, 4149, 517, 198, 198, 464, 2119, 318, 47600, 19943, 290, 6247, 13, 383, 7588, 389, 5391, 11, 290, 262, 4639, 11, 991, 287, 465, 3091, 88, 15224, 11, 6296, 13970, 674, 3084, 13, 679, 1595, 447, 247, 83, 12589, 502, 618, 314, 2239, 2641, 13, 314, 1265, 611, 339, 447, 247, 82, 351, 13573, 10424, 13, 679, 34833, 3812, 262, 40018, 13, 198, 198, 12211, 82, 1568, 11, 40014, 694, 17907, 503, 11, 19921, 2651, 11, 32766, 29779, 287, 465, 16139, 3903, 11, 49303, 6546, 11, 4190, 49038, 1068, 290, 29776, 40325, 13, 198, 198, 464, 995, 447, 247, 82, 1353, 22693, 286, 10398, 19102, 12, 31343, 11613, 318, 3492, 329, 465, 717, 8661, 2056, 2720, 287, 812, 13, 198, 198, 447, 250, 1639, 466, 257, 1049, 1171, 2139, 416, 2282, 777, 7974, 389, 1088, 11, 447, 251, 1139, 8662, 1559, 11, 508, 3377, 812, 286, 465, 3451, 9646, 40014, 694, 447, 247, 82, 670, 13, 564, 250, 1026, 447, 247, 82, 5448, 329, 661, 284, 651, 1969, 284, 11854, 523, 484, 460, 1333, 648, 5039, 644, 11854, 3073, 588, 13, 447, 251, 198, 198, 40, 717, 11237, 40014, 694, 938, 3945, 11, 706, 11065, 262, 8050, 30627, 3854, 3337, 447, 247, 82, 8308, 3199, 564, 250, 39, 378, 9347, 447, 251, 13, 198, 198, 5189, 262, 860, 1558, 4075, 5465, 2628, 3402, 13673, 11, 7388, 547, 12267, 2628, 11, 290, 1936, 3214, 1626, 262, 11637, 286, 616, 1363, 1181, 286, 18329, 13, 4930, 286, 606, 287, 262, 19102, 12, 31343, 6536, 25, 262, 399, 5258, 2969, 14, 32, 46, 11, 262, 2679, 38787, 329, 262, 2351, 21773, 2679, 16847, 3615, 14, 46, 4399, 292, 12275, 290, 10467, 24560, 13661, 11, 1111, 46766, 416, 10936, 564, 250, 38069, 10424, 447, 251, 40014, 694, 13, 383, 1966, 373, 5610, 287, 262, 3139, 1748, 286, 12406, 11, 262, 6846, 287, 7011, 10711, 11, 3265, 513, 11, 7410, 13, 198, 198, 3260, 4917, 262, 3052, 329, 10467, 24560, 13661, 11, 314, 2952, 24315, 262, 691, 2209, 5610, 11, 40962, 284, 1064, 2130, 351, 40014, 694, 447, 247, 82, 2106, 2877, 12703, 1871, 514, 13, 314, 1422, 447, 247, 83, 892, 339, 561, 3551, 736, 11, 475, 655, 1160, 2431, 1568, 11, 465, 2882, 373, 5586, 287, 616, 37282, 13, 198, 198, 447, 250, 40, 447, 247, 76, 10663, 12, 1186, 1202, 11, 447, 251, 339, 2630, 11, 564, 250, 392, 16231, 7794, 9299, 422, 262, 1957, 290, 1181, 2056, 13, 447, 251, 1002, 314, 547, 3597, 329, 257, 2260, 393, 3230, 9207, 11, 2158, 11, 339, 447, 247, 67, 307, 4684, 284, 787, 281, 6631, 13, 7486, 11188, 351, 257, 19102, 12, 31343, 23018, 616, 25377, 784, 262, 845, 1807, 286, 465, 1438, 287, 616, 13734, 5291, 502, 510, 379, 1755, 784, 356, 9658, 287, 3638, 329, 1811, 1933, 1566, 356, 3443, 8970, 287, 257, 640, 290, 1295, 284, 1826, 13, 198, 198, 818, 1048, 11, 40014, 694, 318, 2147, 611, 407, 2116, 12, 9685, 13, 198, 198, 1544, 14532, 546, 465, 564, 250, 1069, 45428, 48740, 447, 251, 355, 1690, 355, 339, 3848, 2241, 257, 564, 250, 8044, 6194, 447, 251, 11, 543, 318, 284, 910, 25, 6777, 13, 679, 318, 3734, 351, 852, 1444, 257, 12267, 8928, 392, 396, 13, 679, 1595, 447, 247, 83, 7267, 4306, 11, 996, 339, 17424, 339, 9664, 689, 564, 250, 64, 1256, 1342, 621, 749, 8928, 392, 1023, 447, 251, 290, 3729, 1342, 621, 564, 250, 1169, 2347, 2056, 447, 251, 13, 383, 517, 5035, 1808, 11, 339, 1568, 4952, 502, 11, 2125, 447, 247, 83, 1771, 393, 407, 644, 339, 1139, 318, 2081, 11, 475, 703, 881, 339, 447, 247, 82, 22532, 13, 679, 1139, 777, 1243, 6777, 11, 39033, 9644, 2241, 287, 4719, 11, 823, 3929, 287, 262, 13791, 6516, 1022, 1109, 290, 10165, 13, 198, 198, 6653, 6562, 1387, 287, 2679, 468, 2957, 867, 287, 262, 1613, 284, 27531, 257, 845, 1103, 4046, 26795, 3681, 351, 281, 5676, 2679, 18702, 13, 679, 12766, 351, 262, 564, 250, 81, 447, 251, 2128, 11, 21436, 15129, 257, 564, 250, 86, 447, 251, 393, 34817, 340, 5000, 13, 843, 339, 1690, 1067, 315, 2052, 319, 262, 9546, 564, 250, 4906, 286, 1517, 447, 251, 11, 772, 618, 262, 9546, 1595, 447, 247, 83, 4174, 25, 564, 250, 40, 447, 247, 76, 407, 910, 447, 247, 77, 356, 815, 11, 2099, 286, 1517, 11, 475, 262, 369, 74, 395, 286, 2877, 2272, 284, 21810, 478, 331, 7456, 898, 266, 558, 11, 331, 7456, 898, 10845, 11693, 318, 7477, 13, 447, 251, 198, 198, 28524, 287, 16629, 287, 24217, 11, 40014, 694, 6348, 510, 287, 257, 14310, 286, 386, 12, 16010, 15598, 13, 4874, 35786, 262, 564, 250, 16010, 21891, 286, 2253, 447, 251, 11, 416, 262, 15533, 82, 16629, 12007, 262, 4387, 2679, 12, 6286, 3265, 287, 262, 1499, 2354, 286, 4842, 290, 968, 1971, 2254, 13, 887, 257, 2583, 286, 5087, 23273, 262, 7918, 286, 257, 937, 30764, 2679, 1605, 2055, 11, 407, 262, 1551, 286, 543, 373, 262, 1964, 3164, 284, 23934, 663, 15012, 287, 262, 7765, 286, 262, 717, 995, 1175, 784, 281, 6980, 286, 14800, 3098, 12, 16010, 15598, 784, 290, 262, 28236, 9082, 286, 262, 10467, 24560, 13, 198, 198, 43, 559, 694, 447, 247, 82, 3397, 11, 1111, 422, 9279, 11, 6348, 510, 287, 428, 13559, 2890, 8137, 11, 996, 339, 3667, 484, 547, 1729, 12, 23149, 13, 564, 250, 5189, 1781, 11, 612, 547, 3651, 546, 262, 1578, 1829, 4330, 319, 262, 2642, 1735, 11, 447, 251, 339, 1139, 13, 564, 250, 1537, 11, 706, 477, 11, 326, 447, 247, 82, 2495, 3489, 13, 447, 251, 198, 198, 34, 420, 2049, 276, 416, 465, 15012, 11, 40014, 694, 4166, 281, 14352, 2565, 286, 1641, 290, 9450, 5369, 784, 1223, 339, 447, 247, 67, 32870, 1096, 329, 262, 1334, 286, 465, 1204, 13, 198, 198, 447, 250, 40, 447, 247, 303, 587, 281, 1605, 329, 625, 3126, 812, 11, 447, 251, 339, 1139, 13, 564, 250, 40, 447, 247, 303, 587, 257, 2679, 329, 625, 604, 11, 830, 13, 447, 251, 198, 198, 40, 447, 247, 303, 587, 281, 1605, 329, 625, 3126, 812, 13, 314, 447, 247, 303, 587, 257, 2679, 329, 625, 604, 11, 830, 13573, 10424, 40014, 694, 198, 198, 2215, 40014, 694, 373, 1367, 11, 465, 2988, 12155, 11, 15734, 281, 11949, 351, 262, 317, 46, 4176, 10501, 11, 6292, 257, 2992, 11094, 379, 262, 2059, 286, 18329, 11, 23319, 278, 262, 1641, 284, 12406, 13, 1119, 3888, 656, 257, 12949, 1363, 319, 257, 5897, 11, 5509, 12, 10837, 4675, 319, 262, 10183, 5743, 286, 262, 1748, 25, 3504, 1398, 11, 18644, 306, 5175, 290, 21658, 2330, 13, 198, 198, 10915, 18329, 11, 1165, 11, 32758, 2383, 2679, 15012, 11, 262, 16064, 994, 547, 517, 1605, 1143, 11, 40014, 694, 1139, 13, 20463, 286, 606, 17383, 262, 3303, 11, 475, 517, 621, 326, 11, 484, 447, 247, 67, 2626, 644, 339, 783, 9569, 355, 262, 6899, 14306, 286, 2679, 3968, 13, 198, 198, 1212, 4802, 12928, 422, 465, 2260, 2569, 40678, 287, 16629, 11, 5929, 351, 257, 38935, 1605, 3753, 25584, 11, 7121, 683, 625, 262, 5743, 13, 679, 3177, 12406, 564, 250, 64, 45319, 3215, 1499, 447, 251, 13, 198, 198, 3792, 612, 257, 19102, 12, 31343, 6388, 21778, 287, 1301, 1499, 30, 4149, 517, 198, 198, 1544, 25518, 656, 3835, 290, 5901, 465, 2119, 351, 2679, 1175, 842, 9752, 13, 1649, 339, 3443, 1965, 329, 2185, 259, 45327, 69, 11, 379, 655, 1478, 11, 340, 1422, 447, 247, 83, 1283, 326, 19501, 329, 262, 976, 3718, 32346, 5141, 508, 447, 247, 67, 1965, 329, 383, 14884, 36757, 78, 655, 262, 1285, 878, 13, 679, 16563, 262, 1966, 11, 286, 1781, 13, 887, 2185, 259, 45327, 69, 564, 250, 9727, 2818, 2565, 447, 251, 11, 31578, 2890, 262, 386, 12, 16010, 3815, 339, 373, 302, 1144, 319, 11, 290, 36642, 278, 6071, 262, 564, 250, 9866, 3234, 447, 251, 7918, 339, 991, 662, 3694, 1909, 13, 198, 198, 1544, 18303, 422, 12406, 3687, 1029, 1524, 287, 655, 1115, 812, 11, 22292, 2252, 656, 262, 2260, 15889, 8876, 11, 24353, 27126, 284, 12267, 16125, 290, 23202, 1111, 465, 2988, 290, 13325, 3956, 5199, 284, 262, 2151, 13, 198, 198, 818, 281, 719, 286, 564, 250, 38546, 10510, 447, 251, 11, 339, 3421, 465, 1438, 422, 10936, 284, 564, 250, 38069, 10424, 447, 251, 11, 290, 12020, 910, 339, 447, 247, 67, 1690, 3376, 606, 319, 340, 13, 564, 250, 1532, 14154, 3754, 15551, 460, 307, 15870, 12104, 11, 447, 251, 339, 1139, 11, 564, 250, 8524, 10936, 40014, 694, 460, 307, 13573, 10424, 40014, 694, 13, 447, 251, 198, 198, 46445, 32289, 910, 465, 2802, 11, 16753, 11, 373, 564, 250, 1169, 5336, 395, 530, 625, 612, 447, 251, 13, 1119, 3505, 607, 351, 25303, 11, 10609, 503, 673, 2921, 2279, 284, 326, 1641, 290, 3947, 284, 651, 523, 1310, 287, 1441, 13, 1119, 910, 40014, 694, 447, 247, 82, 6621, 2365, 501, 11615, 329, 812, 351, 644, 262, 1641, 1807, 373, 257, 5110, 8967, 11, 691, 284, 1064, 503, 673, 550, 257, 3632, 22359, 13, 198, 198, 2061, 262, 12020, 1422, 447, 247, 83, 760, 784, 644, 484, 3521, 447, 247, 83, 784, 373, 326, 262, 18887, 3367, 286, 428, 19501, 9279, 1641, 373, 2582, 284, 4474, 530, 286, 262, 749, 1593, 290, 1290, 12, 30771, 19102, 12, 31343, 5745, 287, 2106, 11, 290, 326, 339, 447, 247, 67, 466, 340, 477, 287, 8631, 6504, 11, 826, 612, 287, 465, 3397, 447, 247, 17012, 13, 198, 198, 3260, 734, 812, 379, 6403, 11, 40014, 694, 5710, 503, 284, 2962, 465, 4040, 10522, 13, 679, 717, 5884, 351, 257, 48525, 2685, 286, 19102, 12, 44527, 287, 968, 1971, 878, 7348, 284, 564, 250, 1169, 2988, 1044, 447, 251, 287, 16101, 11, 810, 339, 1043, 47923, 1134, 292, 290, 31139, 3194, 287, 28530, 11, 996, 645, 5895, 286, 257, 22706, 3356, 13, 679, 373, 5169, 329, 25950, 12267, 9285, 11, 5734, 43203, 416, 4486, 447, 247, 82, 4301, 2438, 355, 636, 286, 262, 2853, 1031, 2649, 1430, 1708, 262, 1218, 995, 1175, 11, 475, 340, 2921, 683, 281, 2126, 13, 198, 198, 1544, 8477, 340, 287, 383, 7868, 286, 281, 10461, 32562, 11, 465, 2116, 12, 30271, 41795, 25, 198, 198, 447, 250, 2025, 11292, 4009, 1912, 287, 257, 1479, 1499, 561, 5127, 262, 11447, 6625, 351, 28049, 4635, 10398, 2300, 13, 632, 561, 423, 257, 8187, 2800, 2209, 287, 262, 1479, 1499, 13, 17193, 557, 3808, 561, 3328, 1479, 6291, 9285, 290, 511, 898, 3748, 564, 246, 2389, 1271, 447, 247, 329, 779, 287, 2003, 22440, 2427, 286, 511, 1103, 1438, 290, 2209, 13, 770, 6861, 511, 5369, 287, 262, 1785, 286, 257, 1568, 28759, 286, 262, 6920, 13, 447, 251, 198, 198, 19093, 262, 2351, 21773, 2679, 16847, 3615, 14, 46, 4399, 292, 12275, 11, 393, 399, 5258, 2969, 14, 32, 46, 11, 373, 4642, 13, 198, 198, 464, 2126, 3521, 447, 247, 83, 423, 587, 18599, 11, 475, 287, 262, 662, 12, 37675, 2479, 11, 1239, 878, 550, 262, 19102, 12, 31343, 6625, 287, 2031, 587, 523, 5884, 13, 35305, 738, 4778, 508, 4271, 2993, 2147, 286, 530, 1194, 547, 783, 287, 6946, 11, 290, 13573, 10424, 40014, 694, 11, 8523, 656, 465, 1160, 82, 11, 1752, 257, 1492, 680, 16330, 1582, 9520, 11, 4983, 355, 262, 44457, 422, 4138, 286, 4608, 1497, 13, 198, 198, 12025, 3009, 17334, 7326, 2304, 515, 2587, 422, 257, 1290, 826, 20440, 1448, 319, 3359, 379, 257, 1803, 4495, 2714, 416, 13512, 37036, 447, 247, 82, 9475, 286, 262, 19614, 287, 2813, 13, 18903, 25, 978, 14814, 198, 198, 1026, 2067, 351, 257, 1057, 286, 655, 352, 11, 830, 47923, 9232, 28568, 37789, 284, 4486, 13, 317, 614, 1568, 11, 40014, 694, 6797, 287, 465, 41795, 11, 262, 2811, 1057, 373, 1802, 11, 830, 13, 15894, 11, 262, 317, 46, 3199, 257, 2679, 12, 16129, 7533, 11, 262, 10896, 45327, 69, 622, 69, 11, 290, 1568, 991, 11, 281, 3594, 2196, 1444, 383, 2351, 21773, 6358, 13, 198, 198, 7191, 262, 1306, 734, 4647, 11, 1022, 15231, 290, 8735, 11, 339, 3888, 1088, 1022, 12406, 11, 4842, 290, 968, 1971, 11, 475, 339, 1464, 973, 262, 976, 1441, 2209, 319, 262, 11613, 13, 383, 14741, 9902, 284, 3016, 257, 8667, 8950, 290, 1542, 2678, 13, 10797, 301, 1068, 416, 663, 898, 14580, 784, 564, 250, 1135, 6939, 356, 547, 407, 655, 257, 7684, 286, 1468, 661, 3492, 284, 4656, 11, 447, 251, 40014, 694, 1139, 784, 262, 6625, 3356, 287, 2031, 11, 290, 2592, 287, 4486, 11, 18750, 13, 198, 198, 10915, 3146, 15641, 6768, 379, 262, 640, 11, 262, 12297, 12, 7469, 14755, 4041, 6108, 287, 9656, 326, 3016, 3126, 11, 830, 16064, 547, 2950, 287, 19102, 12, 31343, 3842, 13, 383, 2679, 1230, 2346, 25183, 326, 981, 691, 362, 11, 830, 16064, 4197, 257, 7646, 12267, 15936, 11, 3016, 5946, 11, 830, 826, 5469, 20674, 547, 4075, 287, 2972, 5465, 2628, 11, 290, 355, 867, 355, 718, 11, 7029, 550, 587, 28131, 1143, 13, 15467, 835, 11, 477, 286, 262, 7746, 550, 33096, 5566, 1201, 40014, 694, 447, 247, 82, 10325, 287, 262, 8069, 82, 13, 198, 198, 818, 3389, 15408, 11, 17499, 262, 4353, 400, 11162, 286, 14912, 439, 77, 19725, 11, 19102, 12, 44527, 287, 31970, 458, 14054, 262, 1748, 351, 317, 46, 19379, 326, 1100, 25, 564, 250, 1135, 389, 994, 757, 13, 2297, 8880, 42531, 13, 2094, 447, 247, 83, 2822, 5582, 13, 447, 251, 198, 198, 2215, 257, 28509, 286, 33064, 368, 16233, 6741, 6265, 503, 287, 9530, 2502, 287, 3945, 15524, 11, 2679, 4237, 4260, 262, 25016, 547, 29768, 416, 262, 317, 46, 832, 257, 14780, 3331, 1848, 13, 198, 198, 1870, 618, 257, 2608, 12, 1941, 12, 727, 5103, 8383, 2823, 290, 2923, 2241, 290, 1115, 19670, 351, 257, 16790, 379, 257, 26617, 287, 399, 495, 47369, 287, 2932, 14489, 11, 1644, 1043, 465, 16511, 22259, 351, 317, 46, 28568, 13, 198, 198, 818, 9768, 11, 40014, 694, 447, 247, 82, 11613, 373, 1043, 379, 262, 4065, 3715, 286, 517, 621, 939, 4301, 10986, 11, 3016, 477, 286, 606, 13973, 416, 6590, 4568, 13, 843, 287, 262, 1903, 1528, 286, 262, 30655, 9976, 11, 40014, 694, 973, 262, 317, 46, 14741, 284, 10960, 290, 25063, 1637, 329, 257, 4326, 286, 517, 621, 1802, 19102, 12, 31343, 13162, 286, 2972, 2260, 871, 284, 1907, 329, 28975, 11, 257, 12267, 12525, 287, 262, 1218, 995, 1175, 13, 198, 198, 43, 559, 694, 447, 247, 82, 4568, 547, 7272, 20738, 784, 416, 262, 5349, 11, 416, 262, 7688, 11, 416, 262, 12297, 12, 7469, 14755, 4041, 13, 843, 1865, 11, 40014, 694, 6150, 9826, 1418, 7673, 540, 319, 1605, 9260, 13, 7945, 465, 31564, 329, 7996, 11, 339, 447, 247, 67, 24856, 2241, 351, 262, 3274, 8441, 13, 198, 198, 12025, 3009, 17334, 317, 21493, 3073, 379, 12566, 286, 6723, 6553, 290, 12267, 11613, 379, 262, 5849, 4867, 286, 14915, 13257, 287, 11307, 13, 18903, 25, 1757, 4100, 67, 20805, 439, 14, 17449, 14, 6633, 5382, 198, 198, 1858, 547, 289, 44240, 4739, 13, 327, 3413, 287, 2805, 15408, 351, 1160, 11, 830, 28568, 11, 257, 8390, 17981, 290, 257, 1588, 2160, 286, 1637, 11, 40014, 694, 3377, 1440, 290, 257, 2063, 1933, 287, 2679, 3770, 11, 393, 355, 339, 3848, 340, 11, 257, 564, 250, 5219, 12, 5143, 13064, 7541, 447, 251, 13, 843, 339, 1752, 2722, 257, 24518, 12, 3524, 19943, 2042, 11913, 5194, 287, 262, 6920, 11, 3665, 1576, 284, 1494, 2687, 1626, 257, 1936, 12, 284, 838, 701, 16874, 13, 1320, 714, 423, 587, 465, 2802, 11, 2988, 393, 465, 3956, 5199, 11, 477, 286, 4150, 6777, 6497, 510, 465, 6920, 13, 1471, 340, 714, 423, 587, 465, 49033, 666, 3656, 11, 257, 23860, 7509, 339, 1138, 287, 4842, 3706, 2365, 1437, 38234, 9160, 13, 198, 198, 6653, 3956, 13075, 11, 2158, 11, 508, 550, 7271, 47612, 6972, 465, 3956, 447, 247, 82, 12959, 11, 1239, 6497, 510, 262, 6920, 13, 198, 198, 43, 559, 694, 1139, 4819, 550, 2147, 284, 466, 351, 340, 11, 475, 319, 767, 3945, 837, 15524, 11, 339, 9639, 257, 1105, 12, 70, 559, 469, 18607, 11, 8998, 340, 379, 13075, 290, 5954, 262, 7616, 11, 40942, 683, 13, 1629, 262, 4854, 11, 13075, 784, 508, 8020, 447, 247, 83, 10764, 2366, 287, 262, 1363, 329, 517, 621, 734, 812, 784, 15463, 326, 339, 447, 247, 67, 5025, 416, 262, 1363, 284, 4268, 572, 257, 5301, 329, 465, 257, 4386, 15203, 6621, 11, 475, 465, 2988, 11, 4939, 422, 12317, 4890, 11, 3405, 18339, 683, 13, 40014, 694, 11, 508, 4030, 257, 18607, 6716, 287, 1339, 286, 1964, 6611, 1891, 11, 2982, 262, 725, 9650, 290, 4966, 26148, 13, 1649, 13075, 31213, 465, 2988, 11, 40014, 694, 1718, 4031, 290, 6294, 13, 383, 4530, 11, 543, 5281, 257, 7389, 286, 510, 284, 2026, 812, 11, 547, 5710, 706, 13075, 6520, 284, 1282, 2651, 290, 262, 640, 329, 257, 35564, 4473, 42118, 13, 198, 198, 14202, 286, 340, 20955, 40014, 694, 866, 13, 1892, 262, 29489, 602, 13, 1892, 262, 5194, 7432, 13, 1892, 262, 1641, 10512, 13, 383, 6920, 4030, 2406, 13, 383, 6920, 4030, 1016, 13, 843, 416, 262, 1903, 6303, 82, 11, 40014, 694, 373, 3177, 564, 250, 1169, 4094, 22693, 286, 19102, 12, 31343, 5696, 287, 262, 2679, 3715, 447, 251, 11, 1864, 284, 347, 24313, 40765, 11, 257, 6523, 329, 4486, 447, 247, 82, 4452, 329, 262, 9985, 286, 262, 7965, 11, 4920, 5734, 284, 5671, 19102, 12, 31343, 3842, 13, 198, 198, 447, 250, 2504, 447, 247, 82, 644, 314, 466, 11, 447, 251, 40014, 694, 1139, 13, 564, 250, 1026, 447, 247, 82, 257, 1107, 922, 1693, 13, 447, 251, 198, 198, 6653, 18377, 561, 4929, 510, 351, 683, 13, 554, 3426, 9656, 11, 706, 812, 286, 14285, 351, 2679, 1099, 5394, 290, 34209, 284, 26057, 262, 582, 3737, 749, 4497, 329, 511, 1499, 447, 247, 82, 13853, 287, 826, 5469, 25571, 11, 788, 12, 39379, 3437, 5593, 3232, 71, 4721, 281, 3645, 5670, 319, 3399, 36045, 2679, 19102, 12, 44527, 13, 198, 198, 10915, 339, 1422, 447, 247, 83, 5911, 3925, 11, 883, 508, 3940, 262, 1339, 2993, 3446, 810, 339, 447, 247, 67, 923, 13, 198, 198, 7571, 812, 1568, 11, 287, 2805, 8735, 11, 40014, 694, 373, 5169, 287, 16490, 319, 23070, 4884, 422, 32526, 2884, 4225, 16104, 11642, 683, 351, 4570, 4553, 9853, 286, 564, 250, 17080, 2455, 278, 11613, 1028, 262, 2679, 11784, 11, 12577, 8425, 13763, 11, 47494, 4301, 6529, 290, 11983, 287, 257, 4301, 4009, 13, 447, 251, 679, 373, 2582, 25963, 863, 284, 4486, 11, 543, 550, 587, 6549, 4509, 465, 4568, 290, 33509, 5912, 465, 2679, 40182, 13, 1867, 2597, 11, 611, 597, 11, 262, 5349, 2826, 287, 40014, 694, 447, 247, 82, 22338, 3793, 10061, 11, 996, 1111, 40014, 694, 2241, 290, 584, 4237, 1969, 284, 262, 1339, 508, 1965, 407, 284, 307, 5174, 1624, 1294, 2828, 547, 2950, 13, 198, 198, 32478, 835, 11, 40014, 694, 373, 11897, 284, 1440, 812, 287, 2679, 3770, 13, 679, 16047, 339, 2993, 262, 3251, 373, 2406, 11, 12988, 262, 3767, 9207, 286, 262, 10896, 45327, 69, 622, 69, 706, 465, 3251, 355, 6617, 286, 262, 317, 46, 447, 247, 82, 1674, 18627, 13, 198, 198, 2061, 339, 2492, 447, 247, 83, 5597, 329, 11, 2158, 11, 373, 262, 5230, 13, 198, 198, 12025, 3009, 17334, 10936, 40014, 694, 5667, 262, 31104, 17700, 2184, 287, 2932, 8735, 13, 18903, 25, 347, 9491, 365, 440, 263, 30679, 14, 2969, 198, 198, 3886, 262, 640, 339, 5201, 465, 6827, 290, 4504, 284, 262, 1294, 784, 4842, 717, 11, 290, 788, 736, 284, 465, 2802, 447, 247, 82, 17012, 351, 465, 3656, 287, 6094, 784, 262, 5230, 550, 43726, 262, 6817, 286, 465, 670, 13, 35305, 738, 19102, 12, 31343, 4778, 477, 625, 262, 995, 714, 783, 3538, 2018, 2691, 11, 290, 262, 3601, 2831, 550, 2626, 3016, 477, 286, 663, 7395, 13, 198, 198, 1544, 5982, 3770, 355, 257, 5822, 1871, 465, 661, 11, 8662, 1559, 1139, 13, 679, 1364, 355, 1223, 1342, 13, 198, 198, 46445, 32289, 287, 12406, 910, 339, 9658, 5897, 706, 465, 1441, 13, 18023, 1661, 11, 530, 4780, 1297, 502, 11, 262, 691, 835, 345, 714, 1560, 339, 373, 1363, 379, 477, 373, 262, 6283, 8508, 286, 17626, 2082, 701, 278, 422, 262, 24296, 13, 198, 198, 1537, 40014, 694, 2492, 447, 247, 83, 3190, 503, 286, 262, 983, 13, 554, 4751, 11, 655, 257, 614, 706, 465, 1441, 11, 339, 5611, 257, 3992, 12, 4774, 278, 3859, 1444, 1168, 641, 333, 19503, 72, 13, 785, 11, 3616, 564, 250, 42595, 11094, 1479, 447, 251, 13, 198, 198, 49945, 276, 7525, 379, 3427, 5465, 2628, 11, 262, 3052, 22103, 25, 564, 250, 1135, 3926, 1975, 612, 318, 645, 884, 1517, 355, 564, 246, 37035, 4046, 11, 447, 247, 691, 1479, 4046, 326, 318, 16563, 416, 262, 4920, 4773, 13, 1168, 641, 333, 19503, 72, 13, 785, 318, 262, 717, 3938, 5257, 290, 5713, 3992, 13662, 10131, 329, 883, 6699, 1895, 284, 8661, 13662, 2233, 284, 564, 246, 45055, 447, 247, 2695, 13, 447, 251, 198, 198, 818, 3090, 284, 663, 991, 11581, 3992, 12, 4774, 278, 3859, 11, 262, 317, 46, 4477, 284, 7715, 1327, 12, 30073, 25231, 286, 2656, 6723, 9285, 832, 663, 2691, 44346, 11, 10467, 24560, 13661, 13, 554, 2472, 11, 484, 7715, 625, 7337, 8714, 287, 1511, 8950, 11, 40014, 694, 1139, 11, 290, 2904, 8155, 511, 898, 12765, 5112, 11, 3867, 262, 2104, 1492, 12, 16894, 1429, 287, 12, 4803, 13, 198, 198, 447, 250, 1532, 340, 373, 510, 284, 502, 314, 447, 247, 67, 1445, 736, 284, 4486, 284, 307, 319, 262, 2166, 3951, 11, 447, 251, 339, 1139, 13, 564, 250, 1537, 340, 447, 247, 82, 407, 8472, 3926, 314, 460, 466, 517, 922, 994, 13, 447, 251, 198, 198, 3260, 674, 2720, 11, 262, 16413, 290, 314, 13488, 329, 40014, 694, 290, 465, 4639, 284, 2834, 1497, 13, 314, 550, 645, 1393, 287, 1708, 606, 784, 407, 287, 257, 39903, 2565, 11, 6949, 784, 475, 314, 14028, 703, 12020, 2936, 546, 281, 19765, 18192, 12267, 8928, 392, 396, 22498, 503, 12703, 1871, 606, 11, 523, 356, 10357, 262, 1542, 2431, 7421, 11, 49289, 832, 44194, 11676, 25747, 319, 18956, 21056, 13, 198, 198, 29011, 11, 314, 8020, 447, 247, 83, 6027, 319, 13852, 278, 656, 40014, 694, 757, 655, 1936, 2431, 706, 7647, 262, 1097, 13, 198, 198, 1544, 2952, 9899, 2241, 616, 43176, 11, 36642, 278, 502, 656, 2972, 5692, 1088, 3240, 13, 679, 1965, 611, 314, 447, 247, 67, 588, 284, 766, 465, 24522, 3108, 13, 775, 1364, 262, 6616, 290, 6807, 3812, 262, 7703, 4518, 7850, 11, 810, 262, 1748, 11513, 281, 1468, 6787, 1627, 329, 18136, 779, 13, 198, 198, 40, 11434, 683, 3690, 262, 4205, 546, 2635, 2097, 4039, 25651, 6542, 19688, 11, 508, 373, 1752, 1444, 257, 564, 250, 11186, 31352, 447, 251, 416, 18496, 29611, 13, 679, 14358, 683, 564, 250, 86, 34190, 866, 447, 251, 3688, 284, 262, 564, 250, 17889, 1754, 6205, 290, 775, 11840, 76, 19725, 447, 251, 339, 1752, 2993, 287, 4486, 26, 546, 3759, 1301, 11, 4150, 339, 3848, 257, 35577, 3785, 319, 262, 3108, 3812, 5854, 11, 13975, 284, 564, 250, 42, 567, 5907, 2584, 287, 3284, 447, 251, 11, 22688, 564, 250, 1169, 1109, 326, 339, 714, 307, 7018, 319, 262, 3275, 339, 5545, 3926, 318, 4457, 2383, 447, 251, 13, 198, 198, 40, 635, 1965, 683, 546, 262, 7786, 4485, 287, 33064, 368, 16233, 6741, 287, 2177, 13, 564, 250, 464, 6771, 1624, 326, 11, 475, 257, 1256, 286, 1661, 6771, 4589, 262, 6741, 2405, 290, 8390, 340, 284, 651, 20242, 11, 447, 251, 339, 1139, 13, 564, 250, 1639, 447, 247, 297, 760, 484, 447, 247, 260, 1107, 1642, 4371, 611, 345, 2342, 262, 4283, 1910, 329, 257, 1664, 326, 46306, 257, 1256, 286, 40905, 74, 14995, 347, 13, 447, 251, 198, 198, 43, 559, 694, 8155, 257, 2156, 617, 640, 1088, 3648, 13, 2750, 788, 11, 465, 2988, 550, 890, 587, 2636, 11, 290, 40014, 694, 550, 1309, 465, 3397, 447, 247, 1363, 2121, 656, 595, 49932, 11, 3381, 2737, 6600, 262, 18570, 290, 772, 262, 2779, 286, 465, 2802, 447, 247, 82, 19132, 13, 198, 198, 46445, 32289, 910, 339, 8365, 1718, 465, 2802, 503, 286, 262, 2156, 13, 679, 4191, 3888, 607, 284, 257, 19167, 1363, 2354, 286, 12406, 11, 37205, 24157, 510, 511, 30985, 290, 644, 6150, 286, 465, 4560, 287, 262, 17012, 13, 317, 1790, 4731, 286, 649, 4393, 3888, 287, 290, 503, 13, 15658, 7819, 1088, 13, 198, 198, 1722, 890, 355, 339, 14768, 319, 465, 1735, 286, 262, 13990, 314, 1839, 470, 423, 284, 1234, 257, 9036, 287, 465, 1986, 3271, 4502, 11, 40014, 694, 338, 4780, 198, 198, 6943, 5085, 994, 287, 7011, 10711, 836, 447, 247, 83, 7564, 465, 1438, 13, 5845, 508, 466, 5298, 511, 26928, 11, 31992, 546, 465, 4568, 11, 6487, 606, 1497, 355, 611, 340, 547, 257, 627, 14232, 286, 428, 32450, 2513, 263, 447, 247, 82, 1613, 13, 1119, 836, 447, 247, 83, 48058, 12819, 1042, 11, 3729, 11, 475, 484, 1839, 447, 247, 83, 1057, 683, 572, 13, 1119, 2461, 465, 6782, 11, 290, 1607, 339, 447, 247, 297, 36564, 378, 13, 564, 250, 1722, 890, 355, 339, 14768, 503, 8326, 616, 12699, 314, 836, 447, 247, 83, 1337, 644, 339, 857, 11, 447, 251, 1139, 3271, 4502, 11, 508, 12216, 1345, 64, 3461, 406, 7305, 290, 42151, 13, 564, 250, 1722, 890, 355, 339, 14768, 319, 465, 1735, 286, 262, 13990, 314, 1839, 447, 247, 83, 423, 284, 1234, 257, 9036, 287, 465, 1986, 13, 447, 251, 198, 198, 43, 559, 694, 7893, 326, 314, 1394, 7011, 10711, 503, 286, 262, 1621, 11, 5149, 502, 257, 13041, 560, 12838, 286, 32625, 278, 257, 10099, 508, 599, 798, 319, 465, 2802, 13, 679, 4952, 502, 340, 2125, 447, 247, 83, 257, 2372, 13, 198, 198, 447, 250, 1532, 314, 2227, 284, 16180, 345, 314, 447, 247, 67, 423, 655, 2077, 345, 503, 2252, 656, 262, 16479, 290, 5954, 503, 616, 31541, 263, 290, 6235, 340, 379, 345, 13, 447, 251, 198, 198, 818, 3648, 11, 257, 18706, 9189, 5103, 8383, 3706, 20824, 312, 978, 399, 4763, 11, 33188, 329, 465, 1862, 1641, 447, 247, 82, 3747, 287, 257, 7702, 1143, 3908, 11, 2540, 491, 38840, 465, 835, 832, 262, 3586, 1429, 329, 281, 1605, 14552, 13, 198, 198, 3260, 1936, 9299, 11, 484, 11406, 287, 12406, 11, 18329, 13, 978, 399, 4763, 1043, 7184, 351, 257, 1957, 5103, 1664, 11, 810, 339, 714, 1234, 465, 11091, 4678, 284, 670, 13, 1629, 717, 484, 5615, 351, 465, 16933, 11, 788, 281, 7962, 286, 511, 898, 11, 290, 788, 257, 1402, 2156, 13, 554, 1853, 11, 484, 17955, 757, 11, 428, 640, 656, 257, 12949, 1363, 319, 257, 5897, 11, 5509, 12, 10837, 4675, 319, 262, 10183, 1735, 286, 262, 1748, 25, 3504, 1398, 11, 18644, 306, 5175, 290, 21658, 2330, 13, 198, 198, 464, 12699, 373, 257, 2085, 290, 262, 18570, 547, 3190, 686, 28734, 832, 13, 24262, 329, 606, 11, 484, 2993, 257, 3516, 508, 714, 4259, 340, 510, 7026, 11, 257, 14297, 15728, 805, 351, 4647, 286, 1998, 2157, 683, 13, 198, 198, 2990, 447, 247, 260, 991, 1762, 319, 262, 12699, 11, 475, 262, 11087, 1909, 318, 287, 45707, 540, 4006, 13, 968, 18570, 13, 968, 7521, 13, 15183, 276, 5205, 286, 1641, 1866, 10938, 422, 262, 7714, 11, 407, 257, 2060, 530, 379, 2326, 13, 679, 3170, 257, 649, 2166, 33179, 13, 679, 31285, 257, 649, 24058, 284, 262, 2166, 3420, 13, 679, 5818, 447, 247, 83, 5201, 262, 17012, 1865, 11, 475, 340, 447, 247, 82, 319, 262, 1351, 13, 1649, 484, 717, 3888, 287, 11, 339, 1139, 11, 339, 1043, 1223, 6283, 866, 612, 13, 198, 198, 447, 250, 1544, 1043, 257, 1492, 3375, 546, 11908, 290, 1243, 11, 447, 251, 465, 3367, 12039, 28485, 1139, 287, 2818, 3594, 11, 34665, 329, 465, 2988, 13, 198, 198, 40, 1265, 611, 314, 460, 766, 340, 13, 978, 399, 4763, 22051, 290, 24621, 3371, 262, 13913, 460, 13, 198, 198, 447, 250, 40, 447, 247, 76, 7926, 11, 616, 1545, 13, 314, 22591, 340, 13, 447, 251, 50256], 'len': 5552}\n",
            "{'ids': [28182, 440, 6, 9131, 42, 1546, 1377, 317, 18609, 4099, 373, 5169, 706, 339, 4481, 3651, 739, 465, 898, 366, 86, 4126, 1, 4590, 319, 262, 17454, 1073, 3418, 12874, 338, 4452, 3203, 2443, 13, 198, 198, 25372, 15416, 11, 2242, 11, 373, 5169, 3217, 319, 281, 4075, 8084, 329, 257, 18609, 13, 198, 198, 30098, 1428, 338, 4590, 373, 4481, 3583, 284, 262, 4217, 15821, 3203, 2443, 355, 636, 286, 4086, 338, 366, 37, 1018, 1800, 286, 262, 3596, 1, 1430, 13, 317, 1790, 981, 1568, 11, 15416, 2540, 10754, 3651, 739, 465, 4590, 11, 21861, 531, 13, 198, 198, 2202, 3217, 11, 262, 4217, 15821, 47832, 1800, 4816, 4481, 262, 1708, 2912, 25, 366, 25341, 286, 262, 47832, 1800, 1810, 15087, 11801, 6515, 4705, 15416, 355, 339, 6807, 656, 281, 7962, 5762, 257, 309, 12, 15600, 13, 45209, 1568, 339, 1625, 736, 2354, 810, 340, 318, 4019, 7370, 11, 5762, 257, 31962, 2064, 15224, 351, 262, 14263, 510, 13, 679, 373, 41979, 1231, 597, 6625, 526, 198, 198, 11712, 3205, 290, 12793, 3497, 2237, 1933, 286, 1479, 4875, 1895, 284, 262, 8114, 26673, 18277, 198, 198, 25883, 531, 326, 981, 484, 423, 587, 1498, 284, 787, 14794, 832, 262, 47832, 1800, 286, 262, 3596, 1430, 11, 428, 318, 262, 717, 640, 262, 4099, 468, 10630, 290, 7271, 7953, 351, 262, 4086, 832, 262, 3203, 2443, 13, 50256], 'len': 229}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = split_dataset.map(\n",
        "    process,\n",
        "    remove_columns=['text'],\n",
        "    desc='tokenizing the splits',\n",
        "    num_proc=NUM_PROC,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b5d2542da8ae46f1ad39e6fbc69fdd14",
            "97219d6fa5ed4a658f16b680e5224d83",
            "a6514b188a424f73803677254f1ab983",
            "79466e6201014fd8a4cfad76a415ce39",
            "8634a384088a4f3491c8fd78a6f4b48a",
            "aad58170a56940469916f9f1de90a031",
            "9a4c98f31eee4c63b74d7fb38051576d",
            "20b08be4f1ff4585802d85543ed2d36f",
            "ccce48e38dfc4fa48ff85b572df237df",
            "f9b40c5a3e504fed80388dc783ee91c1",
            "6493dafee2274868b603cd801801248e",
            "72ea15479a514c9b8421b5b433ac6014",
            "a19a8095f9d241b88443cc1fc0e2bff6",
            "419aed6914fa4310b4c494daeb9ace9c",
            "4947e5bd914e4a8599497b91de610d96",
            "c042d2954a59453592e5d9ac165bceef",
            "034d40327bd74719ae669a628c630c92",
            "c4e7fc3c47444f8393e6c13bb0e08a96",
            "7b96a911c7384c1191a45a786457bf6c",
            "ebc752460c2146ddbd8e5913ad5ec782",
            "eada107d1fbf4bc0ac2e3f6a280a01b7",
            "aadce6e45f254597b0a7ecdc3f63f89d"
          ]
        },
        "id": "QTE6hg2GrW91",
        "outputId": "865cb6bc-ebf4-4dc6-ec2f-de10b4449705"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the splits (num_proc=24):   0%|          | 0/7212392 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5d2542da8ae46f1ad39e6fbc69fdd14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizing the splits (num_proc=24):   0%|          | 0/801377 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72ea15479a514c9b8421b5b433ac6014"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('data', dataset)"
      ],
      "metadata": {
        "id": "QzcGXhfSxEcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# concatenate all the ids in each dataset into one large file we can use for training\n",
        "for split, dset in tokenized.items():\n",
        "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "    filename = os.path.join(os.path.dirname('ds'), f'{split}.bin')\n",
        "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "    total_batches = 1024\n",
        "\n",
        "    idx = 0\n",
        "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "        # Batch together samples for faster write\n",
        "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "        arr_batch = np.concatenate(batch['ids'])\n",
        "        # Write into mmap\n",
        "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "        idx += len(arr_batch)\n",
        "    arr.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A4vJOPcuqXM",
        "outputId": "3f93cdd4-75c4-48da-ae22-1b4db7fdd6f1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "writing train.bin: 100%|██████████| 1024/1024 [04:49<00:00,  3.54it/s]\n",
            "writing dev.bin: 100%|██████████| 1024/1024 [00:28<00:00, 35.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DS"
      ],
      "metadata": {
        "id": "YOR2jjPlxZAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.memmap(os.path.join(os.path.dirname('ds'), 'train.bin'), dtype=np.uint16, mode='r')\n",
        "dev_data = np.memmap(os.path.join(os.path.dirname('ds'), 'dev.bin'), dtype=np.uint16, mode='r')\n",
        "print(f'{train_data.shape=}, {dev_data.shape=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_gU4xJzw-ct",
        "outputId": "0608f77a-4662-4868-e091-f089c1f93762"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data.shape=(8136098388,), dev_data.shape=(903918707,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg8y-yUaz5Bf",
        "outputId": "b1161303-6f56-4a3b-ed6d-301d00f9799d"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([ 3109, 35570, 42743, ..., 13815,    13, 50256], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# str ---> list of integer\n",
        "# encode = tokenizer.encode\n",
        "encode = lambda str: enc.encode(str)\n",
        "\n",
        "# list of integer ---> str\n",
        "decode = enc.decode\n",
        "\n",
        "_test_str = \"adb dfd \\nplace\"\n",
        "\n",
        "# TODO: this fails  because the tokenizer's encoding result doesn't include \"\\n\"\n",
        "# assert _test_str == decode(encode(_test_str).ids)"
      ],
      "metadata": {
        "id": "_wjY-ZmAK_Mt"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size: int, block_size: int):\n",
        "  \"\"\" Sample a batch using Causal style. \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  ix = torch.randint(0, len(data)-block_size, (batch_size,), generator=g_cpu)\n",
        "  # X = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix]).to(device)\n",
        "  X = torch.stack([torch.from_numpy(np.array(data[i:i+block_size], dtype=np.int64)) for i in ix]).to(device)\n",
        "  # X = torch.stack([torch.tensor(data[i:i+block_size].astype(np.int64)) for i in ix]).to(device)\n",
        "  # Y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix]).to(device)\n",
        "  Y = torch.stack([torch.from_numpy(np.array(data[i+1:i+1+block_size], dtype=np.int64)) for i in ix]).to(device)\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "eWvuZyaOwXai"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(train_data, 16, BLOCK_SIZE)\n",
        "\n",
        "for b in range(3):\n",
        "  it = 0\n",
        "  for t in range(X.shape[1]):\n",
        "    print(f'{decode(X[b, :t+1].tolist())} ---> {decode([Y[b, t].item()])}')\n",
        "    it += 1\n",
        "    if it > 7:\n",
        "      break"
      ],
      "metadata": {
        "id": "F2EL4GRCyDcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77bd828-9d4a-4d20-a678-38fdd31cc705"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " me ---> ,\n",
            " me, --->  or\n",
            " me, or --->  my\n",
            " me, or my --->  neighbors\n",
            " me, or my neighbors ---> ,\n",
            " me, or my neighbors, --->  whether\n",
            " me, or my neighbors, whether --->  they\n",
            " me, or my neighbors, whether they --->  are\n",
            " spared --->  the\n",
            " spared the --->  pe\n",
            " spared the pe ---> ll\n",
            " spared the pell ---> -\n",
            " spared the pell- ---> m\n",
            " spared the pell-m ---> ell\n",
            " spared the pell-mell --->  development\n",
            " spared the pell-mell development --->  that\n",
            "14 ---> ]\n",
            "14] --->  This\n",
            "14] This --->  is\n",
            "14] This is --->  especially\n",
            "14] This is especially --->  prevalent\n",
            "14] This is especially prevalent --->  amongst\n",
            "14] This is especially prevalent amongst --->  teenagers\n",
            "14] This is especially prevalent amongst teenagers ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(dev_data, 16, BLOCK_SIZE)\n",
        "\n",
        "for b in range(3):\n",
        "  it = 0\n",
        "  for t in range(X.shape[1]):\n",
        "    print(f'{decode(X[b, :t+1].tolist())} ---> {decode([Y[b, t].item()])}')\n",
        "    it += 1\n",
        "    if it > 7:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIAejnm44YiR",
        "outputId": "1f77b246-5f42-4bbd-b45b-8759284c1e14"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "– ---> cl\n",
            "–cl ---> arity\n",
            "–clarity --->  that\n",
            "–clarity that --->  is\n",
            "–clarity that is --->  particularly\n",
            "–clarity that is particularly --->  necessary\n",
            "–clarity that is particularly necessary --->  in\n",
            "–clarity that is particularly necessary in --->  light\n",
            " Europe ---> .\n",
            " Europe. ---> The\n",
            " Europe.The --->  Norwegian\n",
            " Europe.The Norwegian --->  broadcaster\n",
            " Europe.The Norwegian broadcaster --->  NR\n",
            " Europe.The Norwegian broadcaster NR ---> K\n",
            " Europe.The Norwegian broadcaster NRK --->  has\n",
            " Europe.The Norwegian broadcaster NRK has --->  interviewed\n",
            ", --->  the\n",
            ", the --->  old\n",
            ", the old --->  Marxist\n",
            ", the old Marxist --->  theory\n",
            ", the old Marxist theory ---> .\n",
            ", the old Marxist theory. --->  They\n",
            ", the old Marxist theory. They --->  may\n",
            ", the old Marxist theory. They may --->  be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "bm2SpJ6CfOHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 768 # the dimensionality of the character embedding vectors\n",
        "d_head = 2 * n_embd # the dim of the transformer's head\n",
        "N_HIDDEN = n_embd * 4 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
      ],
      "metadata": {
        "id": "lAE6DUi2qcNO"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model class"
      ],
      "metadata": {
        "id": "o2LWaIYbri0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "  \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "  def __init__(self, ndim: int, bias: bool):\n",
        "    super().__init__()\n",
        "    self.weight = torch.nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "R-mApi-YzvvD"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_in, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_in: dim of input. If this is the immediate next layer of the token\n",
        "        embedding layer, this is the dim of the embedding for a token.\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "    self.key1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.query1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "    self.value1 = torch.nn.Linear(d_in, d_head, bias=False)\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(d_head, d_hidden, bias=True)\n",
        "    self.tanh1 = torch.nn.Tanh()\n",
        "    # Project d_hidden back to d_head as output\n",
        "    self.proj = torch.nn.Linear(d_hidden, d_head, bias=True)\n",
        "\n",
        "    self.ln1 = LayerNorm(d_in, bias=True)\n",
        "    self.ln2 = LayerNorm(d_head, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T, C). The input to the model.\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-2]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    x = self.ln1(x)\n",
        "\n",
        "    k = self.key1(x)  # (B, T, d_head)\n",
        "    q = self.query1(x) # (B, T, d_head)\n",
        "    wei = k @ q.transpose(-2, -1) # (B, T, d_head) @ (B, d_head, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(tril==0, -float('inf')) # (B, T, T)\n",
        "    wei = wei * self.d_head**-0.5\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    v = self.value1(x) # (B, T, d_head)\n",
        "    # This makes the y[:,t,:], to have the information of the embedding (v)\n",
        "    # v[:, u (u<=t), :], but not have the infomration of v[:, w (w>t), :]\n",
        "    #\n",
        "    # 1. Spread v's information at all t to y[:, any t, :]\n",
        "    #\n",
        "    # Because v is on rhs of @, its information at different T are spread out to\n",
        "    # the different T in y\n",
        "    #\n",
        "    # Think about (T, T) @ (T, d_head) = (T, d_head)\n",
        "    #\n",
        "    # a11, a12     b1   a11*b1+a12*b2\n",
        "    # a21, a22  @  b2 = a21*b1+a22*b2\n",
        "    #\n",
        "    # In the result, at T=1, it has b1 and b2, which are the rhs of @'s info at\n",
        "    # different T\n",
        "    #\n",
        "    # 2. Limit y[:, t, :] to not access v[:, w (w>t), :].\n",
        "    #\n",
        "    # This is done by `tril`\n",
        "    y = wei @ v # (B, T, d_head)\n",
        "\n",
        "    y = self.ln2(y)\n",
        "\n",
        "    # It doesn't need tril here, because the lhs and rhs doesn't exchange\n",
        "    # information at different T.\n",
        "    #\n",
        "    # Let's say:\n",
        "    # - input is y (B, T, d_head)\n",
        "    # - Linear(d_in, d_out) is a matrix l(d_in, d_out), here dim is l(d_head, d_hidden)\n",
        "    # - result is z (B, T, d_hidden)\n",
        "    #\n",
        "    # linear(y) = y @ l = z\n",
        "    #\n",
        "    # To simplify, ignore B, T=3, d_head=2, d_hidden=1\n",
        "    #\n",
        "    #         y11 y12         y11*l1+y12*l2\n",
        "    # y @ l = y21 y22 @ l1  = y21*l1+y22*l2\n",
        "    #         y31 y32   l2    y31*l1+y32*l2\n",
        "    #\n",
        "    # We can see z[:, T, :] only contains y[:, T, :]'s info\n",
        "    #\n",
        "    # To summarize this and the previous section\n",
        "    #\n",
        "    # Z = X @ Y\n",
        "    #\n",
        "    # Z[T, :] only contains X[T, :]'s info, doesn't contain X[S != T, :]'s infor\n",
        "    # Z[T, :] contains Y[at any index, :]'s info\n",
        "    y = self.linear1(y) # (B, T, d_hidden)\n",
        "    y = self.tanh1(y) # (B, T, d_hidden)\n",
        "    y = self.proj(y) # (B, T, d_head)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B,T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "LT6iVQP14kXa"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_embd, d_hidden, d_head):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      vocab_size: volabulary size\n",
        "      d_embd: dim of embedding for the token\n",
        "      d_hidden: dim of hidden FFN layers\n",
        "      d_head: dim of the transformer head\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_head = d_head\n",
        "\n",
        "    self.embd = torch.nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=d_embd\n",
        "    )\n",
        "    self.attn1 = AttentionBlock(vocab_size, d_embd, d_hidden, d_head)\n",
        "    self.attn2 = AttentionBlock(vocab_size, d_head, d_hidden, d_head)\n",
        "    self.linear_logit = torch.nn.Linear(d_head, vocab_size, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x: (B, T). The input to the model.\n",
        "      targets: (B, T). When it is not None, the func calculates and return the\n",
        "        loss in additional to other returned item(s)\n",
        "    Returns:\n",
        "      loss: int\n",
        "      logits: (B, T, C)\n",
        "    \"\"\"\n",
        "\n",
        "    T = x.shape[-1]\n",
        "    tril = torch.tril(torch.ones(T, T)).to(device)\n",
        "\n",
        "    xemb = self.embd(x) # (B, T, C)\n",
        "\n",
        "    y = self.attn1(xemb)\n",
        "    y = self.attn2(y)\n",
        "\n",
        "    logits = self.linear_logit(y) # (B, T, vocab_size)\n",
        "    logits = logits.view(-1, logits.shape[-1]) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits, targets.view(-1))\n",
        "\n",
        "    return logits.view(-1, T, logits.shape[1]), loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens: int):\n",
        "    \"\"\"Generate new tokens given idx as context.\n",
        "\n",
        "    Args:\n",
        "      idx: (B, T)\n",
        "      max_new_tokens: number of new tokens to generate\n",
        "\n",
        "    Returns:\n",
        "      (B, T+max_new_tokens)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx, targets=None)  # logits: (B, T, vocab_size), loss: None\n",
        "      logits = logits[:, -1, :] # (B, vocab_size)\n",
        "      prob = F.softmax(logits, dim=1) # (B, vocab_size)\n",
        "      idx_next = torch.multinomial(prob, num_samples=1, replacement=False)  # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "net = Net(vocab_size, d_embd=n_embd, d_hidden=N_HIDDEN, d_head=N_HIDDEN).to(device)"
      ],
      "metadata": {
        "id": "6QusOiY5HEBH"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_total_params = 0\n",
        "\n",
        "for p in net.parameters():\n",
        "  _total_params += p.nelement()\n",
        "\n",
        "print(f'Total params = {_total_params}')"
      ],
      "metadata": {
        "id": "0eMi8TezrkY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48fd6d3-0744-4b73-f6de-53eac36217dc"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params = 266207569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define optimizer"
      ],
      "metadata": {
        "id": "m04BZ-9f26w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5xJsIU2z28us"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5ObPwkVj3MFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 300000\n",
        "batch_size = 16\n",
        "lossi = []\n",
        "lossi_dev = []\n",
        "ud = []\n",
        "log_interval = 50\n",
        "\n",
        "running_loss = 0.0\n",
        "running_loss_dev = 0.0\n",
        "running_loss_steps = 0\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # Forward\n",
        "  Xb, Yb = get_batch(train_data, batch_size, BLOCK_SIZE)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = net(Xb, targets=Yb)\n",
        "\n",
        "  # Loss\n",
        "  # print(f'{outputs.shape=}, {Yb.shape=}')\n",
        "  running_loss += loss.item()\n",
        "  running_loss_steps += 1\n",
        "\n",
        "  # Eval dev DS\n",
        "  Xb_dev, Yb_dev = get_batch(dev_data, batch_size, BLOCK_SIZE)\n",
        "  logits_dev, loss_dev = net(Xb_dev, targets=Yb_dev)\n",
        "  running_loss_dev += loss_dev.item()\n",
        "\n",
        "  # Update\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Track status\n",
        "  if i % log_interval == 0:\n",
        "    print(f'{i}/{max_steps}: training loss={running_loss/running_loss_steps:.4f}, dev loss={running_loss_dev/running_loss_steps:.4f}')\n",
        "    running_loss = 0.0\n",
        "    running_loss_dev = 0.0\n",
        "    running_loss_steps = 0\n",
        "\n",
        "  lossi.append(loss.log10().item())\n",
        "  lossi_dev.append(loss_dev.log10().item())"
      ],
      "metadata": {
        "id": "3BtGf_U93M9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab8d9715-dd9b-4886-f772-da870e903fc9"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/300000: training loss=10.8269, dev loss=10.8256\n",
            "50/300000: training loss=10.3494, dev loss=10.3502\n",
            "100/300000: training loss=9.0323, dev loss=9.0377\n",
            "150/300000: training loss=8.5859, dev loss=8.5333\n",
            "200/300000: training loss=8.3623, dev loss=8.3472\n",
            "250/300000: training loss=8.2259, dev loss=8.2126\n",
            "300/300000: training loss=8.1302, dev loss=8.1023\n",
            "350/300000: training loss=8.0428, dev loss=8.0217\n",
            "400/300000: training loss=7.9412, dev loss=7.9463\n",
            "450/300000: training loss=7.9240, dev loss=7.8979\n",
            "500/300000: training loss=7.8578, dev loss=7.8969\n",
            "550/300000: training loss=7.8315, dev loss=7.8270\n",
            "600/300000: training loss=7.7952, dev loss=7.7882\n",
            "650/300000: training loss=7.7416, dev loss=7.7468\n",
            "700/300000: training loss=7.7268, dev loss=7.6841\n",
            "750/300000: training loss=7.7089, dev loss=7.6791\n",
            "800/300000: training loss=7.6767, dev loss=7.6615\n",
            "850/300000: training loss=7.6413, dev loss=7.6554\n",
            "900/300000: training loss=7.6276, dev loss=7.6116\n",
            "950/300000: training loss=7.6044, dev loss=7.6139\n",
            "1000/300000: training loss=7.5555, dev loss=7.5801\n",
            "1050/300000: training loss=7.5569, dev loss=7.5490\n",
            "1100/300000: training loss=7.5210, dev loss=7.5491\n",
            "1150/300000: training loss=7.5126, dev loss=7.5351\n",
            "1200/300000: training loss=7.5120, dev loss=7.5036\n",
            "1250/300000: training loss=7.4910, dev loss=7.4607\n",
            "1300/300000: training loss=7.4955, dev loss=7.4658\n",
            "1350/300000: training loss=7.4826, dev loss=7.5215\n",
            "1400/300000: training loss=7.5144, dev loss=7.4658\n",
            "1450/300000: training loss=7.4773, dev loss=7.4634\n",
            "1500/300000: training loss=7.4491, dev loss=7.4351\n",
            "1550/300000: training loss=7.4487, dev loss=7.3801\n",
            "1600/300000: training loss=7.4008, dev loss=7.4379\n",
            "1650/300000: training loss=7.4791, dev loss=7.4350\n",
            "1700/300000: training loss=7.4015, dev loss=7.4123\n",
            "1750/300000: training loss=7.3944, dev loss=7.4010\n",
            "1800/300000: training loss=7.4213, dev loss=7.3838\n",
            "1850/300000: training loss=7.4159, dev loss=7.3974\n",
            "1900/300000: training loss=7.4020, dev loss=7.3653\n",
            "1950/300000: training loss=7.3979, dev loss=7.3875\n",
            "2000/300000: training loss=7.3590, dev loss=7.3563\n",
            "2050/300000: training loss=7.3245, dev loss=7.3885\n",
            "2100/300000: training loss=7.3340, dev loss=7.3171\n",
            "2150/300000: training loss=7.3976, dev loss=7.3473\n",
            "2200/300000: training loss=7.3679, dev loss=7.3419\n",
            "2250/300000: training loss=7.3310, dev loss=7.3225\n",
            "2300/300000: training loss=7.3840, dev loss=7.3587\n",
            "2350/300000: training loss=7.3514, dev loss=7.3779\n",
            "2400/300000: training loss=7.3259, dev loss=7.3365\n",
            "2450/300000: training loss=7.3628, dev loss=7.3113\n",
            "2500/300000: training loss=7.3226, dev loss=7.3258\n",
            "2550/300000: training loss=7.3522, dev loss=7.3305\n",
            "2600/300000: training loss=7.3367, dev loss=7.3121\n",
            "2650/300000: training loss=7.3170, dev loss=7.3379\n",
            "2700/300000: training loss=7.3369, dev loss=7.3289\n",
            "2750/300000: training loss=7.3076, dev loss=7.3074\n",
            "2800/300000: training loss=7.2531, dev loss=7.3240\n",
            "2850/300000: training loss=7.2838, dev loss=7.3001\n",
            "2900/300000: training loss=7.3165, dev loss=7.3012\n",
            "2950/300000: training loss=7.3093, dev loss=7.2798\n",
            "3000/300000: training loss=7.2973, dev loss=7.2695\n",
            "3050/300000: training loss=7.2665, dev loss=7.2807\n",
            "3100/300000: training loss=7.2723, dev loss=7.2342\n",
            "3150/300000: training loss=7.2534, dev loss=7.2847\n",
            "3200/300000: training loss=7.2597, dev loss=7.2754\n",
            "3250/300000: training loss=7.2623, dev loss=7.2586\n",
            "3300/300000: training loss=7.2392, dev loss=7.2494\n",
            "3350/300000: training loss=7.2489, dev loss=7.2689\n",
            "3400/300000: training loss=7.2837, dev loss=7.2804\n",
            "3450/300000: training loss=7.2767, dev loss=7.1988\n",
            "3500/300000: training loss=7.2334, dev loss=7.2143\n",
            "3550/300000: training loss=7.2326, dev loss=7.2414\n",
            "3600/300000: training loss=7.2313, dev loss=7.2452\n",
            "3650/300000: training loss=7.2750, dev loss=7.2152\n",
            "3700/300000: training loss=7.2628, dev loss=7.2619\n",
            "3750/300000: training loss=7.1981, dev loss=7.2707\n",
            "3800/300000: training loss=7.2499, dev loss=7.2475\n",
            "3850/300000: training loss=7.1968, dev loss=7.2549\n",
            "3900/300000: training loss=7.2191, dev loss=7.2461\n",
            "3950/300000: training loss=7.1914, dev loss=7.1961\n",
            "4000/300000: training loss=7.2402, dev loss=7.2292\n",
            "4050/300000: training loss=7.2160, dev loss=7.1879\n",
            "4100/300000: training loss=7.2301, dev loss=7.2498\n",
            "4150/300000: training loss=7.2215, dev loss=7.2264\n",
            "4200/300000: training loss=7.1973, dev loss=7.2032\n",
            "4250/300000: training loss=7.1734, dev loss=7.2136\n",
            "4300/300000: training loss=7.2126, dev loss=7.1927\n",
            "4350/300000: training loss=7.1843, dev loss=7.2156\n",
            "4400/300000: training loss=7.1712, dev loss=7.2085\n",
            "4450/300000: training loss=7.1580, dev loss=7.1932\n",
            "4500/300000: training loss=7.2341, dev loss=7.1955\n",
            "4550/300000: training loss=7.1685, dev loss=7.2041\n",
            "4600/300000: training loss=7.2039, dev loss=7.2107\n",
            "4650/300000: training loss=7.1740, dev loss=7.1990\n",
            "4700/300000: training loss=7.1760, dev loss=7.1824\n",
            "4750/300000: training loss=7.1775, dev loss=7.1307\n",
            "4800/300000: training loss=7.1792, dev loss=7.1515\n",
            "4850/300000: training loss=7.1893, dev loss=7.2253\n",
            "4900/300000: training loss=7.1544, dev loss=7.1680\n",
            "4950/300000: training loss=7.1727, dev loss=7.1732\n",
            "5000/300000: training loss=7.1351, dev loss=7.1599\n",
            "5050/300000: training loss=7.1851, dev loss=7.1805\n",
            "5100/300000: training loss=7.1572, dev loss=7.1523\n",
            "5150/300000: training loss=7.1316, dev loss=7.1280\n",
            "5200/300000: training loss=7.1981, dev loss=7.1055\n",
            "5250/300000: training loss=7.1745, dev loss=7.1668\n",
            "5300/300000: training loss=7.1340, dev loss=7.1708\n",
            "5350/300000: training loss=7.1195, dev loss=7.1497\n",
            "5400/300000: training loss=7.1553, dev loss=7.1880\n",
            "5450/300000: training loss=7.1389, dev loss=7.1464\n",
            "5500/300000: training loss=7.1874, dev loss=7.1196\n",
            "5550/300000: training loss=7.1243, dev loss=7.1477\n",
            "5600/300000: training loss=7.1130, dev loss=7.1602\n",
            "5650/300000: training loss=7.1137, dev loss=7.1087\n",
            "5700/300000: training loss=7.0978, dev loss=7.1457\n",
            "5750/300000: training loss=7.1029, dev loss=7.1297\n",
            "5800/300000: training loss=7.1240, dev loss=7.1249\n",
            "5850/300000: training loss=7.0933, dev loss=7.1133\n",
            "5900/300000: training loss=7.1229, dev loss=7.1085\n",
            "5950/300000: training loss=7.1286, dev loss=7.1383\n",
            "6000/300000: training loss=7.0657, dev loss=7.1098\n",
            "6050/300000: training loss=7.1178, dev loss=7.1102\n",
            "6100/300000: training loss=7.0985, dev loss=7.1206\n",
            "6150/300000: training loss=7.1183, dev loss=7.0824\n",
            "6200/300000: training loss=7.1039, dev loss=7.1181\n",
            "6250/300000: training loss=7.1102, dev loss=7.0932\n",
            "6300/300000: training loss=7.0765, dev loss=7.1089\n",
            "6350/300000: training loss=7.1391, dev loss=7.0941\n",
            "6400/300000: training loss=7.1091, dev loss=7.0952\n",
            "6450/300000: training loss=7.0700, dev loss=7.0678\n",
            "6500/300000: training loss=7.1241, dev loss=7.1029\n",
            "6550/300000: training loss=7.0876, dev loss=7.0681\n",
            "6600/300000: training loss=7.0930, dev loss=7.0440\n",
            "6650/300000: training loss=7.0434, dev loss=7.1141\n",
            "6700/300000: training loss=7.0752, dev loss=7.0708\n",
            "6750/300000: training loss=7.0948, dev loss=7.1280\n",
            "6800/300000: training loss=7.1001, dev loss=7.0886\n",
            "6850/300000: training loss=7.0620, dev loss=7.0704\n",
            "6900/300000: training loss=7.0731, dev loss=7.0749\n",
            "6950/300000: training loss=7.0799, dev loss=7.0763\n",
            "7000/300000: training loss=7.0774, dev loss=7.0570\n",
            "7050/300000: training loss=7.0361, dev loss=7.0938\n",
            "7100/300000: training loss=7.0645, dev loss=7.0584\n",
            "7150/300000: training loss=7.1155, dev loss=7.0384\n",
            "7200/300000: training loss=7.0717, dev loss=7.0093\n",
            "7250/300000: training loss=7.0220, dev loss=7.0557\n",
            "7300/300000: training loss=7.0377, dev loss=7.0499\n",
            "7350/300000: training loss=7.0493, dev loss=7.0737\n",
            "7400/300000: training loss=7.0649, dev loss=7.0809\n",
            "7450/300000: training loss=7.0384, dev loss=7.0197\n",
            "7500/300000: training loss=7.0447, dev loss=7.0669\n",
            "7550/300000: training loss=7.0627, dev loss=7.0375\n",
            "7600/300000: training loss=7.0584, dev loss=7.0774\n",
            "7650/300000: training loss=7.0518, dev loss=7.0144\n",
            "7700/300000: training loss=7.0636, dev loss=7.0599\n",
            "7750/300000: training loss=7.0712, dev loss=7.0429\n",
            "7800/300000: training loss=6.9906, dev loss=7.0453\n",
            "7850/300000: training loss=7.0313, dev loss=7.0681\n",
            "7900/300000: training loss=7.0084, dev loss=7.0205\n",
            "7950/300000: training loss=6.9940, dev loss=7.0175\n",
            "8000/300000: training loss=6.9971, dev loss=7.0230\n",
            "8050/300000: training loss=7.0099, dev loss=7.0075\n",
            "8100/300000: training loss=7.0288, dev loss=7.0158\n",
            "8150/300000: training loss=7.0281, dev loss=7.0001\n",
            "8200/300000: training loss=6.9967, dev loss=6.9785\n",
            "8250/300000: training loss=7.0107, dev loss=7.0014\n",
            "8300/300000: training loss=7.0562, dev loss=7.0089\n",
            "8350/300000: training loss=6.9718, dev loss=6.9682\n",
            "8400/300000: training loss=7.0159, dev loss=7.0128\n",
            "8450/300000: training loss=6.9716, dev loss=7.0147\n",
            "8500/300000: training loss=6.9837, dev loss=6.9970\n",
            "8550/300000: training loss=7.0012, dev loss=7.0156\n",
            "8600/300000: training loss=7.0193, dev loss=6.9662\n",
            "8650/300000: training loss=6.9492, dev loss=7.0010\n",
            "8700/300000: training loss=6.9899, dev loss=6.9853\n",
            "8750/300000: training loss=6.9695, dev loss=6.9558\n",
            "8800/300000: training loss=6.9747, dev loss=6.9480\n",
            "8850/300000: training loss=6.9638, dev loss=6.9754\n",
            "8900/300000: training loss=6.9829, dev loss=6.9306\n",
            "8950/300000: training loss=6.9595, dev loss=7.0052\n",
            "9000/300000: training loss=6.9410, dev loss=6.9769\n",
            "9050/300000: training loss=6.9289, dev loss=6.9627\n",
            "9100/300000: training loss=6.9471, dev loss=6.9345\n",
            "9150/300000: training loss=6.9639, dev loss=6.9481\n",
            "9200/300000: training loss=7.0043, dev loss=7.0215\n",
            "9250/300000: training loss=6.9327, dev loss=6.9885\n",
            "9300/300000: training loss=6.9582, dev loss=6.9088\n",
            "9350/300000: training loss=6.9121, dev loss=6.9315\n",
            "9400/300000: training loss=6.9748, dev loss=6.9741\n",
            "9450/300000: training loss=6.9349, dev loss=6.9848\n",
            "9500/300000: training loss=6.9454, dev loss=6.9310\n",
            "9550/300000: training loss=6.9175, dev loss=6.9119\n",
            "9600/300000: training loss=6.9075, dev loss=6.9772\n",
            "9650/300000: training loss=6.9084, dev loss=6.9202\n",
            "9700/300000: training loss=6.9778, dev loss=6.9190\n",
            "9750/300000: training loss=6.9343, dev loss=6.9256\n",
            "9800/300000: training loss=6.8980, dev loss=6.9115\n",
            "9850/300000: training loss=6.9214, dev loss=6.9387\n",
            "9900/300000: training loss=6.9354, dev loss=6.9117\n",
            "9950/300000: training loss=6.9189, dev loss=6.9311\n",
            "10000/300000: training loss=6.8953, dev loss=6.9263\n",
            "10050/300000: training loss=6.9286, dev loss=6.9216\n",
            "10100/300000: training loss=6.9381, dev loss=6.9196\n",
            "10150/300000: training loss=6.8711, dev loss=6.8997\n",
            "10200/300000: training loss=6.8937, dev loss=6.9167\n",
            "10250/300000: training loss=6.9413, dev loss=6.9100\n",
            "10300/300000: training loss=6.9050, dev loss=6.9612\n",
            "10350/300000: training loss=6.8888, dev loss=6.9499\n",
            "10400/300000: training loss=6.8658, dev loss=6.8961\n",
            "10450/300000: training loss=6.8931, dev loss=6.8438\n",
            "10500/300000: training loss=6.8672, dev loss=6.9291\n",
            "10550/300000: training loss=6.8674, dev loss=6.9286\n",
            "10600/300000: training loss=6.9119, dev loss=6.9036\n",
            "10650/300000: training loss=6.9186, dev loss=6.9327\n",
            "10700/300000: training loss=6.8411, dev loss=6.8596\n",
            "10750/300000: training loss=6.8832, dev loss=6.9251\n",
            "10800/300000: training loss=6.9097, dev loss=6.9130\n",
            "10850/300000: training loss=6.8804, dev loss=6.8837\n",
            "10900/300000: training loss=6.8691, dev loss=6.9044\n",
            "10950/300000: training loss=6.8750, dev loss=6.8898\n",
            "11000/300000: training loss=6.9156, dev loss=6.8441\n",
            "11050/300000: training loss=6.8787, dev loss=6.8539\n",
            "11100/300000: training loss=6.8712, dev loss=6.8326\n",
            "11150/300000: training loss=6.8786, dev loss=6.8367\n",
            "11200/300000: training loss=6.8659, dev loss=6.8289\n",
            "11250/300000: training loss=6.8352, dev loss=6.8859\n",
            "11300/300000: training loss=6.9070, dev loss=6.8981\n",
            "11350/300000: training loss=6.8519, dev loss=6.8609\n",
            "11400/300000: training loss=6.8629, dev loss=6.8243\n",
            "11450/300000: training loss=6.8940, dev loss=6.8407\n",
            "11500/300000: training loss=6.8417, dev loss=6.8434\n",
            "11550/300000: training loss=6.8509, dev loss=6.8397\n",
            "11600/300000: training loss=6.8340, dev loss=6.8266\n",
            "11650/300000: training loss=6.8689, dev loss=6.8308\n",
            "11700/300000: training loss=6.9038, dev loss=6.8606\n",
            "11750/300000: training loss=6.8744, dev loss=6.8580\n",
            "11800/300000: training loss=6.9148, dev loss=6.8525\n",
            "11850/300000: training loss=6.8035, dev loss=6.8932\n",
            "11900/300000: training loss=6.7805, dev loss=6.8459\n",
            "11950/300000: training loss=6.7963, dev loss=6.8415\n",
            "12000/300000: training loss=6.8623, dev loss=6.8221\n",
            "12050/300000: training loss=6.8599, dev loss=6.8599\n",
            "12100/300000: training loss=6.8842, dev loss=6.8259\n",
            "12150/300000: training loss=6.8662, dev loss=6.8078\n",
            "12200/300000: training loss=6.8315, dev loss=6.8451\n",
            "12250/300000: training loss=6.8511, dev loss=6.8157\n",
            "12300/300000: training loss=6.8023, dev loss=6.8245\n",
            "12350/300000: training loss=6.7895, dev loss=6.8242\n",
            "12400/300000: training loss=6.8447, dev loss=6.8480\n",
            "12450/300000: training loss=6.8347, dev loss=6.8597\n",
            "12500/300000: training loss=6.8157, dev loss=6.8080\n",
            "12550/300000: training loss=6.7834, dev loss=6.8433\n",
            "12600/300000: training loss=6.8186, dev loss=6.7823\n",
            "12650/300000: training loss=6.7949, dev loss=6.8812\n",
            "12700/300000: training loss=6.8298, dev loss=6.8572\n",
            "12750/300000: training loss=6.8296, dev loss=6.7906\n",
            "12800/300000: training loss=6.8251, dev loss=6.8125\n",
            "12850/300000: training loss=6.8621, dev loss=6.8400\n",
            "12900/300000: training loss=6.7768, dev loss=6.7976\n",
            "12950/300000: training loss=6.8294, dev loss=6.8057\n",
            "13000/300000: training loss=6.7990, dev loss=6.8217\n",
            "13050/300000: training loss=6.8250, dev loss=6.8044\n",
            "13100/300000: training loss=6.8184, dev loss=6.7964\n",
            "13150/300000: training loss=6.7922, dev loss=6.7995\n",
            "13200/300000: training loss=6.8162, dev loss=6.8312\n",
            "13250/300000: training loss=6.7911, dev loss=6.7765\n",
            "13300/300000: training loss=6.8310, dev loss=6.7413\n",
            "13350/300000: training loss=6.8510, dev loss=6.7680\n",
            "13400/300000: training loss=6.7991, dev loss=6.8330\n",
            "13450/300000: training loss=6.7828, dev loss=6.8496\n",
            "13500/300000: training loss=6.7908, dev loss=6.7781\n",
            "13550/300000: training loss=6.7726, dev loss=6.7791\n",
            "13600/300000: training loss=6.8460, dev loss=6.8135\n",
            "13650/300000: training loss=6.7727, dev loss=6.8241\n",
            "13700/300000: training loss=6.7948, dev loss=6.7769\n",
            "13750/300000: training loss=6.7884, dev loss=6.7772\n",
            "13800/300000: training loss=6.8418, dev loss=6.7296\n",
            "13850/300000: training loss=6.7748, dev loss=6.7948\n",
            "13900/300000: training loss=6.7787, dev loss=6.7570\n",
            "13950/300000: training loss=6.7970, dev loss=6.7485\n",
            "14000/300000: training loss=6.8087, dev loss=6.7784\n",
            "14050/300000: training loss=6.7344, dev loss=6.7462\n",
            "14100/300000: training loss=6.8000, dev loss=6.8023\n",
            "14150/300000: training loss=6.7242, dev loss=6.7461\n",
            "14200/300000: training loss=6.7750, dev loss=6.7731\n",
            "14250/300000: training loss=6.7619, dev loss=6.7763\n",
            "14300/300000: training loss=6.7277, dev loss=6.7807\n",
            "14350/300000: training loss=6.7614, dev loss=6.7939\n",
            "14400/300000: training loss=6.7140, dev loss=6.8037\n",
            "14450/300000: training loss=6.7420, dev loss=6.7144\n",
            "14500/300000: training loss=6.7759, dev loss=6.7691\n",
            "14550/300000: training loss=6.7031, dev loss=6.7839\n",
            "14600/300000: training loss=6.7365, dev loss=6.7920\n",
            "14650/300000: training loss=6.7655, dev loss=6.7633\n",
            "14700/300000: training loss=6.7128, dev loss=6.7180\n",
            "14750/300000: training loss=6.7047, dev loss=6.7571\n",
            "14800/300000: training loss=6.7828, dev loss=6.7528\n",
            "14850/300000: training loss=6.7278, dev loss=6.7739\n",
            "14900/300000: training loss=6.7401, dev loss=6.7398\n",
            "14950/300000: training loss=6.7869, dev loss=6.7066\n",
            "15000/300000: training loss=6.7448, dev loss=6.7271\n",
            "15050/300000: training loss=6.7275, dev loss=6.7350\n",
            "15100/300000: training loss=6.7753, dev loss=6.7625\n",
            "15150/300000: training loss=6.7305, dev loss=6.7575\n",
            "15200/300000: training loss=6.7473, dev loss=6.7704\n",
            "15250/300000: training loss=6.7266, dev loss=6.7324\n",
            "15300/300000: training loss=6.7473, dev loss=6.7481\n",
            "15350/300000: training loss=6.7310, dev loss=6.7148\n",
            "15400/300000: training loss=6.7071, dev loss=6.7370\n",
            "15450/300000: training loss=6.7539, dev loss=6.6935\n",
            "15500/300000: training loss=6.7008, dev loss=6.7229\n",
            "15550/300000: training loss=6.7092, dev loss=6.7103\n",
            "15600/300000: training loss=6.7383, dev loss=6.7813\n",
            "15650/300000: training loss=6.7248, dev loss=6.7412\n",
            "15700/300000: training loss=6.7187, dev loss=6.7290\n",
            "15750/300000: training loss=6.7380, dev loss=6.6932\n",
            "15800/300000: training loss=6.7155, dev loss=6.7315\n",
            "15850/300000: training loss=6.7053, dev loss=6.7063\n",
            "15900/300000: training loss=6.7603, dev loss=6.7283\n",
            "15950/300000: training loss=6.6862, dev loss=6.7152\n",
            "16000/300000: training loss=6.7368, dev loss=6.7286\n",
            "16050/300000: training loss=6.7124, dev loss=6.7490\n",
            "16100/300000: training loss=6.7197, dev loss=6.7201\n",
            "16150/300000: training loss=6.6957, dev loss=6.6965\n",
            "16200/300000: training loss=6.6890, dev loss=6.7296\n",
            "16250/300000: training loss=6.7300, dev loss=6.7038\n",
            "16300/300000: training loss=6.6714, dev loss=6.7173\n",
            "16350/300000: training loss=6.6743, dev loss=6.7167\n",
            "16400/300000: training loss=6.6876, dev loss=6.7117\n",
            "16450/300000: training loss=6.6525, dev loss=6.6936\n",
            "16500/300000: training loss=6.7083, dev loss=6.6945\n",
            "16550/300000: training loss=6.6862, dev loss=6.6961\n",
            "16600/300000: training loss=6.7565, dev loss=6.7480\n",
            "16650/300000: training loss=6.7364, dev loss=6.6819\n",
            "16700/300000: training loss=6.7575, dev loss=6.7093\n",
            "16750/300000: training loss=6.6497, dev loss=6.6484\n",
            "16800/300000: training loss=6.6989, dev loss=6.7061\n",
            "16850/300000: training loss=6.7073, dev loss=6.7031\n",
            "16900/300000: training loss=6.6929, dev loss=6.6927\n",
            "16950/300000: training loss=6.6698, dev loss=6.6560\n",
            "17000/300000: training loss=6.6889, dev loss=6.6946\n",
            "17050/300000: training loss=6.6897, dev loss=6.7192\n",
            "17100/300000: training loss=6.6902, dev loss=6.6653\n",
            "17150/300000: training loss=6.7308, dev loss=6.6659\n",
            "17200/300000: training loss=6.6822, dev loss=6.6439\n",
            "17250/300000: training loss=6.6701, dev loss=6.6636\n",
            "17300/300000: training loss=6.7180, dev loss=6.6946\n",
            "17350/300000: training loss=6.7221, dev loss=6.6847\n",
            "17400/300000: training loss=6.6953, dev loss=6.7241\n",
            "17450/300000: training loss=6.6994, dev loss=6.7006\n",
            "17500/300000: training loss=6.6440, dev loss=6.6734\n",
            "17550/300000: training loss=6.6639, dev loss=6.6748\n",
            "17600/300000: training loss=6.6952, dev loss=6.6793\n",
            "17650/300000: training loss=6.6582, dev loss=6.6566\n",
            "17700/300000: training loss=6.7099, dev loss=6.6463\n",
            "17750/300000: training loss=6.6833, dev loss=6.6343\n",
            "17800/300000: training loss=6.6751, dev loss=6.6814\n",
            "17850/300000: training loss=6.7087, dev loss=6.6545\n",
            "17900/300000: training loss=6.6336, dev loss=6.6860\n",
            "17950/300000: training loss=6.6718, dev loss=6.6535\n",
            "18000/300000: training loss=6.6934, dev loss=6.6108\n",
            "18050/300000: training loss=6.6145, dev loss=6.6839\n",
            "18100/300000: training loss=6.6519, dev loss=6.6488\n",
            "18150/300000: training loss=6.6330, dev loss=6.6777\n",
            "18200/300000: training loss=6.6705, dev loss=6.6862\n",
            "18250/300000: training loss=6.6412, dev loss=6.6418\n",
            "18300/300000: training loss=6.6447, dev loss=6.6758\n",
            "18350/300000: training loss=6.6401, dev loss=6.6721\n",
            "18400/300000: training loss=6.6350, dev loss=6.6348\n",
            "18450/300000: training loss=6.6725, dev loss=6.6848\n",
            "18500/300000: training loss=6.6144, dev loss=6.7032\n",
            "18550/300000: training loss=6.6817, dev loss=6.6582\n",
            "18600/300000: training loss=6.6274, dev loss=6.6421\n",
            "18650/300000: training loss=6.6748, dev loss=6.6566\n",
            "18700/300000: training loss=6.6923, dev loss=6.6802\n",
            "18750/300000: training loss=6.6511, dev loss=6.6348\n",
            "18800/300000: training loss=6.6609, dev loss=6.6050\n",
            "18850/300000: training loss=6.6261, dev loss=6.6678\n",
            "18900/300000: training loss=6.6462, dev loss=6.6162\n",
            "18950/300000: training loss=6.6281, dev loss=6.5993\n",
            "19000/300000: training loss=6.6132, dev loss=6.6502\n",
            "19050/300000: training loss=6.6380, dev loss=6.6130\n",
            "19100/300000: training loss=6.6684, dev loss=6.6107\n",
            "19150/300000: training loss=6.6408, dev loss=6.6088\n",
            "19200/300000: training loss=6.6264, dev loss=6.6580\n",
            "19250/300000: training loss=6.5990, dev loss=6.6668\n",
            "19300/300000: training loss=6.6462, dev loss=6.6068\n",
            "19350/300000: training loss=6.6074, dev loss=6.6752\n",
            "19400/300000: training loss=6.5981, dev loss=6.6187\n",
            "19450/300000: training loss=6.6251, dev loss=6.6335\n",
            "19500/300000: training loss=6.6282, dev loss=6.6153\n",
            "19550/300000: training loss=6.6142, dev loss=6.6089\n",
            "19600/300000: training loss=6.6760, dev loss=6.6388\n",
            "19650/300000: training loss=6.5948, dev loss=6.6146\n",
            "19700/300000: training loss=6.6432, dev loss=6.6851\n",
            "19750/300000: training loss=6.5449, dev loss=6.6220\n",
            "19800/300000: training loss=6.6204, dev loss=6.6356\n",
            "19850/300000: training loss=6.6388, dev loss=6.6216\n",
            "19900/300000: training loss=6.6051, dev loss=6.6311\n",
            "19950/300000: training loss=6.6032, dev loss=6.6320\n",
            "20000/300000: training loss=6.5905, dev loss=6.6680\n",
            "20050/300000: training loss=6.6140, dev loss=6.5896\n",
            "20100/300000: training loss=6.6242, dev loss=6.5869\n",
            "20150/300000: training loss=6.6335, dev loss=6.5801\n",
            "20200/300000: training loss=6.6439, dev loss=6.5898\n",
            "20250/300000: training loss=6.6394, dev loss=6.6729\n",
            "20300/300000: training loss=6.6336, dev loss=6.5925\n",
            "20350/300000: training loss=6.5891, dev loss=6.6523\n",
            "20400/300000: training loss=6.6174, dev loss=6.6081\n",
            "20450/300000: training loss=6.6197, dev loss=6.6640\n",
            "20500/300000: training loss=6.5562, dev loss=6.6288\n",
            "20550/300000: training loss=6.6167, dev loss=6.6219\n",
            "20600/300000: training loss=6.6067, dev loss=6.6330\n",
            "20650/300000: training loss=6.5596, dev loss=6.6143\n",
            "20700/300000: training loss=6.5955, dev loss=6.5801\n",
            "20750/300000: training loss=6.5528, dev loss=6.5819\n",
            "20800/300000: training loss=6.5729, dev loss=6.5503\n",
            "20850/300000: training loss=6.6125, dev loss=6.5873\n",
            "20900/300000: training loss=6.5903, dev loss=6.6425\n",
            "20950/300000: training loss=6.5770, dev loss=6.6388\n",
            "21000/300000: training loss=6.5535, dev loss=6.5479\n",
            "21050/300000: training loss=6.5633, dev loss=6.5726\n",
            "21100/300000: training loss=6.5835, dev loss=6.6196\n",
            "21150/300000: training loss=6.5397, dev loss=6.6074\n",
            "21200/300000: training loss=6.5415, dev loss=6.6374\n",
            "21250/300000: training loss=6.5851, dev loss=6.6142\n",
            "21300/300000: training loss=6.6252, dev loss=6.5919\n",
            "21350/300000: training loss=6.5671, dev loss=6.6305\n",
            "21400/300000: training loss=6.5917, dev loss=6.5928\n",
            "21450/300000: training loss=6.5645, dev loss=6.6073\n",
            "21500/300000: training loss=6.6218, dev loss=6.5747\n",
            "21550/300000: training loss=6.5469, dev loss=6.5861\n",
            "21600/300000: training loss=6.6014, dev loss=6.5009\n",
            "21650/300000: training loss=6.5673, dev loss=6.6190\n",
            "21700/300000: training loss=6.5792, dev loss=6.5879\n",
            "21750/300000: training loss=6.5482, dev loss=6.5561\n",
            "21800/300000: training loss=6.5620, dev loss=6.6152\n",
            "21850/300000: training loss=6.5609, dev loss=6.5962\n",
            "21900/300000: training loss=6.5514, dev loss=6.5842\n",
            "21950/300000: training loss=6.6143, dev loss=6.5811\n",
            "22000/300000: training loss=6.5829, dev loss=6.5828\n",
            "22050/300000: training loss=6.5981, dev loss=6.5749\n",
            "22100/300000: training loss=6.6070, dev loss=6.5696\n",
            "22150/300000: training loss=6.5563, dev loss=6.5458\n",
            "22200/300000: training loss=6.5530, dev loss=6.6133\n",
            "22250/300000: training loss=6.6206, dev loss=6.6109\n",
            "22300/300000: training loss=6.6207, dev loss=6.5704\n",
            "22350/300000: training loss=6.5552, dev loss=6.6168\n",
            "22400/300000: training loss=6.5666, dev loss=6.6093\n",
            "22450/300000: training loss=6.5620, dev loss=6.5906\n",
            "22500/300000: training loss=6.5850, dev loss=6.5279\n",
            "22550/300000: training loss=6.6399, dev loss=6.5979\n",
            "22600/300000: training loss=6.5264, dev loss=6.5430\n",
            "22650/300000: training loss=6.5978, dev loss=6.5603\n",
            "22700/300000: training loss=6.5931, dev loss=6.5656\n",
            "22750/300000: training loss=6.5772, dev loss=6.5549\n",
            "22800/300000: training loss=6.5668, dev loss=6.5617\n",
            "22850/300000: training loss=6.5954, dev loss=6.5248\n",
            "22900/300000: training loss=6.6032, dev loss=6.5260\n",
            "22950/300000: training loss=6.5916, dev loss=6.5440\n",
            "23000/300000: training loss=6.5947, dev loss=6.5337\n",
            "23050/300000: training loss=6.5475, dev loss=6.6040\n",
            "23100/300000: training loss=6.5524, dev loss=6.5206\n",
            "23150/300000: training loss=6.5045, dev loss=6.5371\n",
            "23200/300000: training loss=6.5637, dev loss=6.5922\n",
            "23250/300000: training loss=6.6175, dev loss=6.5587\n",
            "23300/300000: training loss=6.5695, dev loss=6.5588\n",
            "23350/300000: training loss=6.5780, dev loss=6.5241\n",
            "23400/300000: training loss=6.5814, dev loss=6.6078\n",
            "23450/300000: training loss=6.5576, dev loss=6.6088\n",
            "23500/300000: training loss=6.5858, dev loss=6.5609\n",
            "23550/300000: training loss=6.5847, dev loss=6.5264\n",
            "23600/300000: training loss=6.5728, dev loss=6.5992\n",
            "23650/300000: training loss=6.5459, dev loss=6.5866\n",
            "23700/300000: training loss=6.4992, dev loss=6.5436\n",
            "23750/300000: training loss=6.5103, dev loss=6.5972\n",
            "23800/300000: training loss=6.5947, dev loss=6.5520\n",
            "23850/300000: training loss=6.5694, dev loss=6.4968\n",
            "23900/300000: training loss=6.5295, dev loss=6.5172\n",
            "23950/300000: training loss=6.5677, dev loss=6.4942\n",
            "24000/300000: training loss=6.5390, dev loss=6.5835\n",
            "24050/300000: training loss=6.5949, dev loss=6.6078\n",
            "24100/300000: training loss=6.5265, dev loss=6.5308\n",
            "24150/300000: training loss=6.5852, dev loss=6.5450\n",
            "24200/300000: training loss=6.5404, dev loss=6.5434\n",
            "24250/300000: training loss=6.5628, dev loss=6.5598\n",
            "24300/300000: training loss=6.5505, dev loss=6.5130\n",
            "24350/300000: training loss=6.5319, dev loss=6.4978\n",
            "24400/300000: training loss=6.5197, dev loss=6.5185\n",
            "24450/300000: training loss=6.5737, dev loss=6.5334\n",
            "24500/300000: training loss=6.5710, dev loss=6.5439\n",
            "24550/300000: training loss=6.5479, dev loss=6.5511\n",
            "24600/300000: training loss=6.5441, dev loss=6.5423\n",
            "24650/300000: training loss=6.4967, dev loss=6.5516\n",
            "24700/300000: training loss=6.5122, dev loss=6.5346\n",
            "24750/300000: training loss=6.5488, dev loss=6.5260\n",
            "24800/300000: training loss=6.5284, dev loss=6.5699\n",
            "24850/300000: training loss=6.5431, dev loss=6.5472\n",
            "24900/300000: training loss=6.5833, dev loss=6.5081\n",
            "24950/300000: training loss=6.5332, dev loss=6.5444\n",
            "25000/300000: training loss=6.5251, dev loss=6.4596\n",
            "25050/300000: training loss=6.5207, dev loss=6.5414\n",
            "25100/300000: training loss=6.4788, dev loss=6.5033\n",
            "25150/300000: training loss=6.5260, dev loss=6.4943\n",
            "25200/300000: training loss=6.5175, dev loss=6.5388\n",
            "25250/300000: training loss=6.5374, dev loss=6.5333\n",
            "25300/300000: training loss=6.4950, dev loss=6.5234\n",
            "25350/300000: training loss=6.5099, dev loss=6.5411\n",
            "25400/300000: training loss=6.5444, dev loss=6.5194\n",
            "25450/300000: training loss=6.5279, dev loss=6.4905\n",
            "25500/300000: training loss=6.5412, dev loss=6.5153\n",
            "25550/300000: training loss=6.5319, dev loss=6.5224\n",
            "25600/300000: training loss=6.5310, dev loss=6.5303\n",
            "25650/300000: training loss=6.5174, dev loss=6.5354\n",
            "25700/300000: training loss=6.4798, dev loss=6.4971\n",
            "25750/300000: training loss=6.4932, dev loss=6.4730\n",
            "25800/300000: training loss=6.5049, dev loss=6.5061\n",
            "25850/300000: training loss=6.5376, dev loss=6.4893\n",
            "25900/300000: training loss=6.4892, dev loss=6.5283\n",
            "25950/300000: training loss=6.5233, dev loss=6.5057\n",
            "26000/300000: training loss=6.5009, dev loss=6.5570\n",
            "26050/300000: training loss=6.4692, dev loss=6.5200\n",
            "26100/300000: training loss=6.4910, dev loss=6.4973\n",
            "26150/300000: training loss=6.5038, dev loss=6.4694\n",
            "26200/300000: training loss=6.5113, dev loss=6.5031\n",
            "26250/300000: training loss=6.5198, dev loss=6.5018\n",
            "26300/300000: training loss=6.4912, dev loss=6.5045\n",
            "26350/300000: training loss=6.4997, dev loss=6.5059\n",
            "26400/300000: training loss=6.5299, dev loss=6.5081\n",
            "26450/300000: training loss=6.5069, dev loss=6.5214\n",
            "26500/300000: training loss=6.5179, dev loss=6.4986\n",
            "26550/300000: training loss=6.5361, dev loss=6.5031\n",
            "26600/300000: training loss=6.5407, dev loss=6.4933\n",
            "26650/300000: training loss=6.4956, dev loss=6.4769\n",
            "26700/300000: training loss=6.5006, dev loss=6.4857\n",
            "26750/300000: training loss=6.4767, dev loss=6.5207\n",
            "26800/300000: training loss=6.4926, dev loss=6.5195\n",
            "26850/300000: training loss=6.4577, dev loss=6.5201\n",
            "26900/300000: training loss=6.4649, dev loss=6.5189\n",
            "26950/300000: training loss=6.4447, dev loss=6.5321\n",
            "27000/300000: training loss=6.4920, dev loss=6.5356\n",
            "27050/300000: training loss=6.5061, dev loss=6.5003\n",
            "27100/300000: training loss=6.4558, dev loss=6.4561\n",
            "27150/300000: training loss=6.5289, dev loss=6.4842\n",
            "27200/300000: training loss=6.4655, dev loss=6.5118\n",
            "27250/300000: training loss=6.4894, dev loss=6.4990\n",
            "27300/300000: training loss=6.4666, dev loss=6.4678\n",
            "27350/300000: training loss=6.4617, dev loss=6.5024\n",
            "27400/300000: training loss=6.4818, dev loss=6.4556\n",
            "27450/300000: training loss=6.4741, dev loss=6.5049\n",
            "27500/300000: training loss=6.4957, dev loss=6.4565\n",
            "27550/300000: training loss=6.4890, dev loss=6.5111\n",
            "27600/300000: training loss=6.4967, dev loss=6.5203\n",
            "27650/300000: training loss=6.4643, dev loss=6.4743\n",
            "27700/300000: training loss=6.4216, dev loss=6.5286\n",
            "27750/300000: training loss=6.5218, dev loss=6.4765\n",
            "27800/300000: training loss=6.4981, dev loss=6.4879\n",
            "27850/300000: training loss=6.4974, dev loss=6.4833\n",
            "27900/300000: training loss=6.4843, dev loss=6.5013\n",
            "27950/300000: training loss=6.4705, dev loss=6.4636\n",
            "28000/300000: training loss=6.4750, dev loss=6.5011\n",
            "28050/300000: training loss=6.5387, dev loss=6.4601\n",
            "28100/300000: training loss=6.4849, dev loss=6.5217\n",
            "28150/300000: training loss=6.4745, dev loss=6.4675\n",
            "28200/300000: training loss=6.4495, dev loss=6.4581\n",
            "28250/300000: training loss=6.4435, dev loss=6.4640\n",
            "28300/300000: training loss=6.4560, dev loss=6.4968\n",
            "28350/300000: training loss=6.4635, dev loss=6.4935\n",
            "28400/300000: training loss=6.4878, dev loss=6.4593\n",
            "28450/300000: training loss=6.4490, dev loss=6.4789\n",
            "28500/300000: training loss=6.4285, dev loss=6.4885\n",
            "28550/300000: training loss=6.5249, dev loss=6.5003\n",
            "28600/300000: training loss=6.4743, dev loss=6.4996\n",
            "28650/300000: training loss=6.4920, dev loss=6.4822\n",
            "28700/300000: training loss=6.4773, dev loss=6.4608\n",
            "28750/300000: training loss=6.4659, dev loss=6.4471\n",
            "28800/300000: training loss=6.4840, dev loss=6.4660\n",
            "28850/300000: training loss=6.4762, dev loss=6.4870\n",
            "28900/300000: training loss=6.4347, dev loss=6.4230\n",
            "28950/300000: training loss=6.5051, dev loss=6.4447\n",
            "29000/300000: training loss=6.4583, dev loss=6.4632\n",
            "29050/300000: training loss=6.4679, dev loss=6.4282\n",
            "29100/300000: training loss=6.5108, dev loss=6.4918\n",
            "29150/300000: training loss=6.4745, dev loss=6.5025\n",
            "29200/300000: training loss=6.4904, dev loss=6.4307\n",
            "29250/300000: training loss=6.4347, dev loss=6.4830\n",
            "29300/300000: training loss=6.4641, dev loss=6.3920\n",
            "29350/300000: training loss=6.4376, dev loss=6.4873\n",
            "29400/300000: training loss=6.4788, dev loss=6.4484\n",
            "29450/300000: training loss=6.4789, dev loss=6.4513\n",
            "29500/300000: training loss=6.4606, dev loss=6.4290\n",
            "29550/300000: training loss=6.4739, dev loss=6.4708\n",
            "29600/300000: training loss=6.4635, dev loss=6.4778\n",
            "29650/300000: training loss=6.4205, dev loss=6.4838\n",
            "29700/300000: training loss=6.4198, dev loss=6.4386\n",
            "29750/300000: training loss=6.4823, dev loss=6.4777\n",
            "29800/300000: training loss=6.4703, dev loss=6.4405\n",
            "29850/300000: training loss=6.4463, dev loss=6.4489\n",
            "29900/300000: training loss=6.4720, dev loss=6.4255\n",
            "29950/300000: training loss=6.4686, dev loss=6.4506\n",
            "30000/300000: training loss=6.4119, dev loss=6.5189\n",
            "30050/300000: training loss=6.4756, dev loss=6.4676\n",
            "30100/300000: training loss=6.4504, dev loss=6.5141\n",
            "30150/300000: training loss=6.4462, dev loss=6.4790\n",
            "30200/300000: training loss=6.4983, dev loss=6.4687\n",
            "30250/300000: training loss=6.4752, dev loss=6.4170\n",
            "30300/300000: training loss=6.4474, dev loss=6.4264\n",
            "30350/300000: training loss=6.4157, dev loss=6.4234\n",
            "30400/300000: training loss=6.4361, dev loss=6.4478\n",
            "30450/300000: training loss=6.4284, dev loss=6.4721\n",
            "30500/300000: training loss=6.4843, dev loss=6.4230\n",
            "30550/300000: training loss=6.4713, dev loss=6.3946\n",
            "30600/300000: training loss=6.4732, dev loss=6.4186\n",
            "30650/300000: training loss=6.4138, dev loss=6.4400\n",
            "30700/300000: training loss=6.4290, dev loss=6.4637\n",
            "30750/300000: training loss=6.4544, dev loss=6.4414\n",
            "30800/300000: training loss=6.4504, dev loss=6.4060\n",
            "30850/300000: training loss=6.4303, dev loss=6.4281\n",
            "30900/300000: training loss=6.3870, dev loss=6.4256\n",
            "30950/300000: training loss=6.4117, dev loss=6.4830\n",
            "31000/300000: training loss=6.4615, dev loss=6.4202\n",
            "31050/300000: training loss=6.4729, dev loss=6.4355\n",
            "31100/300000: training loss=6.4515, dev loss=6.4550\n",
            "31150/300000: training loss=6.4188, dev loss=6.4625\n",
            "31200/300000: training loss=6.4073, dev loss=6.4270\n",
            "31250/300000: training loss=6.4219, dev loss=6.4134\n",
            "31300/300000: training loss=6.4110, dev loss=6.4303\n",
            "31350/300000: training loss=6.4112, dev loss=6.4642\n",
            "31400/300000: training loss=6.4149, dev loss=6.4177\n",
            "31450/300000: training loss=6.3921, dev loss=6.4117\n",
            "31500/300000: training loss=6.4484, dev loss=6.4299\n",
            "31550/300000: training loss=6.4465, dev loss=6.4318\n",
            "31600/300000: training loss=6.4090, dev loss=6.4290\n",
            "31650/300000: training loss=6.4334, dev loss=6.3824\n",
            "31700/300000: training loss=6.4470, dev loss=6.3950\n",
            "31750/300000: training loss=6.3967, dev loss=6.4532\n",
            "31800/300000: training loss=6.4417, dev loss=6.4309\n",
            "31850/300000: training loss=6.4297, dev loss=6.4030\n",
            "31900/300000: training loss=6.4247, dev loss=6.4238\n",
            "31950/300000: training loss=6.4143, dev loss=6.4158\n",
            "32000/300000: training loss=6.4119, dev loss=6.4594\n",
            "32050/300000: training loss=6.4243, dev loss=6.4009\n",
            "32100/300000: training loss=6.4479, dev loss=6.4042\n",
            "32150/300000: training loss=6.4620, dev loss=6.4672\n",
            "32200/300000: training loss=6.4430, dev loss=6.4353\n",
            "32250/300000: training loss=6.3653, dev loss=6.4126\n",
            "32300/300000: training loss=6.4027, dev loss=6.4237\n",
            "32350/300000: training loss=6.4227, dev loss=6.4951\n",
            "32400/300000: training loss=6.4182, dev loss=6.4197\n",
            "32450/300000: training loss=6.3415, dev loss=6.4262\n",
            "32500/300000: training loss=6.4271, dev loss=6.4103\n",
            "32550/300000: training loss=6.4345, dev loss=6.4161\n",
            "32600/300000: training loss=6.4102, dev loss=6.3903\n",
            "32650/300000: training loss=6.4552, dev loss=6.3916\n",
            "32700/300000: training loss=6.4071, dev loss=6.4261\n",
            "32750/300000: training loss=6.4498, dev loss=6.4156\n",
            "32800/300000: training loss=6.3908, dev loss=6.4451\n",
            "32850/300000: training loss=6.3709, dev loss=6.3636\n",
            "32900/300000: training loss=6.4156, dev loss=6.4071\n",
            "32950/300000: training loss=6.4525, dev loss=6.4188\n",
            "33000/300000: training loss=6.3999, dev loss=6.3991\n",
            "33050/300000: training loss=6.4017, dev loss=6.4200\n",
            "33100/300000: training loss=6.4371, dev loss=6.4142\n",
            "33150/300000: training loss=6.3797, dev loss=6.4015\n",
            "33200/300000: training loss=6.4014, dev loss=6.4499\n",
            "33250/300000: training loss=6.3767, dev loss=6.4057\n",
            "33300/300000: training loss=6.4515, dev loss=6.4187\n",
            "33350/300000: training loss=6.4074, dev loss=6.3982\n",
            "33400/300000: training loss=6.3804, dev loss=6.4394\n",
            "33450/300000: training loss=6.3955, dev loss=6.4048\n",
            "33500/300000: training loss=6.4128, dev loss=6.4137\n",
            "33550/300000: training loss=6.3772, dev loss=6.4215\n",
            "33600/300000: training loss=6.3768, dev loss=6.3640\n",
            "33650/300000: training loss=6.3648, dev loss=6.4339\n",
            "33700/300000: training loss=6.3941, dev loss=6.3894\n",
            "33750/300000: training loss=6.4244, dev loss=6.4103\n",
            "33800/300000: training loss=6.3588, dev loss=6.3905\n",
            "33850/300000: training loss=6.4134, dev loss=6.3758\n",
            "33900/300000: training loss=6.3785, dev loss=6.3504\n",
            "33950/300000: training loss=6.3427, dev loss=6.3515\n",
            "34000/300000: training loss=6.3389, dev loss=6.4056\n",
            "34050/300000: training loss=6.4003, dev loss=6.4104\n",
            "34100/300000: training loss=6.3752, dev loss=6.3916\n",
            "34150/300000: training loss=6.3633, dev loss=6.3933\n",
            "34200/300000: training loss=6.3547, dev loss=6.3662\n",
            "34250/300000: training loss=6.4398, dev loss=6.4067\n",
            "34300/300000: training loss=6.3924, dev loss=6.4118\n",
            "34350/300000: training loss=6.4063, dev loss=6.3919\n",
            "34400/300000: training loss=6.4091, dev loss=6.3785\n",
            "34450/300000: training loss=6.3918, dev loss=6.3643\n",
            "34500/300000: training loss=6.4211, dev loss=6.3637\n",
            "34550/300000: training loss=6.4117, dev loss=6.3855\n",
            "34600/300000: training loss=6.3874, dev loss=6.4197\n",
            "34650/300000: training loss=6.3782, dev loss=6.3998\n",
            "34700/300000: training loss=6.4118, dev loss=6.3607\n",
            "34750/300000: training loss=6.3612, dev loss=6.3850\n",
            "34800/300000: training loss=6.3827, dev loss=6.4163\n",
            "34850/300000: training loss=6.4102, dev loss=6.3451\n",
            "34900/300000: training loss=6.4096, dev loss=6.3461\n",
            "34950/300000: training loss=6.3722, dev loss=6.3773\n",
            "35000/300000: training loss=6.3561, dev loss=6.3852\n",
            "35050/300000: training loss=6.3879, dev loss=6.3561\n",
            "35100/300000: training loss=6.4082, dev loss=6.3475\n",
            "35150/300000: training loss=6.3863, dev loss=6.3405\n",
            "35200/300000: training loss=6.3620, dev loss=6.3923\n",
            "35250/300000: training loss=6.4274, dev loss=6.3649\n",
            "35300/300000: training loss=6.3820, dev loss=6.4040\n",
            "35350/300000: training loss=6.3713, dev loss=6.4227\n",
            "35400/300000: training loss=6.3801, dev loss=6.3365\n",
            "35450/300000: training loss=6.3434, dev loss=6.3818\n",
            "35500/300000: training loss=6.3702, dev loss=6.3594\n",
            "35550/300000: training loss=6.3882, dev loss=6.3740\n",
            "35600/300000: training loss=6.3477, dev loss=6.3423\n",
            "35650/300000: training loss=6.3616, dev loss=6.3532\n",
            "35700/300000: training loss=6.3670, dev loss=6.3588\n",
            "35750/300000: training loss=6.4077, dev loss=6.3789\n",
            "35800/300000: training loss=6.3934, dev loss=6.2824\n",
            "35850/300000: training loss=6.3755, dev loss=6.3519\n",
            "35900/300000: training loss=6.3773, dev loss=6.3867\n",
            "35950/300000: training loss=6.3617, dev loss=6.3320\n",
            "36000/300000: training loss=6.3085, dev loss=6.3465\n",
            "36050/300000: training loss=6.3562, dev loss=6.3569\n",
            "36100/300000: training loss=6.3201, dev loss=6.3234\n",
            "36150/300000: training loss=6.3991, dev loss=6.3647\n",
            "36200/300000: training loss=6.3229, dev loss=6.3341\n",
            "36250/300000: training loss=6.3262, dev loss=6.3551\n",
            "36300/300000: training loss=6.3852, dev loss=6.3796\n",
            "36350/300000: training loss=6.3306, dev loss=6.3109\n",
            "36400/300000: training loss=6.3567, dev loss=6.3935\n",
            "36450/300000: training loss=6.4167, dev loss=6.3412\n",
            "36500/300000: training loss=6.3174, dev loss=6.3358\n",
            "36550/300000: training loss=6.3356, dev loss=6.3847\n",
            "36600/300000: training loss=6.3226, dev loss=6.3894\n",
            "36650/300000: training loss=6.3646, dev loss=6.3671\n",
            "36700/300000: training loss=6.3570, dev loss=6.3861\n",
            "36750/300000: training loss=6.3149, dev loss=6.3810\n",
            "36800/300000: training loss=6.3365, dev loss=6.3343\n",
            "36850/300000: training loss=6.3276, dev loss=6.3963\n",
            "36900/300000: training loss=6.3299, dev loss=6.3520\n",
            "36950/300000: training loss=6.3010, dev loss=6.4023\n",
            "37000/300000: training loss=6.3474, dev loss=6.3800\n",
            "37050/300000: training loss=6.3306, dev loss=6.3838\n",
            "37100/300000: training loss=6.3534, dev loss=6.3555\n",
            "37150/300000: training loss=6.3380, dev loss=6.3576\n",
            "37200/300000: training loss=6.3130, dev loss=6.3847\n",
            "37250/300000: training loss=6.3644, dev loss=6.3507\n",
            "37300/300000: training loss=6.3852, dev loss=6.3339\n",
            "37350/300000: training loss=6.3426, dev loss=6.3723\n",
            "37400/300000: training loss=6.3550, dev loss=6.2832\n",
            "37450/300000: training loss=6.3194, dev loss=6.3664\n",
            "37500/300000: training loss=6.3229, dev loss=6.3600\n",
            "37550/300000: training loss=6.3152, dev loss=6.3630\n",
            "37600/300000: training loss=6.3221, dev loss=6.3245\n",
            "37650/300000: training loss=6.3136, dev loss=6.3472\n",
            "37700/300000: training loss=6.3224, dev loss=6.3261\n",
            "37750/300000: training loss=6.3580, dev loss=6.3596\n",
            "37800/300000: training loss=6.3534, dev loss=6.3919\n",
            "37850/300000: training loss=6.3448, dev loss=6.3064\n",
            "37900/300000: training loss=6.3251, dev loss=6.3088\n",
            "37950/300000: training loss=6.2967, dev loss=6.3364\n",
            "38000/300000: training loss=6.3293, dev loss=6.3910\n",
            "38050/300000: training loss=6.3562, dev loss=6.2995\n",
            "38100/300000: training loss=6.3219, dev loss=6.3424\n",
            "38150/300000: training loss=6.3419, dev loss=6.3696\n",
            "38200/300000: training loss=6.3544, dev loss=6.3493\n",
            "38250/300000: training loss=6.3079, dev loss=6.3281\n",
            "38300/300000: training loss=6.3408, dev loss=6.3602\n",
            "38350/300000: training loss=6.3599, dev loss=6.3319\n",
            "38400/300000: training loss=6.3017, dev loss=6.3339\n",
            "38450/300000: training loss=6.3193, dev loss=6.3210\n",
            "38500/300000: training loss=6.4131, dev loss=6.3316\n",
            "38550/300000: training loss=6.3314, dev loss=6.3370\n",
            "38600/300000: training loss=6.3284, dev loss=6.2906\n",
            "38650/300000: training loss=6.2705, dev loss=6.3331\n",
            "38700/300000: training loss=6.2926, dev loss=6.3166\n",
            "38750/300000: training loss=6.3198, dev loss=6.3172\n",
            "38800/300000: training loss=6.3596, dev loss=6.3289\n",
            "38850/300000: training loss=6.3490, dev loss=6.3136\n",
            "38900/300000: training loss=6.3303, dev loss=6.3164\n",
            "38950/300000: training loss=6.3351, dev loss=6.3130\n",
            "39000/300000: training loss=6.3012, dev loss=6.3312\n",
            "39050/300000: training loss=6.3052, dev loss=6.3116\n",
            "39100/300000: training loss=6.3504, dev loss=6.3300\n",
            "39150/300000: training loss=6.3480, dev loss=6.3136\n",
            "39200/300000: training loss=6.3350, dev loss=6.2860\n",
            "39250/300000: training loss=6.3470, dev loss=6.2960\n",
            "39300/300000: training loss=6.2978, dev loss=6.3524\n",
            "39350/300000: training loss=6.3191, dev loss=6.3196\n",
            "39400/300000: training loss=6.3023, dev loss=6.3409\n",
            "39450/300000: training loss=6.3166, dev loss=6.2552\n",
            "39500/300000: training loss=6.3371, dev loss=6.3119\n",
            "39550/300000: training loss=6.3448, dev loss=6.3229\n",
            "39600/300000: training loss=6.3324, dev loss=6.3360\n",
            "39650/300000: training loss=6.3217, dev loss=6.2996\n",
            "39700/300000: training loss=6.2981, dev loss=6.2873\n",
            "39750/300000: training loss=6.3283, dev loss=6.2715\n",
            "39800/300000: training loss=6.3453, dev loss=6.2678\n",
            "39850/300000: training loss=6.3338, dev loss=6.3105\n",
            "39900/300000: training loss=6.2388, dev loss=6.3102\n",
            "39950/300000: training loss=6.2446, dev loss=6.3426\n",
            "40000/300000: training loss=6.3131, dev loss=6.3361\n",
            "40050/300000: training loss=6.3003, dev loss=6.3162\n",
            "40100/300000: training loss=6.2958, dev loss=6.3219\n",
            "40150/300000: training loss=6.2673, dev loss=6.3309\n",
            "40200/300000: training loss=6.3030, dev loss=6.3383\n",
            "40250/300000: training loss=6.3113, dev loss=6.3210\n",
            "40300/300000: training loss=6.2941, dev loss=6.3186\n",
            "40350/300000: training loss=6.3220, dev loss=6.2918\n",
            "40400/300000: training loss=6.2845, dev loss=6.2936\n",
            "40450/300000: training loss=6.2679, dev loss=6.2670\n",
            "40500/300000: training loss=6.2903, dev loss=6.3036\n",
            "40550/300000: training loss=6.2355, dev loss=6.3080\n",
            "40600/300000: training loss=6.3502, dev loss=6.2948\n",
            "40650/300000: training loss=6.2885, dev loss=6.3037\n",
            "40700/300000: training loss=6.3244, dev loss=6.2918\n",
            "40750/300000: training loss=6.3252, dev loss=6.3297\n",
            "40800/300000: training loss=6.2768, dev loss=6.3124\n",
            "40850/300000: training loss=6.2885, dev loss=6.2664\n",
            "40900/300000: training loss=6.3172, dev loss=6.3584\n",
            "40950/300000: training loss=6.3007, dev loss=6.3344\n",
            "41000/300000: training loss=6.3092, dev loss=6.2848\n",
            "41050/300000: training loss=6.3124, dev loss=6.2896\n",
            "41100/300000: training loss=6.3015, dev loss=6.3171\n",
            "41150/300000: training loss=6.3097, dev loss=6.2875\n",
            "41200/300000: training loss=6.2722, dev loss=6.2667\n",
            "41250/300000: training loss=6.2942, dev loss=6.3014\n",
            "41300/300000: training loss=6.3042, dev loss=6.2899\n",
            "41350/300000: training loss=6.2559, dev loss=6.2994\n",
            "41400/300000: training loss=6.2764, dev loss=6.3133\n",
            "41450/300000: training loss=6.2687, dev loss=6.2972\n",
            "41500/300000: training loss=6.3411, dev loss=6.2729\n",
            "41550/300000: training loss=6.2638, dev loss=6.2823\n",
            "41600/300000: training loss=6.2624, dev loss=6.2716\n",
            "41650/300000: training loss=6.2466, dev loss=6.2593\n",
            "41700/300000: training loss=6.2533, dev loss=6.2662\n",
            "41750/300000: training loss=6.3068, dev loss=6.2525\n",
            "41800/300000: training loss=6.2418, dev loss=6.3171\n",
            "41850/300000: training loss=6.2967, dev loss=6.2785\n",
            "41900/300000: training loss=6.2866, dev loss=6.3126\n",
            "41950/300000: training loss=6.2478, dev loss=6.2531\n",
            "42000/300000: training loss=6.2812, dev loss=6.2735\n",
            "42050/300000: training loss=6.3097, dev loss=6.2501\n",
            "42100/300000: training loss=6.3256, dev loss=6.2965\n",
            "42150/300000: training loss=6.3148, dev loss=6.2427\n",
            "42200/300000: training loss=6.2669, dev loss=6.2494\n",
            "42250/300000: training loss=6.2819, dev loss=6.2524\n",
            "42300/300000: training loss=6.2816, dev loss=6.3001\n",
            "42350/300000: training loss=6.2576, dev loss=6.2657\n",
            "42400/300000: training loss=6.3137, dev loss=6.2479\n",
            "42450/300000: training loss=6.2996, dev loss=6.2840\n",
            "42500/300000: training loss=6.2768, dev loss=6.2951\n",
            "42550/300000: training loss=6.2975, dev loss=6.2823\n",
            "42600/300000: training loss=6.2454, dev loss=6.2730\n",
            "42650/300000: training loss=6.2729, dev loss=6.3078\n",
            "42700/300000: training loss=6.2833, dev loss=6.2611\n",
            "42750/300000: training loss=6.3000, dev loss=6.2443\n",
            "42800/300000: training loss=6.2509, dev loss=6.3287\n",
            "42850/300000: training loss=6.2291, dev loss=6.2628\n",
            "42900/300000: training loss=6.2602, dev loss=6.2821\n",
            "42950/300000: training loss=6.3199, dev loss=6.2643\n",
            "43000/300000: training loss=6.2569, dev loss=6.2561\n",
            "43050/300000: training loss=6.2392, dev loss=6.2406\n",
            "43100/300000: training loss=6.2348, dev loss=6.2624\n",
            "43150/300000: training loss=6.2789, dev loss=6.2957\n",
            "43200/300000: training loss=6.2215, dev loss=6.2339\n",
            "43250/300000: training loss=6.2616, dev loss=6.2834\n",
            "43300/300000: training loss=6.2590, dev loss=6.2747\n",
            "43350/300000: training loss=6.2865, dev loss=6.2844\n",
            "43400/300000: training loss=6.2283, dev loss=6.2618\n",
            "43450/300000: training loss=6.2390, dev loss=6.2598\n",
            "43500/300000: training loss=6.2652, dev loss=6.2227\n",
            "43550/300000: training loss=6.2755, dev loss=6.2763\n",
            "43600/300000: training loss=6.2468, dev loss=6.2478\n",
            "43650/300000: training loss=6.2287, dev loss=6.2957\n",
            "43700/300000: training loss=6.2632, dev loss=6.3096\n",
            "43750/300000: training loss=6.2394, dev loss=6.2489\n",
            "43800/300000: training loss=6.2873, dev loss=6.2465\n",
            "43850/300000: training loss=6.2489, dev loss=6.2436\n",
            "43900/300000: training loss=6.2603, dev loss=6.2212\n",
            "43950/300000: training loss=6.2725, dev loss=6.2407\n",
            "44000/300000: training loss=6.2500, dev loss=6.2871\n",
            "44050/300000: training loss=6.2866, dev loss=6.2006\n",
            "44100/300000: training loss=6.2342, dev loss=6.2901\n",
            "44150/300000: training loss=6.2523, dev loss=6.2865\n",
            "44200/300000: training loss=6.1978, dev loss=6.2248\n",
            "44250/300000: training loss=6.2129, dev loss=6.2834\n",
            "44300/300000: training loss=6.2469, dev loss=6.2609\n",
            "44350/300000: training loss=6.2622, dev loss=6.2408\n",
            "44400/300000: training loss=6.2698, dev loss=6.3074\n",
            "44450/300000: training loss=6.2649, dev loss=6.2464\n",
            "44500/300000: training loss=6.2786, dev loss=6.2107\n",
            "44550/300000: training loss=6.2687, dev loss=6.2747\n",
            "44600/300000: training loss=6.2614, dev loss=6.2712\n",
            "44650/300000: training loss=6.2285, dev loss=6.2097\n",
            "44700/300000: training loss=6.2388, dev loss=6.2412\n",
            "44750/300000: training loss=6.2452, dev loss=6.2286\n",
            "44800/300000: training loss=6.2490, dev loss=6.2789\n",
            "44850/300000: training loss=6.2367, dev loss=6.2088\n",
            "44900/300000: training loss=6.2160, dev loss=6.2362\n",
            "44950/300000: training loss=6.2545, dev loss=6.2468\n",
            "45000/300000: training loss=6.2357, dev loss=6.1787\n",
            "45050/300000: training loss=6.2019, dev loss=6.2509\n",
            "45100/300000: training loss=6.2578, dev loss=6.2461\n",
            "45150/300000: training loss=6.2135, dev loss=6.2886\n",
            "45200/300000: training loss=6.1918, dev loss=6.2336\n",
            "45250/300000: training loss=6.1988, dev loss=6.1830\n",
            "45300/300000: training loss=6.2326, dev loss=6.2097\n",
            "45350/300000: training loss=6.2259, dev loss=6.2148\n",
            "45400/300000: training loss=6.2385, dev loss=6.2126\n",
            "45450/300000: training loss=6.2451, dev loss=6.2436\n",
            "45500/300000: training loss=6.1885, dev loss=6.2031\n",
            "45550/300000: training loss=6.1831, dev loss=6.2124\n",
            "45600/300000: training loss=6.2438, dev loss=6.2786\n",
            "45650/300000: training loss=6.2169, dev loss=6.1966\n",
            "45700/300000: training loss=6.2280, dev loss=6.2543\n",
            "45750/300000: training loss=6.2236, dev loss=6.2022\n",
            "45800/300000: training loss=6.2601, dev loss=6.2666\n",
            "45850/300000: training loss=6.2051, dev loss=6.2178\n",
            "45900/300000: training loss=6.2142, dev loss=6.1906\n",
            "45950/300000: training loss=6.2136, dev loss=6.2123\n",
            "46000/300000: training loss=6.2088, dev loss=6.1888\n",
            "46050/300000: training loss=6.2251, dev loss=6.2195\n",
            "46100/300000: training loss=6.2115, dev loss=6.1871\n",
            "46150/300000: training loss=6.2295, dev loss=6.2429\n",
            "46200/300000: training loss=6.2025, dev loss=6.2103\n",
            "46250/300000: training loss=6.2064, dev loss=6.2420\n",
            "46300/300000: training loss=6.2339, dev loss=6.2105\n",
            "46350/300000: training loss=6.1983, dev loss=6.2627\n",
            "46400/300000: training loss=6.1877, dev loss=6.2338\n",
            "46450/300000: training loss=6.1724, dev loss=6.2351\n",
            "46500/300000: training loss=6.1891, dev loss=6.2328\n",
            "46550/300000: training loss=6.2531, dev loss=6.2146\n",
            "46600/300000: training loss=6.2152, dev loss=6.2234\n",
            "46650/300000: training loss=6.2164, dev loss=6.2273\n",
            "46700/300000: training loss=6.2045, dev loss=6.1895\n",
            "46750/300000: training loss=6.1884, dev loss=6.1812\n",
            "46800/300000: training loss=6.2479, dev loss=6.1937\n",
            "46850/300000: training loss=6.2614, dev loss=6.1935\n",
            "46900/300000: training loss=6.2283, dev loss=6.2398\n",
            "46950/300000: training loss=6.2413, dev loss=6.1945\n",
            "47000/300000: training loss=6.2402, dev loss=6.2033\n",
            "47050/300000: training loss=6.2334, dev loss=6.2234\n",
            "47100/300000: training loss=6.1783, dev loss=6.2174\n",
            "47150/300000: training loss=6.1868, dev loss=6.1370\n",
            "47200/300000: training loss=6.2447, dev loss=6.1622\n",
            "47250/300000: training loss=6.2300, dev loss=6.2172\n",
            "47300/300000: training loss=6.2417, dev loss=6.2077\n",
            "47350/300000: training loss=6.2022, dev loss=6.1966\n",
            "47400/300000: training loss=6.1822, dev loss=6.2056\n",
            "47450/300000: training loss=6.1942, dev loss=6.1636\n",
            "47500/300000: training loss=6.2220, dev loss=6.2245\n",
            "47550/300000: training loss=6.2052, dev loss=6.2102\n",
            "47600/300000: training loss=6.2076, dev loss=6.2412\n",
            "47650/300000: training loss=6.2340, dev loss=6.2243\n",
            "47700/300000: training loss=6.2359, dev loss=6.2125\n",
            "47750/300000: training loss=6.1709, dev loss=6.2162\n",
            "47800/300000: training loss=6.2134, dev loss=6.2428\n",
            "47850/300000: training loss=6.2153, dev loss=6.2125\n",
            "47900/300000: training loss=6.1675, dev loss=6.2567\n",
            "47950/300000: training loss=6.2234, dev loss=6.2109\n",
            "48000/300000: training loss=6.1963, dev loss=6.2264\n",
            "48050/300000: training loss=6.2324, dev loss=6.2205\n",
            "48100/300000: training loss=6.1953, dev loss=6.2410\n",
            "48150/300000: training loss=6.2104, dev loss=6.2044\n",
            "48200/300000: training loss=6.1929, dev loss=6.2165\n",
            "48250/300000: training loss=6.2027, dev loss=6.2220\n",
            "48300/300000: training loss=6.1272, dev loss=6.1854\n",
            "48350/300000: training loss=6.1903, dev loss=6.2264\n",
            "48400/300000: training loss=6.1420, dev loss=6.1690\n",
            "48450/300000: training loss=6.1996, dev loss=6.2305\n",
            "48500/300000: training loss=6.2040, dev loss=6.1865\n",
            "48550/300000: training loss=6.1819, dev loss=6.2273\n",
            "48600/300000: training loss=6.1803, dev loss=6.1896\n",
            "48650/300000: training loss=6.2020, dev loss=6.2155\n",
            "48700/300000: training loss=6.1507, dev loss=6.1618\n",
            "48750/300000: training loss=6.1669, dev loss=6.1868\n",
            "48800/300000: training loss=6.2170, dev loss=6.1679\n",
            "48850/300000: training loss=6.1319, dev loss=6.1960\n",
            "48900/300000: training loss=6.1756, dev loss=6.1657\n",
            "48950/300000: training loss=6.2006, dev loss=6.1866\n",
            "49000/300000: training loss=6.2210, dev loss=6.1495\n",
            "49050/300000: training loss=6.2119, dev loss=6.1723\n",
            "49100/300000: training loss=6.1586, dev loss=6.2354\n",
            "49150/300000: training loss=6.1409, dev loss=6.1339\n",
            "49200/300000: training loss=6.2053, dev loss=6.1938\n",
            "49250/300000: training loss=6.2128, dev loss=6.2257\n",
            "49300/300000: training loss=6.1532, dev loss=6.1573\n",
            "49350/300000: training loss=6.1517, dev loss=6.1988\n",
            "49400/300000: training loss=6.1617, dev loss=6.1455\n",
            "49450/300000: training loss=6.2063, dev loss=6.1772\n",
            "49500/300000: training loss=6.1790, dev loss=6.1792\n",
            "49550/300000: training loss=6.1846, dev loss=6.1864\n",
            "49600/300000: training loss=6.1934, dev loss=6.1874\n",
            "49650/300000: training loss=6.2253, dev loss=6.2360\n",
            "49700/300000: training loss=6.1314, dev loss=6.1845\n",
            "49750/300000: training loss=6.2077, dev loss=6.1676\n",
            "49800/300000: training loss=6.2091, dev loss=6.1961\n",
            "49850/300000: training loss=6.1499, dev loss=6.1877\n",
            "49900/300000: training loss=6.1719, dev loss=6.1869\n",
            "49950/300000: training loss=6.1804, dev loss=6.1998\n",
            "50000/300000: training loss=6.1927, dev loss=6.1679\n",
            "50050/300000: training loss=6.1892, dev loss=6.1456\n",
            "50100/300000: training loss=6.1732, dev loss=6.1634\n",
            "50150/300000: training loss=6.1655, dev loss=6.1699\n",
            "50200/300000: training loss=6.2011, dev loss=6.1270\n",
            "50250/300000: training loss=6.1747, dev loss=6.1898\n",
            "50300/300000: training loss=6.2173, dev loss=6.1868\n",
            "50350/300000: training loss=6.1431, dev loss=6.1577\n",
            "50400/300000: training loss=6.2218, dev loss=6.1663\n",
            "50450/300000: training loss=6.1598, dev loss=6.2076\n",
            "50500/300000: training loss=6.1364, dev loss=6.1606\n",
            "50550/300000: training loss=6.1661, dev loss=6.1404\n",
            "50600/300000: training loss=6.1219, dev loss=6.1816\n",
            "50650/300000: training loss=6.1818, dev loss=6.1556\n",
            "50700/300000: training loss=6.1354, dev loss=6.1462\n",
            "50750/300000: training loss=6.1663, dev loss=6.1821\n",
            "50800/300000: training loss=6.1922, dev loss=6.1940\n",
            "50850/300000: training loss=6.1187, dev loss=6.1265\n",
            "50900/300000: training loss=6.1425, dev loss=6.1396\n",
            "50950/300000: training loss=6.1310, dev loss=6.1508\n",
            "51000/300000: training loss=6.1484, dev loss=6.1558\n",
            "51050/300000: training loss=6.1668, dev loss=6.1685\n",
            "51100/300000: training loss=6.1591, dev loss=6.1555\n",
            "51150/300000: training loss=6.1296, dev loss=6.0804\n",
            "51200/300000: training loss=6.1307, dev loss=6.1622\n",
            "51250/300000: training loss=6.1685, dev loss=6.2095\n",
            "51300/300000: training loss=6.1489, dev loss=6.1927\n",
            "51350/300000: training loss=6.1510, dev loss=6.1418\n",
            "51400/300000: training loss=6.1563, dev loss=6.1775\n",
            "51450/300000: training loss=6.1842, dev loss=6.1773\n",
            "51500/300000: training loss=6.1465, dev loss=6.1266\n",
            "51550/300000: training loss=6.1688, dev loss=6.1635\n",
            "51600/300000: training loss=6.1794, dev loss=6.1597\n",
            "51650/300000: training loss=6.1548, dev loss=6.1997\n",
            "51700/300000: training loss=6.1524, dev loss=6.1342\n",
            "51750/300000: training loss=6.1608, dev loss=6.1680\n",
            "51800/300000: training loss=6.1712, dev loss=6.1556\n",
            "51850/300000: training loss=6.2038, dev loss=6.1603\n",
            "51900/300000: training loss=6.2048, dev loss=6.1706\n",
            "51950/300000: training loss=6.1562, dev loss=6.1314\n",
            "52000/300000: training loss=6.1231, dev loss=6.1480\n",
            "52050/300000: training loss=6.1536, dev loss=6.1509\n",
            "52100/300000: training loss=6.1100, dev loss=6.0912\n",
            "52150/300000: training loss=6.1737, dev loss=6.1515\n",
            "52200/300000: training loss=6.1693, dev loss=6.1566\n",
            "52250/300000: training loss=6.2113, dev loss=6.1595\n",
            "52300/300000: training loss=6.2039, dev loss=6.1402\n",
            "52350/300000: training loss=6.1571, dev loss=6.1963\n",
            "52400/300000: training loss=6.1264, dev loss=6.0978\n",
            "52450/300000: training loss=6.1398, dev loss=6.1853\n",
            "52500/300000: training loss=6.1593, dev loss=6.0974\n",
            "52550/300000: training loss=6.1330, dev loss=6.2125\n",
            "52600/300000: training loss=6.1384, dev loss=6.1271\n",
            "52650/300000: training loss=6.1721, dev loss=6.1529\n",
            "52700/300000: training loss=6.1358, dev loss=6.1452\n",
            "52750/300000: training loss=6.1742, dev loss=6.1448\n",
            "52800/300000: training loss=6.1817, dev loss=6.1726\n",
            "52850/300000: training loss=6.1449, dev loss=6.1237\n",
            "52900/300000: training loss=6.1139, dev loss=6.1541\n",
            "52950/300000: training loss=6.1309, dev loss=6.1372\n",
            "53000/300000: training loss=6.1229, dev loss=6.1580\n",
            "53050/300000: training loss=6.1458, dev loss=6.1457\n",
            "53100/300000: training loss=6.1567, dev loss=6.1227\n",
            "53150/300000: training loss=6.1261, dev loss=6.2110\n",
            "53200/300000: training loss=6.1416, dev loss=6.1110\n",
            "53250/300000: training loss=6.1716, dev loss=6.1777\n",
            "53300/300000: training loss=6.1579, dev loss=6.1352\n",
            "53350/300000: training loss=6.1672, dev loss=6.1163\n",
            "53400/300000: training loss=6.1728, dev loss=6.1114\n",
            "53450/300000: training loss=6.1288, dev loss=6.1196\n",
            "53500/300000: training loss=6.1283, dev loss=6.1642\n",
            "53550/300000: training loss=6.1209, dev loss=6.1191\n",
            "53600/300000: training loss=6.1987, dev loss=6.1028\n",
            "53650/300000: training loss=6.1305, dev loss=6.1148\n",
            "53700/300000: training loss=6.1608, dev loss=6.1862\n",
            "53750/300000: training loss=6.1526, dev loss=6.1092\n",
            "53800/300000: training loss=6.1216, dev loss=6.0849\n",
            "53850/300000: training loss=6.1170, dev loss=6.1610\n",
            "53900/300000: training loss=6.1381, dev loss=6.0957\n",
            "53950/300000: training loss=6.1091, dev loss=6.1702\n",
            "54000/300000: training loss=6.1299, dev loss=6.1531\n",
            "54050/300000: training loss=6.1091, dev loss=6.0853\n",
            "54100/300000: training loss=6.1060, dev loss=6.1326\n",
            "54150/300000: training loss=6.1156, dev loss=6.1398\n",
            "54200/300000: training loss=6.1375, dev loss=6.1048\n",
            "54250/300000: training loss=6.1277, dev loss=6.1157\n",
            "54300/300000: training loss=6.1154, dev loss=6.1533\n",
            "54350/300000: training loss=6.1387, dev loss=6.1220\n",
            "54400/300000: training loss=6.1294, dev loss=6.1488\n",
            "54450/300000: training loss=6.1591, dev loss=6.1179\n",
            "54500/300000: training loss=6.1540, dev loss=6.1565\n",
            "54550/300000: training loss=6.1483, dev loss=6.1426\n",
            "54600/300000: training loss=6.1130, dev loss=6.1504\n",
            "54650/300000: training loss=6.1237, dev loss=6.1910\n",
            "54700/300000: training loss=6.0967, dev loss=6.1533\n",
            "54750/300000: training loss=6.1184, dev loss=6.1441\n",
            "54800/300000: training loss=6.1257, dev loss=6.1621\n",
            "54850/300000: training loss=6.0688, dev loss=6.1061\n",
            "54900/300000: training loss=6.1555, dev loss=6.1389\n",
            "54950/300000: training loss=6.0886, dev loss=6.0798\n",
            "55000/300000: training loss=6.1049, dev loss=6.1250\n",
            "55050/300000: training loss=6.1725, dev loss=6.0689\n",
            "55100/300000: training loss=6.0941, dev loss=6.1420\n",
            "55150/300000: training loss=6.1808, dev loss=6.0682\n",
            "55200/300000: training loss=6.1390, dev loss=6.1441\n",
            "55250/300000: training loss=6.1543, dev loss=6.1302\n",
            "55300/300000: training loss=6.1073, dev loss=6.1190\n",
            "55350/300000: training loss=6.1413, dev loss=6.1345\n",
            "55400/300000: training loss=6.1191, dev loss=6.1076\n",
            "55450/300000: training loss=6.1072, dev loss=6.0988\n",
            "55500/300000: training loss=6.1302, dev loss=6.1058\n",
            "55550/300000: training loss=6.1319, dev loss=6.1408\n",
            "55600/300000: training loss=6.1261, dev loss=6.1003\n",
            "55650/300000: training loss=6.1280, dev loss=6.1020\n",
            "55700/300000: training loss=6.1162, dev loss=6.1143\n",
            "55750/300000: training loss=6.1642, dev loss=6.1374\n",
            "55800/300000: training loss=6.0822, dev loss=6.1573\n",
            "55850/300000: training loss=6.0842, dev loss=6.1115\n",
            "55900/300000: training loss=6.0900, dev loss=6.0983\n",
            "55950/300000: training loss=6.1279, dev loss=6.1390\n",
            "56000/300000: training loss=6.0920, dev loss=6.1495\n",
            "56050/300000: training loss=6.1467, dev loss=6.1268\n",
            "56100/300000: training loss=6.1014, dev loss=6.1269\n",
            "56150/300000: training loss=6.1043, dev loss=6.1327\n",
            "56200/300000: training loss=6.1052, dev loss=6.1057\n",
            "56250/300000: training loss=6.1652, dev loss=6.1084\n",
            "56300/300000: training loss=6.1756, dev loss=6.1343\n",
            "56350/300000: training loss=6.1243, dev loss=6.1208\n",
            "56400/300000: training loss=6.1171, dev loss=6.1027\n",
            "56450/300000: training loss=6.1287, dev loss=6.1136\n",
            "56500/300000: training loss=6.1174, dev loss=6.1325\n",
            "56550/300000: training loss=6.0761, dev loss=6.0976\n",
            "56600/300000: training loss=6.1102, dev loss=6.1648\n",
            "56650/300000: training loss=6.1101, dev loss=6.1164\n",
            "56700/300000: training loss=6.1225, dev loss=6.1165\n",
            "56750/300000: training loss=6.1006, dev loss=6.0955\n",
            "56800/300000: training loss=6.1258, dev loss=6.1265\n",
            "56850/300000: training loss=6.1032, dev loss=6.0690\n",
            "56900/300000: training loss=6.1269, dev loss=6.1034\n",
            "56950/300000: training loss=6.1450, dev loss=6.0949\n",
            "57000/300000: training loss=6.1439, dev loss=6.1261\n",
            "57050/300000: training loss=6.1205, dev loss=6.0944\n",
            "57100/300000: training loss=6.0919, dev loss=6.0502\n",
            "57150/300000: training loss=6.1233, dev loss=6.1409\n",
            "57200/300000: training loss=6.0722, dev loss=6.1105\n",
            "57250/300000: training loss=6.0846, dev loss=6.0618\n",
            "57300/300000: training loss=6.1008, dev loss=6.1078\n",
            "57350/300000: training loss=6.1267, dev loss=6.0831\n",
            "57400/300000: training loss=6.1051, dev loss=6.1417\n",
            "57450/300000: training loss=6.0697, dev loss=6.1029\n",
            "57500/300000: training loss=6.1379, dev loss=6.1092\n",
            "57550/300000: training loss=6.0908, dev loss=6.0957\n",
            "57600/300000: training loss=6.1122, dev loss=6.1153\n",
            "57650/300000: training loss=6.1395, dev loss=6.1144\n",
            "57700/300000: training loss=6.1425, dev loss=6.0744\n",
            "57750/300000: training loss=6.0634, dev loss=6.0459\n",
            "57800/300000: training loss=6.1089, dev loss=6.1100\n",
            "57850/300000: training loss=6.1173, dev loss=6.1320\n",
            "57900/300000: training loss=6.1149, dev loss=6.1352\n",
            "57950/300000: training loss=6.0725, dev loss=6.0862\n",
            "58000/300000: training loss=6.0622, dev loss=6.0965\n",
            "58050/300000: training loss=6.1346, dev loss=6.1078\n",
            "58100/300000: training loss=6.0940, dev loss=6.1240\n",
            "58150/300000: training loss=6.0659, dev loss=6.1153\n",
            "58200/300000: training loss=6.1306, dev loss=6.0745\n",
            "58250/300000: training loss=6.0660, dev loss=6.1347\n",
            "58300/300000: training loss=6.0620, dev loss=6.0827\n",
            "58350/300000: training loss=6.0834, dev loss=6.0820\n",
            "58400/300000: training loss=6.1161, dev loss=6.0799\n",
            "58450/300000: training loss=6.1152, dev loss=6.0813\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-269-9217f3571cfc>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mrunning_loss_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0mlossi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mlossi_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "jjTgq78BCCRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "caad32a1-805b-4bd2-e17a-fdde0afc8377"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9b04f16f80>]"
            ]
          },
          "metadata": {},
          "execution_count": 270
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi_dev)"
      ],
      "metadata": {
        "id": "c6QgPaXDCHha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "34bf56b6-4fe6-4042-95e2-f667298fd8f0"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9b04f41570>]"
            ]
          },
          "metadata": {},
          "execution_count": 271
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWv0lEQVR4nO3deVwU5eMH8M+ysBzKoYIgiiKeeaGiEh6lSeLxs/Tbt8wsj8rStCwqU1PpptNO0w5Nv11qZVppmKF4oiiK9w0KIoeoHCL3zu8PZNhlZ3d2l4Vll8/79drXi515ZubZidwPzzyHQhAEAURERER2wsHaFSAiIiKyJIYbIiIisisMN0RERGRXGG6IiIjIrjDcEBERkV1huCEiIiK7wnBDREREdoXhhoiIiOyKo7UrYAlqtRpXrlyBu7s7FAqFtatDRERERhAEAQUFBfD394eDg+XaW+wi3Fy5cgUBAQHWrgYRERGZIS0tDW3atLHY+ewi3Li7uwOovDkeHh5Wrg0REREZIz8/HwEBAeL3uKXYRbipehTl4eHBcENERGRjLN2lhB2KiYiIyK4w3BAREZFdYbghIiIiu8JwQ0RERHaF4YaIiIjsCsMNERER2RWGGyIiIrIrDDdERERkVxhuiIiIyK4w3BAREZFdYbghIiIiu8JwQ0RERHbFLhbOrCtlFWpEbz4NtSBg/uiucHZUWrtKREREJIMtNwYIArByTwpW7b2IknK1tatDRERERmC4MUDpUL0Ee0WFYMWaEBERkbEYbgzQyDYoVzPcEBER2QKGGwMUCgUcbyecCoYbIiIim8BwI6Pq0VS5mn1uiIiIbAHDjQy23BAREdkWhhsZ1S03DDdERES2gOFGhqOy8hax5YaIiMg2MNzIEFtuOBSciIjIJjDcyLhaUAIAuF5YauWaEBERkTEYbox0OPWGtatARERERmC4MVJYhxbWrgIREREZgeFGRmALNwAAe9wQERHZBoYbGQpFZYdigemGiIjIJjDcyKhaXkpguiEiIrIJDDdybqcbRhsiIiLbwHAjo7rlxqrVICIiIiMx3MgQ+9yw7YaIiMgmMNzIqGq5YbYhIiKyDQw3MhTsc0NERGRTGG5kKMCh4ERERLaE4UZGdcsN0w0REZEtYLgxEltuiIiIbAPDjYzq0VJERERkCxhuZHCGYiIiItvCcCODo6WIiIhsC8ONDIXYdGPVahAREZGRGG5kiEPBmW6IiIhsAsONDPGxFLMNERGRTWC4kcGFM4mIiGwLw40cDgUnIiKyKQw3MjgUnIiIyLYw3MjgUHAiIiLbwnAjg31uiIiIbIvJ4Wbnzp0YO3Ys/P39oVAosGHDBtlj4uLi0LdvXzg7O6Njx45YtWqVTpmlS5ciMDAQLi4uCA0NRUJCgqlVqxMKTnRDRERkU0wON4WFhQgODsbSpUuNKp+SkoIxY8Zg2LBhSEpKwvPPP48nn3wSW7ZsEcusXbsWkZGRiIqKwqFDhxAcHIyIiAhkZ2ebWj2LY8sNERGRbXE09YBRo0Zh1KhRRpdfvnw52rdvj48++ggAcMcdd2D37t34+OOPERERAQBYsmQJpk+fjmnTponHbNq0CStXrsS8efNMraJFsc8NERGRbanzPjfx8fEIDw/X2hYREYH4+HgAQGlpKRITE7XKODg4IDw8XCxTU0lJCfLz87VedUWcoZjphoiIyCbUebjJzMyEr6+v1jZfX1/k5+ejqKgIOTk5qKiokCyTmZkpec7o6Gh4enqKr4CAgDqrP8SWG6YbIiIiW2CTo6Xmz5+PvLw88ZWWllZn12KfGyIiItticp8bU/n5+SErK0trW1ZWFjw8PODq6gqlUgmlUilZxs/PT/Kczs7OcHZ2rrM6a2KfGyIiIttS5y03YWFhiI2N1dq2detWhIWFAQBUKhVCQkK0yqjVasTGxoplrKm6zw3jDRERkS0wOdzcvHkTSUlJSEpKAlA51DspKQmpqakAKh8ZTZ48WSw/Y8YMJCcnY+7cuTh9+jS+/PJLrFu3Di+88IJYJjIyEt988w1Wr16NU6dOYebMmSgsLBRHT1mTOM0NERER2QSTH0sdPHgQw4YNE99HRkYCAKZMmYJVq1YhIyNDDDoA0L59e2zatAkvvPACPv30U7Rp0wbffvutOAwcACZMmICrV69i8eLFyMzMRO/evRETE6PTydgaxMdSbLghIiKyCQrBDp635Ofnw9PTE3l5efDw8LDouR/9dj92n8/BxxOCMb5PG4uem4iIqDGrq+9vmxwtVZ/YckNERGRbGG5k7DqXAwA4l33TyjUhIiIiYzDcGGlZ3AVrV4GIiIiMwHBDREREdoXhxkhPDm5v7SoQERGRERhuZNzbrXI4epBPUyvXhIiIiIzBcCNDXFuKCzAQERHZBIYbGRwKTkREZFsYbmSIa0tZuR5ERERkHIYbGeLaUmy6ISIisgkMNzLEx1LWrQYREREZieFGhuJ2ulGrGW+IiIhsAcONjOrRUkRERGQLGG5kVLXcsMsNERGRbWC4kcGWGyIiItvCcCOjep4bxhsiIiJbwHAjQyFfhIiIiBoQhhsZ7HNDRERkWxhuZHBtKSIiItvCcCODLTdERES2heFGRlWHYs7hR0REZBsYbmTwsRQREZFtYbiRUT0U3Lr1ICIiIuMw3MhQcDA4ERGRTWG4kcFJ/IiIiGwLw40MPpYiIiKyLQw3sm4PBbdyLYiIiMg4DDcy2HJDRERkWxhuZDhUhRu23RAREdkEhhsZVaOlOIkfERGRbWC4kaEQZ/FjuiEiIrIFDDcyqmcoJiIiIlvAcCODC2cSERHZFoYbGRevFQIAEi5et3JNiIiIyBgMNzLizlwFACSkMNwQERHZAoYbIiIisisMN0RERGRXGG6IiIjIrjDcEBERkV1huCEiIiK7wnBDREREdoXhhoiIiOwKww0RERHZFYYbIiIisitmhZulS5ciMDAQLi4uCA0NRUJCgt6yZWVleOONN9ChQwe4uLggODgYMTExWmVee+01KBQKrVfXrl3NqRoRERE1ciaHm7Vr1yIyMhJRUVE4dOgQgoODERERgezsbMnyCxcuxFdffYXPP/8cJ0+exIwZMzB+/HgcPnxYq1z37t2RkZEhvnbv3m3eJyIiIqJGzeRws2TJEkyfPh3Tpk1Dt27dsHz5cri5uWHlypWS5b///nssWLAAo0ePRlBQEGbOnInRo0fjo48+0irn6OgIPz8/8eXt7W3eJyIiIqJGzaRwU1paisTERISHh1efwMEB4eHhiI+PlzympKQELi4uWttcXV11WmbOnTsHf39/BAUFYdKkSUhNTdVbj5KSEuTn52u9iIiIiAATw01OTg4qKirg6+urtd3X1xeZmZmSx0RERGDJkiU4d+4c1Go1tm7divXr1yMjI0MsExoailWrViEmJgbLli1DSkoKhgwZgoKCAslzRkdHw9PTU3wFBASY8jGIiIjIjtX5aKlPP/0UnTp1QteuXaFSqTB79mxMmzYNDg7Vlx41ahQefPBB9OrVCxEREdi8eTNyc3Oxbt06yXPOnz8feXl54istLa2uPwYRERHZCJPCjbe3N5RKJbKysrS2Z2Vlwc/PT/IYHx8fbNiwAYWFhbh06RJOnz6Npk2bIigoSO91vLy80LlzZ5w/f15yv7OzMzw8PLReRERERICJ4UalUiEkJASxsbHiNrVajdjYWISFhRk81sXFBa1bt0Z5eTl+++033H///XrL3rx5ExcuXECrVq1MqV6dK6tQW7sKREREJMPkx1KRkZH45ptvsHr1apw6dQozZ85EYWEhpk2bBgCYPHky5s+fL5bfv38/1q9fj+TkZOzatQsjR46EWq3G3LlzxTIvvfQSduzYgYsXL2Lv3r0YP348lEolJk6caIGPaDm5t8qsXQUiIiKS4WjqARMmTMDVq1exePFiZGZmonfv3oiJiRE7Gaempmr1pykuLsbChQuRnJyMpk2bYvTo0fj+++/h5eUllrl8+TImTpyIa9euwcfHB4MHD8a+ffvg4+NT+09IREREjYpCEATB2pWorfz8fHh6eiIvL8/i/W8C520Sfz7wajh83J0ten4iIqLGqq6+v7m2FBEREdkVhhsiIiKyKww3REREZFcYbkwgwOa7JxEREdk9hhsTbD6aIV+IiIiIrIrhRoZCUf3z/pTr1qsIERERGYXhRoaTsvoWaQYdIiIiapgYbmQ4OlQnGgWYboiIiBo6hhsZmuGGiIiIGj6GGxmaj6UKSsqtWBMiIiIyBsONDEdldcvNzrNXrVgTIiIiMgbDjQxHB94iIiIiW8JvbhmaLTdERETU8DHcyGCHYiIiItvCcCOj5mMpQeASDERERA0Zw40MTzcnrfdfbDtvpZoQERGRMRhuZMwZ3knr/Udbz1qpJkRERGQMhhsZHi5O8oWIiIiowWC4kcH1pIiIiGwLw40MB6YbIiIim8JwI4Nz+BEREdkWfnXL4ErgREREtoXhhoiIiOwKww0RERHZFYYbGf5eLtauAhEREZmA4UaGu8Q8Nzk3S6xQEyIiIjIGw40ZvtmZbO0qEBERkR4MN0RERGRXGG7McPxKnrWrQERERHow3Jhhz/lrOJdVYO1qEBERkQSGGzOt3HPR2lUgIiIiCQw3REREZFcYboiIiMiuMNyYqbxCbe0qEBERkQSGGzP9knjZ2lUgIiIiCQw3REREZFcYboiIiMiuMNwQERGRXWG4ISIiIrvCcENERER2heGmFradzgJQuUr41zsvWLk2REREBACO1q6ALXt81UE8d09HfLbtPABgQv+28HR1snKtiIiIGje23BjB3UV/BqwKNgAn9iMiImoIGG6M4OigsHYViIiIyEhmhZulS5ciMDAQLi4uCA0NRUJCgt6yZWVleOONN9ChQwe4uLggODgYMTExtTpnfROMLKdQMAQRERFZm8nhZu3atYiMjERUVBQOHTqE4OBgREREIDs7W7L8woUL8dVXX+Hzzz/HyZMnMWPGDIwfPx6HDx82+5z1TTAy3ZzJLKjbihAREZEshSAY+9VdKTQ0FP3798cXX3wBAFCr1QgICMCzzz6LefPm6ZT39/fHq6++ilmzZonbHnjgAbi6uuKHH34w65w15efnw9PTE3l5efDw8DDl4xil52tbUFBcblTZi++Osfj1iYiI7FFdfX+b1HJTWlqKxMREhIeHV5/AwQHh4eGIj4+XPKakpAQuLi5a21xdXbF79+5anTM/P1/rVadMin/S3o85jch1STAxS+q1MSkdG5PSLXIuIiIie2JSuMnJyUFFRQV8fX21tvv6+iIzM1PymIiICCxZsgTnzp2DWq3G1q1bsX79emRkZJh9zujoaHh6eoqvgIAAUz6GydyclbU+x5dxF7D+UDpO63l0lZJTiOyCYqPOVVBchjlrkjBnTRIKS4xrUSIiImos6ny01KeffopOnTqha9euUKlUmD17NqZNmwYHB/MvPX/+fOTl5YmvtLQ0C9ZY12N3trPYuW6WlGPb6SzcKq0OJdkFxRj2YRwGvB1r1DmKy6qHnJeWc/g5ERGRJpMShre3N5RKJbKysrS2Z2Vlwc/PT/IYHx8fbNiwAYWFhbh06RJOnz6Npk2bIigoyOxzOjs7w8PDQ+tVl/oFNje6bIW68rHTh1vOYOp3CTpz30RtPIHHVx3EnDVJ4rbzWTctUk8iIiIyMdyoVCqEhIQgNra6hUGtViM2NhZhYWEGj3VxcUHr1q1RXl6O3377Dffff3+tz9kQHb2cCwD4Yvt5xJ25irgzV7X2n8yo7B+09WRWzUMN2noyCx9vPWuxPjtERET2yuTlFyIjIzFlyhT069cPAwYMwCeffILCwkJMmzYNADB58mS0bt0a0dHRAID9+/cjPT0dvXv3Rnp6Ol577TWo1WrMnTvX6HPakvjka+jZ2lN8X2rErMVxZ6/Klpn+v4MAgOAAT/Rs7SVuP3ElH4M7eZteUSIiIjtlcriZMGECrl69isWLFyMzMxO9e/dGTEyM2CE4NTVVqz9NcXExFi5ciOTkZDRt2hSjR4/G999/Dy8vL6PPaUvejzmD1Gu3xPdyDS3H0/Pw9c5ko8//+KqDWP/MQPH9oyv2c/g5ERGRBrMWzpw9ezZmz54tuS8uLk7r/d13342TJ0/W6pzW1tTZtNu05oDxHZzNmfjvlV+PypYpLCmHytEBTkqusEFERI0Lv/mM0EPjMZOpytWWH81041aZwf35xWXoHrUFQz+Is/i1iYiIGjqzWm7IeHPWJGFNgv6WnLpYjirx0g0AQHpukeVPTkRE1MCx5aYexCdf07svct0RrfcRH+9kKCEiIqoFhhsrqpoTR9OZrAK8s+lU7U7M0eJERNSIMdxY0bRVByS3x53J5nw2REREZmK4saKdeua3KSytwF9HMwwcaX7wKatQY2NSOrLy5dexyi8uw5G0XDFopecWIWrjcVzMKTT7+kRERHWN4aaBijlevWhozVacnJulWu8PXryOe5fswJ7zOZXlNcLPh1vOoKi0Qnz/7a4UzFmThHuX7JCtw4glO3H/0j3YfiYbAPDEqgNYHX8JD30lvVo7ERFRQ8BwY6Rmbk71er1NxzKQe6sUj686gP4yC2pO+HofzmXfxKRv9+vs+2L7eSzdfh4A8NP+VLwXcxoAkF8sv5p45u3WnaqgVbWieXZBifEfhIiIqJ4x3BjphXs71/s1R326C9tOZyPnpuEwUbNjsgLa48vPZFWGkgW/H7NsBYmIiBoghhsjTQ4LrPdrZuTJ94uRInC4FBERNWIMN41AHcwTSERE1GAx3Nih+hhFnnb9FkrLLb+0BBERUW0x3NiZcUv34EqNGY7rYomHIe9vxwPL9lr+xERERLXEtaXsTFJaLo6l52ltq9nBuEpJeQWcHZUAgH9PZsHJ0QF3d/Yx+lo1r0NERNQQsOXGDumMntLTctP/rX8hCAJyb5Xiyf8dxJSVCSir4KMmIiKybQw3jVh+cTnK1QIKNOa8kVrviitBEBGRLWG4aQQM9bm5acRkfkRERLaE4aaRG/XpLp1tP+6/hEHvbjPq+D+PXEHUxuNaLT5qidYfQ9Yfuoyw6FicuMI+PEREVHsMN42Avg7FQOUSCxsOp4vvs/NL8Orvx5FeY8SVPs/+fBir4y/hr6NXAAAHLl5Hr9f/wdoDqUbXL3LdEWTkFWPOmiQAQEFxmdHHNiTf7krGpG/3obisQr4wERHVGYabRmDTsQy89MsRvfs/2npW/Pm9Lad19hszlPx6YeVinjN/OISbJeV45Tf5pR7OZRUgIeW6+L64rALrDqSh52v/4PPYc/IXbWDe2nQKe85fw0/7jQ92RERkeRwK3kj8mnjZqHKaK4jXtXs/3qn1/vKNIsz97SiAysD17PBO4j61WoCDg23MtVzElhsiIqtiy40Jhndtae0q1DlBYmhUcZn88PCcmyXYefaq7CKf5jiffRPBr/+Dz2ywNYeIiOofw40Jlk7qi6bO9t3YJdUV+I8jV2RbdJZuv4DJKxPqpE7Rm0+hoKQcSzQenxEREenDcGMCFycl/o2829rVsIpPLdBqUlLOxzVERFT3GG5M1LyJytpVqFNxZ67q2Z5dq/OeySxAl4UxWLhBvqMxERFRbTDcmKguFqG0BaczC0w+5mpBZf+b8go1Ij6p7Dz8w77ajSRaFndB/Dn12i18vPUsbtweqUVERAQw3JjMobGmGzMMfm8bissq8HOC5YZGvxdTPVT9vqW78WnsOXGElTmKyyqw90IO19QiIrIjDDcmUtrIcOSGoKRcja6LYrAv+brW9vdiTps0F4y++Y5zb1VO9qc5V44+V3KLsOuc7iO3OWsO45Fv9uO9v3Xn9yEiItvEcGOGP2YPsnYVbMqmYxla75fFXcCC3y3X90Zq+HpNA9/dhsdWJGDnWe2As+VEFgBg1d6LAKQXDiUiItvCcGOGXm28rF2FRkWurSy/uNzoJQ/0tfIIqJxPp9viGHy45YxpFSQiogaF4Ybswvgv95p8TM0FPj/Ychol5Wp8sf08dp27is01WpyIiMg2MNyQXTiVkS/+/HNCKiLXJqFcopOwZn/wbae1h7drPt16bEUCnvnxEDLyjFtA1FRXC0o4youIqI4w3JDdmb/+GNYfTsdfRw23vNwsKZc913WZAHL5xi2j+vxo2noyC/3f/hd93tyq03pERES1x3BDDdbq2518z2XfNPqYc1nV8/EUFJfp7NfXf0cQBL2jsvT5Pv4iBr+3HYs3njDpuOn/Oyj+rDYxGBERkTyGG2qwov6oDA2p128ZVT4pLVd7pXGFAhVqQfvxlMZzqdpOWfR+TGXH4+/3XardiYiIyKLsexXIOuTdVIWcm+wzUdce/Xa/zraY45kY2cNPZ/u4pXt0tt27ZIfW4ydDeUaqEUVh4Ai2uRARNUxsuTHTqmkDrF2FRmH3+RydbTN+SDRq6HdZuRrJOYXIvr0MRE1XdbbrxpX3t+if3M/UvjZERFQ/GG7M1KO1J+5o5WHtajRaxiyXsGzHBb379l7IwVubTmltk8oqcWeu4uSVfN0dAApLa7/KuYLLeRARWRzDTS3wL3frmbIyQbaMbstMdT+bVXsuam039F/SmFFV9eHyjVuSw9uJiEgbw00tMNtYz6HUXLOOyysqw9oDqSgo1g0stfnPmZVfbHB/UWkFJn69D9/uSpY9V2FJOaI3n8KRtFxx2/bT2Rj83nZM+U4+1OlTWq7Gldy6mbeHiKghYYfiWjB98DBZ23c1WmyqCAJwq1S6hcaYFrrQd2IN7v9x/yXEJ19DfPI1re1SD6U+3noW3+5OwVc7k3Hx3TEAqte+2nP+msQRhuUXl8Hd2RH3fbEbpzML8MfsQVxChIjsGltuaoHzr9mXmquXV9H8z/zKr0fx4rojyMzT31Lz8dazOtuK9PTPOXDxOh5fdQCp16qHu5/RmKtHyvnsAqMfiZ7OzEev1/7B9P8dxOnMyvNuTLpi1LFERLaKLTe1wD43jUt+cRnWHkwDAGxIStdbrvz23DqOSvm/HSZ8vQ8AkHOzBH/MHmxUPcKX7MRz93RE5IgusmWrJkL891T1UhOW/LUtq1Aju6AErb1cLXdSKyguq8DpzAL0au0JBwd28iaydWy5qQVXldLaVaB6cOxyHq4WlKCiojoVVMg0260/rB1+5PLEldzqliBjRlB9tu28+HN2QbFJQfvnhFTx58y8Ygx+bxuWxekfWaZPVn4x7vtiDwa9uw17L+gO2bclT6w+gHFL94iP/4jItpkVbpYuXYrAwEC4uLggNDQUCQmGOzl+8skn6NKlC1xdXREQEIAXXngBxcXV/5i/9tprUCgUWq+uXbuaU7V65eHiZO0qUD14e/MpcS0oY2VpPLbKu1UmOV+PJrk8o2+Nq41J6RjwdiwWbTxudN2KNOYI+njrWVy+UYT3YvTP5yMlK78Yoe/EiguWrklIE/cJgoDlOy4g9lSW1jHns2/ioeXx2CNzL0xx7ab0HEamqurL9MN+25lt+oMtp/EDZ8cmkmRyuFm7di0iIyMRFRWFQ4cOITg4GBEREcjOzpYs/9NPP2HevHmIiorCqVOnsGLFCqxduxYLFizQKte9e3dkZGSIr927d5v3ierRmF6trF0FaqA+0uh3839f7EJCinR/nipXC0rEdbFq5pzUa7dwLD1P8riqJSB+2JcquV9fg44gCHj02/3iYzZTHbx4Q+++fcnX8e7fp/HE6oNa22f+kIiEi9cxSWLWaXN89M8ZhLz1b6P8gj9xJQ9Lt1/Awg3Gh1qixsTkcLNkyRJMnz4d06ZNQ7du3bB8+XK4ublh5cqVkuX37t2LQYMG4ZFHHkFgYCBGjBiBiRMn6rT2ODo6ws/PT3x5e3ub94nq0cT+bfHTk6Fo0URl7apQA1S1cGfadeOGX9/78U4kXrqBHWevam3/52Sm3mPK1dLz3vx7MgsjPt6BkxnSExAm5xTKtiaZQjNDaQ6L13xcdtVCrSxVPr/9aM6UVit7ITWVARFVMynclJaWIjExEeHh4dUncHBAeHg44uPjJY8ZOHAgEhMTxTCTnJyMzZs3Y/To0Vrlzp07B39/fwQFBWHSpElITZX+SxQASkpKkJ+fr/WyBgcHBQZ29Mb6ZwZa5frUsB29LN3aYsgDy/bqbJOajLBKVr70vif/dxBns27qrUOWgdFecuIvXMOsnw4ZVfaAgRYeSxEE4H/xF7VGnFlSSXntZ6KuS2ezCvDLwTQOcCDSYFK4ycnJQUVFBXx9fbW2+/r6IjNT+q/LRx55BG+88QYGDx4MJycndOjQAUOHDtV6LBUaGopVq1YhJiYGy5YtQ0pKCoYMGYKCAukhsdHR0fD09BRfAQEBpnwMi2vXogmixnazah2o4Zn07X7c/0XtH69+tVN+4j8AiFybhM9jzxn1JT9ZYoZnQRDwxbZzOn1lgMpZmquGv0/8Zp9R9QEq5w66WVKO59ccRu6tMq1rLdl6Fv+ezELk2iSxH05KTiG+3ZVs1NphmhZvPIFhH8WZdEyV3xIv6933yb9n0WVhDPYnX4NaLWD3uRy9/Z+sZcTHO/Hyr0ex+Zj+Fj6ixqbOR0vFxcXhnXfewZdffolDhw5h/fr12LRpE958802xzKhRo/Dggw+iV69eiIiIwObNm5Gbm4t169ZJnnP+/PnIy8sTX2lp5vUbsKRJoe0wbVCgtatBDcwRM1pvjFVzxNb6w+n4aOtZ3PXBdtljyyVGe+08l4MP/zmr01cGAIJf/wd3RsciW89MzH8fy9C7NMTS7eexocbcOp9vO4/PYs/hyf8dxPrD6WI/nGEfxuGtTafwyb/n9NY9I69IfOSnSW4EG1C5hMX89UfF/k0A8OIvR8Sfa/Z3qqrHa3+exPrD6Xh0xX6M+HiH7HVMpVYL2Jd8TfJzSZHqf66vXxZRY2RSuPH29oZSqURWlvZfdllZWfDz85M8ZtGiRXjsscfw5JNPomfPnhg/fjzeeecdREdHQ62nv4CXlxc6d+6M8+fPS+53dnaGh4eH1svaVI4OiBrbHd5N2f+GLGdnjf43mu4xs6VCnwM1Oj3fKi1H3u3WlqrgoG/Zi3K1gI//1Z28EJBemmKJxESHmv46egXlFWrcKi3HxqR05BVV1iM7vxhh0dvQ6/V/DB6vz9PfJ+LnhDTc98Uek4/dervvU87N6pab4rIK2aU3jPFjQioe/nof/rtM+vG+MThjOlE1k8KNSqVCSEgIYmOrp5pXq9WIjY1FWFiY5DG3bt2Cg4P2ZZTKyvlh9D0jvnnzJi5cuIBWrTgaiRo3qcdHVS5ZuI/JF9ur/5joEbUF3RZvQfAb/xi9cOjS7abPlaPP5RtFmPJdAub9dgxz1iRhxveJAIBDqZV9eAx1LzG0uOiJ2yu8F5n42Euf4R/tQOg7sbhw9WatzvP7ocpHY3KzUxOZQq0WoG6kU+mb/FgqMjIS33zzDVavXo1Tp05h5syZKCwsxLRp0wAAkydPxvz588XyY8eOxbJly7BmzRqkpKRg69atWLRoEcaOHSuGnJdeegk7duzAxYsXsXfvXowfPx5KpRITJ0600MckIlNoBpozmdbpsL/n/DX8caTycVbVmlxyfWaT0nLRaeHf+DxW/2Mtc+ibhij99kKkW0/q9lOqd7X8DttzPgeTVyYg7XrddMym+iUIAu5buhtjv9jdKDubm7z8woQJE3D16lUsXrwYmZmZ6N27N2JiYsROxqmpqVotNQsXLoRCocDChQuRnp4OHx8fjB07Fm+//bZY5vLly5g4cSKuXbsGHx8fDB48GPv27YOPj48FPmL9ujOoBf46mgEvNyetDpREtuqBWj0qMd6vBjr2GivqjxMQhMp5hp4Z1hFKA0spFJdVwMXJuFnGFQpAoTfiWHZJC2up6vc0Z81hrH9mkJVrQ7V1vbAUx9PzxZ9bNHW2co3ql1lrS82ePRuzZ8+W3BcXF6d9AUdHREVFISoqSu/51qxZY041GqS3x/fEHa08cF+wP+KTr2Hur0etXSUiq5n23QGjy76k0bHXEu5dsgPbXhqqtU2hqA4i93wYh73zh9fYLx1g5GaQbgj9XSxVg2wDUw8Q2QquLWVhnq5OmDWsIwKau+GhftYdok5kefJfoc+vTbLi1asl5xQa3H/FxLl+DAWcH+JrN0uy9aNRtfpshbpVWjlNQMxxDmMny2K4qWN3tLL+SC6i+lI1qslajqTlmlR+7q/GtRYpoECmgVFRpgYlc5RXqPHvySzc/8Vu/HU0Q2e/Lfar+GpHMjYkXcGMHxKtXRWyMww3dWzqwHbiz0E+TaxYE6Lak2sNSTIxXJgqx8QlHIrLKrRGT9VsfFl3ULufj77GGYUCOKxnGLyxNial4/FVB5Bv5Fw2NUe5vLDuCJ7830EcuZyH7+1kPa3sAv2hcPOxDDzyzT69cysRGcJwU8ds8I8pIr2KSg0PoZ5iYOh6bT25+iAWbzxh0jHdo7ZgxMc7a31tqdDzvokrqc9Zk4Rtp7Ox9PaQ++PpeXpbuhIv3UCP17ZohZg/j1yRLFulPv+tqVALBpcFsYRnfjyEvReu4a1Np+r0OvaqsX/1MNwQkdHkVjevS/9KLAshp0ItIDmnEEWlFXjjz5OQm/LjXLae+WpqdLgRBAFfxmnP6yMVPmb9dAg9X9uC0vLq1qO8W2XYfS4H//f5bgz7ME7ycnPWHMat0gos2nAcr/95wuASEbWVlJarNXTemMdbU1YmoP/b/yLx0g2kXb+FjUnpRs0QbY4btxrWche2SF9HeXtm1mgpMl5jT89kX/ZbMdzUxsu/HpHspyJFrRbg4KAbZjRJLV/x7M+HMTbYXyx/vbAUm25f84caj5GqVnqvWqfKUJ74bs9Fo+pt7r8145aaPltz1YryP+6/hPWH0gEAt0orMHFAW5POU5etTRl5RXjmx0OYOjAQ9/dubdY5vo+/iPWH0/Hd1P7wcuPs87aELTf1iUmHyCqMDTZA1XBxAbN+rF75vObq6nKtFNF/n0bIW/+K76/cnuwPkP9Cv3StECXl+mdZrmvm/jO1//ZEiw3FG3+exOHUXMxZk2T2ORZtPIHDqbk6rXTU8DHc1DE/TxdrV4GITLDlRBZ6vvYPNh3TH4j0zd9zLqsAgiDg6xorudcMDIYCzt0fxJnVn6XmOevqMZGtMLbjtjFq/vekho/hpo4N7eyDF+/tjBVT+ukt88zQDvVYIyIyZMYPibLracXraaW49+OdePdv3Y7G6TeKJEpbVuLtdbf+OnoFoz7dhTsWxRhceFUfDoKwnLUHUjHzh0SUlFtmLTNTNPb/jgw3dUyhUODZ4Z0w/A5fvWVejuhSjzUiorr0lcRf+TEnqiepW3swTauDsaUcScvFldwizP7pME5l5KO0Qo2nvzd9/hjN4dllFWr8lngZl29IrzeVpDE83pyGInv/An7lt2P4+3gm1h5Is2o9Gl93YoabBqEx9mQnaszWHtT+sitXWybsDHx3m+T2z2LPIXzJDuQaMfJILQDnb48aW7XnIl785QiGfhAnWVZz3qPDaTcMnlcQBCzdfh593vgHy+qhD0tDCk4FxYZbAsnyGG6sZMbdlY+iIu/tDAD4fGIfa1aHiKxk59mr4gKHllZUVvk4ZMnWsziffRMrdqeI+wpLyjHq012Sx2253dJUNSpKanRYTWnXi/CfL/eguEz6EUz8hWv4YMsZ3LhVhvdMnCOoIZm//hiiNh4X31eoBfx7Mgsbk9KtWCuqiUPBreSVkV0wcUAA2jZ3AwCEBjW3co2IyBoi11l2wdCaTl6pDk5lFdUh5dfEyziVYdlQdSg1FxsOp+NhiSHhGTWWqDA0O7ElGNNyc72wFMvizuPBfgHo7OsuWz47vxg/J6QCAOaO7AonpQM6L/xb3H9nUAv4enAQSUPAlhsrUSgUaNeiCR9JETVypi4pYSrNSfA0Vy+vOfeOMTS/3PUpqzDuEdsLa5OMWk1917kcrNidgrNZBVqP1fKKyvDQ8nizPkeVeb8dxTe7UoyexVqzBUsA8Mg3+7T2/3U0Q5wT6by+CSHriSVWqv/r6BWM/GQnLly17mcxB8NNA6GwUJevdU+HWeQ8RGQfNEdq7Tybg7+PZUCtFvTPxgzgm13SQ59Hf7YL89cfM3g9fV+ppzO1W4n2nNcecTbrp0N6g9Gbf53EiI93ou+bW8Vty+IuIOHidSzccFzyGM0vd32zLh9Lz5PcbqyDl7T7Gb3510n8eyobADD2891Gn6euFz0192/o2T8dxunMArz0S922LtYFhpt6pHKs+9vdroVbnV+DiGzTqYx8zPzxEKaukp6np0ruLd05Yo5ezkXOTfkOyZrf078mXkZYdCzSc4vwza4U/QcB2HQ0A9/HV7bC6Htkpdn1p1BmuL7WuQ3MWWSsY5fzjGqV2n2ucvh9kZ6+RzUlXrqBkLf+xfpDdbfEhqFr/5yQKhuu5NaUa4gYburRJw/3Rtvmbvh4QrDOPs1k/fecIZg5tAN2zR2GM2+NNOkaDWmEABFZ39zfjupsM2b+m6waq3Hf94VxyzRoflG+9MsRZOQVY5CeUVxnsrRbj6r6AF26Jj303Fy/3F79vUItaK0Sb4qxX+zGC2uTZMuVqwWsO6g79Dvt+i0s33FBZ3LBp79PxPXCUrHvlSAImLwywahrGWTEd8EDy/Zi/vpjYsdxe8JwU4+6+nlg59xhGN+njc4+R421bFp5uuCVkV0R0NwNzo5K9GjtAQBo6izd/zvm+SHiz5pNsRHd9c+tQ0RkSOg7sdhhxiSApjiSlmvWcWUVaq3RScM/ijO4qGtWfjEEQUCHBZvR8dW/zQ44hzTm9dHnx/2pmPurbqC874vdePfv04iqsbK9ZhgsKq3A2ayb2Hn2Kn4/rH/0VWZeMb7bk4ICA7Mw/5wgPbfOrdJyPLn6IH7RCGApGkP67QVHSzUQXm4qcdG5mgu0fTd1AP48cgX/6dsavd/YqnNsK09X2fOveepOPPz1PtlyRES1lZlXjNXxF00+bl/KNQTO2yRbbuXuFORrzB1z4WohHvoqHhffHSNu02zFPp1ZgESN/jHpuUWVAzpMrqH5btx+1Bd/QbuvkWar/R2LY9C8ifwCnQ9+tRdp14twJC0XnzwsPY3ImgPVHb/VAhC5Lgm9A7xQUFyOf09l4d9TWWZ8CtvBcNOARP+np+R2H3dnPD64PQBgcEdvnSZElbK6Ac7ZUVl3FSQikiEAuDM61qxj067LL1Px7a5kLN8hPwlgzacymkPRpR7fJ166gU/+PYuFY7rh4rVChLRrZvD8b/x5wuB+42lHrKqV4msqKq2Ag0Plv/FV92n7GeNa1raezMT6Q+lYfygdT90VZNQxt0pte+JBhhsbs/rxAdh17iqm3l64786g5nBVKfHO+J4oV6u1Ur+lRmARERnL0OMUS3hr0ynZMoIgGHxMlXDxOgK9m2hte2DZXgBAxCeVw8LlWlDWHTS+A7Bm60zVz9cLS9G8icrgSCZBEKBQKFBSXoFuUTFwd3ZEwqvhWvul/LDvklaYyy8yLah8vfMC3tlsuxMtAgw3NkfpoMDQLi2REj0aZ7IK0P72/6CPhOpOmkVEVN+OXq7d8OraOJ6eh4LicsllJi5rDImf++tRPNQvwOC59LWgWMI9H8YhOacQ80Z1NViupFwNFyclNh6+AkEA8ovLcc+HcQaPKSqt0Bke//bm6kAot8J5UWmFTrCxxfnY2KHYRikUCnT18+BjKCKi2/7v892Y+M0+bJXoT1JzyYef9qeatdinOTQXziwuqxDX5Hr379MG29er1grTHPF2Jc/wzM5lZqxTplmHVzfozmN00QY7HDPcNCLN3JysXQUiojq3/pD8o7EFvx9DZn7dLgFRRXNo+40acwgZahSRaz2qi2wmde+Kyip0hrA3dAw3jcifzw42+9hxvf0tWBMiIgLk+0ZWGGpeqsd5zTJlWowaGoYbO1bzL4I2zdyw+bnKOXEe7m/4eXNNLk58/EVEVN/uen+73n0CgISU65jwVTzOZBbUaT0MzanTEDHcNDLd/D1w6o2RePeBXuI2J6UCnX2bWrFWRESNU4XMtPLpuYaHxz/0VTz2p1zH4zJLahiSlCbfCfyBZfFmn98aGG4aIVeVdiuMIAA/Tb/TSrUhImq8rhaYvyq85lDw2pznNyusa1XXGG4aCQ8Xw52JvZs64+hrIzA5rB1+mzmwnmpFRETmKrTBBS3rC+e5sXMf/LcXMvKK0c3fQ7ash4sT3ri/BwCgVxtPrfkqHgltizUHpNcqISIiakjYcmPHFArgwX4BeG54J8n9YUEtAFSWqen3Zwahd4CX+L5XGy+dMkRE1DCUVqhx4koetp/OtnZVGgS23DRiX08OwZ7z1zC0i4/OPqWDQmcVci83J+TenqPB3dkR658ZiP8uj0dekW31oiciskdjPttt9rGTVyZgbkQXg2Uq1AKUDrYxWzFbbhoxdxcnjOzhp3eY97g+rQEAnVpWjqTq4e8JAHBQAEeiRqCTrzt+mxlWP5UlIqI6s/PsVfx3+V6DZT6LPVdPtak9hhvS64G+rfHbzIH4fdYgAMCSCcGYEtYOMc/fBQcj0/uC0YbXTiEiooahuMzw0g0r96TUU01qj+HGjvl6uNTqeIVCgZB2zcTHUy3dXfD6/T3Q2dddLOPv5ar3+OlD2uOpuzpApeSvGRER1R9+69ih76b2x7je/njh3s51fi03lSMSXh2Ozyf20do+b1RXvDqmGwBg97xhdV4PIiKqY/W43ENtsUOxHRrWtSWGdW1Zb9dr6e6CDj7aMxxrTrrZ0r12LUhERESmYMsNWUSHlk2sXQUiIqpDBSXl1q6C0RhuyCKcHZWYOKCt+J7dbIiIyFr4WIos5p3xPVBSXoEjablaQYeIiKg+MdyQxSgUCix5qLe1q0FERI0cHx4QERGRXWG4oXoxqGMLnW1jerbCbzMHYsPtSQLljOjma+lqERGRHWK4oXrx3dQBaKLSXubh84l9ENKuGXoHeGFwR2/Zc0j143m4v+6in0RE1LiZFW6WLl2KwMBAuLi4IDQ0FAkJCQbLf/LJJ+jSpQtcXV0REBCAF154AcXFxbU6J9kWlaMDurby0NqmuYSDoGd2qD9nDxZ/dndxhF+NWZfffaAX5o/iEg9ERFTN5HCzdu1aREZGIioqCocOHUJwcDAiIiKQnS29zPpPP/2EefPmISoqCqdOncKKFSuwdu1aLFiwwOxzkm1q29xN/PmnJ0ONOqZnG0+8EN4Z43r7I6RdM8kyg4xo9SEiosbD5HCzZMkSTJ8+HdOmTUO3bt2wfPlyuLm5YeXKlZLl9+7di0GDBuGRRx5BYGAgRowYgYkTJ2q1zJh6TrJNi/6vG/7TpzXWPHUnBtYIJNozGjtr7ZsT3gmfPNwHCoX0Yp09WntavK5ERGS7TAo3paWlSExMRHh4ePUJHBwQHh6O+Ph4yWMGDhyIxMREMcwkJydj8+bNGD16tNnnLCkpQX5+vtaLGr7mTVRYMqE37gzS7Vys6YGQNrW+lruL7iwHfdt6iT93q/GIjIiI7IdJ4SYnJwcVFRXw9dUeteLr64vMzEzJYx555BG88cYbGDx4MJycnNChQwcMHTpUfCxlzjmjo6Ph6ekpvgIC2KnU1k243TG4VxtPSLfPVFI5Vv/K3hnUXG+5O/x0w8tnE/tgTK9W+P2ZgVj1eH+z60pERA1bnY+WiouLwzvvvIMvv/wShw4dwvr167Fp0ya8+eabZp9z/vz5yMvLE19paWkWrDFZw33B/tj03GCsezoM/+lb2XKj2dJS5ctJfdHayxXv/7cXfp5+p97zSS0c2qaZG5Y+0hd92jZDS3cX9A+U7sMDAFMHBpr8GYiIqGEwaYZib29vKJVKZGVlaW3PysqCn5+f5DGLFi3CY489hieffBIA0LNnTxQWFuKpp57Cq6++atY5nZ2d4ezsLLmPbJNCoUB3/8q+Mx1bNsXhRffCw9VJp1yP1p7YM+8eI85Xu/pothAREZFtMelfcJVKhZCQEMTGxorb1Go1YmNjERYWJnnMrVu34OCgfRmlsnK+E0EQzDon2b9mTVRQOtQyoRARUaNk8tpSkZGRmDJlCvr164cBAwbgk08+QWFhIaZNmwYAmDx5Mlq3bo3o6GgAwNixY7FkyRL06dMHoaGhOH/+PBYtWoSxY8eKIUfunESmaOXpgkmhbfHtrhTk3CzRW06QnlqHiIhsnMnhZsKECbh69SoWL16MzMxM9O7dGzExMWKH4NTUVK2WmoULF0KhUGDhwoVIT0+Hj48Pxo4di7ffftvocxIZK7R9c/z4ZCgclQ7YN/8edHz1b6OPfXNcDyzacBwADHZqNtZTdwXh653JFjgTERGZQiEItv/3a35+Pjw9PZGXlwcPDw7xbYwC520CADx9dxDmj7pD3P5r4mW89MsRAMDFd8doHfPfZXtx8NINrX3iee4Kwle1DCYp0aPRfv7mWp2DiKghqfnvaG3V1fe3yS03RA3RPy/chX9OZOKJwUFa2x/o2xoeLo6SE/0ZTPU1mm78PV1wJa9YuuxtSx4KRvLVQnyx/XzlKWrbq5mIiMzCcEN2obOvOzr7uutsVygUGNFdetSdlGeGdsDvh9Px1JAgfLWjuuVmw6xB+OtoBkb3bIWs/GKk3biF2T8d1jr2P33b4Pv4i2Z/BiIisgyOdyXSMHdkV+yddw9aNK2easDFyQEtPVzw+OD28PN0QXCAF/6vl7/k8VIBi4iI6hdbbqjR0tfdzJzHSV9O6gsACA1qgc8n9kGQT5Na1Y2IiMzHcEOkR0BzV6RdL8LQzrqzHQNAcIAXjqTlImHBcLT0cBG3jw2WbtWpCyqlA0or1PV2PSIiW8BwQ42W3DDBdU+H4a8jGXiov/TaZetnDkRRWQWaOhv3v9F7D/REj9aeaKJyxNAP40yrrB7Nm6iQmW+4ozMRUWPDPjdEerTydMX0u4LgKbEMBAAoHRRGBxsAmNC/Lbr7eyLQu24fWUWN7Van5yciaugYbogamMh7O4s/BzR3NVg2pF0z9GunvQDo4I7eest/O7mfUXXwcefabURkuxhuqNHy9zQcHOrbm+N6YOrAQDx1VxB+mzkQ74zvqTUh4Z+zB+sc89a4HvhlRhheu91aI7eaebMm0q1Qz93TEa08XST3ERHZGva5oUbrtfu6Qy0IeCS0bZ1dY9Nzg/HdnotarTH6DOzQAo/d2Q5AZYtMSLtmEAQB303rj26tPODroRs+mjVRAQCmDmqP8G6+aO3liqKyCpPrGTmiC7zdnbF44wkA8stPDGjfHK+M7IJJ3+5HcRk7NBNRw8KWG2q0fNydsezREAzp5FNn1+ju74kPHwyGv5d0K5G/TGuJQqHAsC4tdYLNxAEBOP56hNa2Ns3coFAo4KZyxMGF4frOqPdapizEsnJqf4S0a47Tb44y/iAionrCcENkRXcGtRB/NiZcOCkrw8l9wa0Ndmb2buqMn6ffaVQdtr14NwCgU8um4rbRPVsZPMaUjtRS2OmZiOoSww2RNZk4X+C++cPx+zMDEdahhWzZsA4tcOGd0Tjz1kit7a/VCBZBPpWhxstNJW6bO7ILPpnQG/d289U578cTgmWv/dVjIQb3TxvUXvYcRETmYrghsiYTHgUBQIumzujTtpl8wduUDgo4OyrF9629XDF1UHt4N1UZOApwcVRiXJ/W6B+ofa2L747B+D5ttLY9dZf2YqVA5fw7RETWwnBD1Aj8PWcI1j0dBr/bfXykHisJEklrclig7LnbtXAzuD+4jSccJFqolBobh3TSP3ydiMhUDDdE1qT1pW9iM44J7mjlgQHtmxsso9nnp2p5LRcnpXRhE8wc2gH92lVfu2rdrR0vDxW3ae7v7Fvd94eIyBwMN0RWpDC1042FVK1qrtmJWJMpi4dKdYTWPLqDT1OtDVUzPmsGJ83LDTIwCeEnE3obXS8iarw4zw1RI/Tc8E7o0doDoe2NH63V3sRlI/554S5k5Rejk6+7SRHu2Xs6YV/ydZzKyNfZ16O1h0l1IKLGiS03RI2QytEBI3u0EicBBKT73Gh69PYEgzXpO6qzr7s4h5B3U93lHLQeg2lsb95Ehb/nDMGhRffqHNOxpTt+mh6KdU+HGazrQCNGkxGR/WK4IbIipcb/gW4q6zakyrXcCCbM8tezjafWe815baQ6M0s9BdM34mpgB28MaN8c80d11Xt9BxMeqxGR/WG4IbKClyO6oLWXK164tzPeHt8D80d11TuLsS3SHH4OAC09XPDZxD7o0doDb4/rqVNe5Sj9T5Ghyf6evruD1vtnhnaQLDeyu59cdRuEsCC2NhFZCsMNkRXMGtYRu18ZhlaerpgU2k7ni9oazB2r1cXX3ahy9wX7469nh6Dt7aHjVR2LAWBSaDv0a9cMz97TUeuYqQMDxUdMSonx5F9rTBYY0NxNXM4iokd1oJEbql7XHu4fYFQ5NjYRWQ47FBNZiSkjkuqDKY+dNA1o3xxfPNIHN26VYeXuFKOXVlA5OuDAq+FQKIAmzo74deZAnTIKhQJfT+6H/8VfxKgeuktCjNBolQls0QSbnhuCI5dzMaSTDxZtOH77JIbrMX9UVwzs4I0JX8fjVqn2oqO+Hs7Iyi8x6vPo88rIrlhzIE18nxI9Gj/su4RFtxcpJSLLY7ghIgDSnX6NVTW0/DE9nY718XGXv2ZTZ0c8M7Sj3v2/zRyIC9k3xSUphnZpKXvOZm5OeKhfAF6O6ALH2x2fdr9yD05eycejK/YDAB4MaQNPVyd8uzvFmI+il1KpgJtKKQYnhUKBDhJD8BeMvgP/9/nuWl2LiCrxsRQRAah8rPPJhN74blp/a1fFJCHtmuEhA49+2ni5YkaNx35fPNIX80ffIQYboLID82CNmZIFVC53oen3Z3Rblwx5IbwzPFycdDprKyVa7Xq09tTZRkTmYbghItG4Pq0xzIiWD1vwv8cH4Km7gjBxQFut+XG+eKSPwYkCNU0bFGj29Ud088Wc8E6S+/oFNkdwG8NhpifDDpHZGG6IyC7d1dkHC2q0zgDVj9DkCIJllp8AdOcQUjoosHH2YJOOMcc743VHphE1Bgw3RGSUMBueGM/MvtK65zGhbIQJQ9DH9dYOXF880seEK9UfLzcn+UJEDQDDDREZlLgwHJueG4zu/o3rMUlVy8n3TwzQ2t5Cz+SCVfw8XPDTk6EY36d19blkUlHN1iU3lXEtRnID7rr4GTdM3xi/zQzDQ/2MG9ZOZG0MN0RkUIumzo0q2LwQ3hktmqgQeW9nABCXkACAts3dEPP8XfjqsRD4ekiP9PJyc8LAjt5wkJiXR5+q8PPivZ0xopsv7u7cUicQSa3t9afMoy19dZSSuDBc/Pmuzj5a+8b0aoWQdoZXlSdqSBhuiIg0zAnvhIMLw9GmWfXkf7vmDsOW5++Cd1Nn+Lg7I6K7HxwdLP/P57PDO+Hryf10Jiw8sngE/L1cdMprjrD6clJfqJTm10lzZJi+WNbRR3oVeaKGhuGGiKiGmhMsBjR303nEM+P2cg/GrFT+3bT+aOrsiI8nBJtVH08DfV12vzIMXz0WglE9/DCml+5Eh5b0QEibOj0/kaUw3BARmeHR0Lb4N/IubHhmkNZ2qf41Azt442jUCIzvY3w4qHkehZ72lDbN3BDR3Q8KhQJvjeuB1+/rbvQ1jOXrXtlqpHRQoF+7ZhY/v6Uteci8EEn2g+GGiMgMCoUCHVu663QG1sdQH5x+gaYFhicGt9fp6AxULmPxYD/TW1f+o9H5GajsqPzd1P7o1LIpxvRshefvlZ6vp6aakyXWxqCO5o/OG9e7teR2qXtG9onLLxCR3evT1svaVZC04+WhOHjxBsb1kf4y1qT5pGzR/xm3fpc+43r7Y0PSFQDAkagRWouYVhnWtSWGdTVtQsd5o7pi+Y4LtaqbJegLkpqdw8m+seWGiOxem2Zu2PHyUCQtvrfOr2VKK0y7Fk3wQEgbyRXP/b1ctd6/cHv01uQw09bvevTOtgCA2cOq1+dq713dMVgq2Bgyc6hu68zI7n5YMaWfSedp6D58MBhHokZYuxpkJoYbImoU2rVoAi83w3PUWMKC0XdY5Dxvj++BiO6++PHJUABA37bNcPKNCLxxfw+DxzlpPCbzdHXCG/f1wJbn7xLDESA/P46h3cPv8EXCguFa25Y/FoLhd/gaPOd/+mq3Tm2cNUhPyao6GD+Uvlsr3U7db9xfu75H/729cCrZJoYbIiILCQtqgSbOlnna7+vhgq8e66e1DpabSv7cTkoH/Dl7MNY/MxDuLk5wcFDojPSq7YzNLT1cxFFid0gECykLx3RDd//qssEBXloTHdYkNa+PPl5uTmjlqT1UfnJYIJ4bblxfoS6+8pMdejc1fs4gsj6GGyKiWvrowWB0bNkU0f9pGGs59Wzjib5t63ZU0+/PDMLKqf2w5qk7DZZLWnwv9s67B82bqLByan+0dHfGc/dUPiLTbEGa0C8Avdp44ufpd2LuyC4G+yHNqRFa/q+XP9S1SGzGTLj40ojOcFIa35pE1sUOxUREtfRASBubmgNG9rGUXAFUthDd01X3UdTap+7EhK/3ie+93FTwuj0foq+HC/YvGF59fo088t5/e4k/h3VogaS0XPG9j7szrhaUSNb/p+mhuLN9C3zy71nZOktROijwysgumPrdAXHbcImO1ApF1X2x0EJlVKfYckNERFqay6yfZUhoUAt8N7U/AODNcbr9g4wJToB2R+fYF+/WW25gB9OWupDSL1B7aYklD/WWLGfKVR6707SO32RZDDdERI2AoPHY5p6uLeHv6YIxPbVnNF7+aF8M7eKD+aO61upaw7q2xLm3R8l+wRtqA2nv3QTzR3XFB//tBQ8XJ3yg0bIj5bOJfaBSOkgGKk01c5AgCDqhRXNG6P+GtKm8V738ZVu8NEnVY0Q3w52uyXL4WIqIqJFxUymx+5V7dFo8RvZohZE9LLOEg1Mt1rmq8rSeSQGVEinjzqAWOPXmSK1h9ZNC2+Kz2HNa5RwdHFBaoRbfyz1k+vDBYKjVAhwcFCaN4JLi66G7Plh9emJwe2w5kYnLN4qsWo/6wJYbIqJGoOaXeG0f5ViCYEInYM3HWZPDAtGuhZvOnDs15wvy9XCBq5OydpVE9b2qmak6+5q2kKig8V9hsMYouPqy6P+64R4TJ2a0VWaFm6VLlyIwMBAuLi4IDQ1FQkKC3rJDhw6FQqHQeY0ZM0YsM3XqVJ39I0eONKdqREQkQTNHGNvvpaHydHPCjpeH4ZWR8o/Pao5wqjnJoiDId7Cuollsclg7rLzdt8gcP9yevwioHBY/oV+A0ceuf2ag5PY3jZjbR/MzfPRgMB60oY7wpjA53KxduxaRkZGIiorCoUOHEBwcjIiICGRnZ0uWX79+PTIyMsTX8ePHoVQq8eCDD2qVGzlypFa5n3/+2bxPREREBjWUaOOqqn2riqk+fbiPbIuPPpqh8I37e6BNMzet/aZ2xHa/PSfSt5P74eWRXYw+rquf9Lw8g01cXqKbvwccbDzo6mNyuFmyZAmmT5+OadOmoVu3bli+fDnc3NywcuVKyfLNmzeHn5+f+Nq6dSvc3Nx0wo2zs7NWuWbNGv7Ks0REtkJogEOYI+/tgu7+HrKdgAGY/XjJpcZxPu7OOi0+zo7GnVsuBnz2cB8AxoecAwvDkbgwHD7uzmjRRIW7O/tgYIcWOp2e3zVy/qTWXq5wdFDAw0V/d9r/hsi3EAU0d5Ut09CZ1KG4tLQUiYmJmD9/vrjNwcEB4eHhiI+PN+ocK1aswMMPP4wmTbRnn4yLi0PLli3RrFkz3HPPPXjrrbfQooX0qrAlJSUoKame8yA/P9+Uj0FE1Kg1lD/Wfdydsem5IUaVjejui/A7fNG3nZdJ1/hmcj88+/NhLBhduxFgAGTTzeBO3tg77x40k1jmY+OsQVh3ME1rm4uTUgxfCoUCqx+vXLW8oLgMDy6Px8gefnj6rg5wVSkxb/0x2eqpHB1w/PUIKBRAl4UxkmV6tvEUf27l6SL5u9C9lSfSrtt2p2OTwk1OTg4qKirg66s9nM3X1xenT5+WPT4hIQHHjx/HihUrtLaPHDkS//nPf9C+fXtcuHABCxYswKhRoxAfHw+lUjdRR0dH4/XXXzel6kREZMMclQ741ozFOYMDvLBz7jCL1GHaoPb4LPacwU65NRc8BYBRPfwQHOClE270cXdxQszzd0nuqxlGFArt/lQ1W6qkxL54N4rLKvSutTZrWEfEnMjU2ubooDBpSQxrq9eh4CtWrEDPnj0xYMAAre0PP/yw+HPPnj3Rq1cvdOjQAXFxcRg+fHjN02D+/PmIjIwU3+fn5yMgwPjOWEREjY1Wh+IG0+vG+pqasBbYnOGdMLijN3pptH4Yo7ZreRk63xOD2uPb3SkmHd/Bx/AorybOugHp+OsRFhneX19Mqqm3tzeUSiWysrK0tmdlZcHPz8/gsYWFhVizZg2eeOIJ2esEBQXB29sb58+fl9zv7OwMDw8PrRcRERmnoTyWsqbvnxiAzr5N8f0TlX9sP9y/8g/k8Dv0t8ooHRQY0L65Ua0jdcXS/+mM+V349OHecHFSGt3xuiEwKdyoVCqEhIQgNjZW3KZWqxEbG4uwsDCDx/7yyy8oKSnBo48+Knudy5cv49q1a2jVyjKTSREREWka0skH/7xwN/rcXmD0tfu6Y8WUfvhsYh8r18yw+n40tHJqP9zfW/8ipg2VyW1MkZGR+Oabb7B69WqcOnUKM2fORGFhIaZNmwYAmDx5slaH4yorVqzAuHHjdDoJ37x5Ey+//DL27duHixcvIjY2Fvfffz86duyIiIgIMz8WERFpsvSjEXvj4qTE8Dt84aaqu94aUv1xjPXXs4MxpmcrrJhi/tw60nRbYzSHvNvqI0yT/ytOmDABV69exeLFi5GZmYnevXsjJiZG7GScmpoKBwftzHTmzBns3r0b//zzj875lEoljh49itWrVyM3Nxf+/v4YMWIE3nzzTTg7O5v5sYiIiKzLxckBxWVq3N2lcv6ZJwa3x+UbRWatMdWjtSeWTuoLACgsKZct/9GDwdh17iqS0nJx8dotk68nss1sY16H4tmzZ2P27NmS++Li4nS2denSRe80266urtiyZYs51SAiImqwdrw8DElpuQi/ozLMuDgpEW3knDWGaPZ9cdPTIfqBkDZ4IKQN7l2yo1bXstFsw7WliIgaA5Vj9T/3Pu5sFa8Pvh4uiOjuZ/GOuC5OSnz6cG988N9e8HJ1Mlh2ysBAAPrXspLrUGyrS3VwVXAiokZA6aDAkcUjUCEIVh3tQ5ZR1cl3pcww8EmhbdGnrRc6tjR+kU/bjDPa2HJDRNRIeLo5mbz+Edk2hUKB7v6eepeYGHc7JLVpJt3Z2VaDDsMNERGRmbybVj7iM2UywIZkQPvm2P7SUGyeI70MhqMNzW2jieGGiIjITD9PD8XI7n74ZYbhud4asvbeTeDsqB0Hpg4MxJBO3ggNkl7jsaGzzahJRETUAHTydcfyx0Ksdn1nJ8u3USgUlZMa2jKGGyIiIhv1QN822HA4HXd18rF2VRoUhhsiIiIb5eKkxC8zBlq7Gg0O+9wQERE1cva2PAfDDREREYlsdT0pTQw3REREZFcYboiIiMiuMNwQERGRXWG4ISIiIpGNrpWpheGGiIiI7ArDDREREdkVhhsiIqJGTqmxQKa7i+3P72v7n4CIiIhqxUnpgO+fGICyCjW83FTWrk6tMdwQERERhtjR+lR8LEVERER2heGGiIiI7ArDDREREdkVhhsiIiKyKww3REREZFcYboiIiMiuMNwQERGRXWG4ISIiIrvCcENERER2heGGiIiI7ArDDREREdkVhhsiIiKyKww3REREZFfsYlVwQRAAAPn5+VauCRERERmr6nu76nvcUuwi3BQUFAAAAgICrFwTIiIiMlVBQQE8PT0tdj6FYOm4ZAVqtRpXrlyBu7s7FAqFRc+dn5+PgIAApKWlwcPDw6Lntle8Z6bjPTMd75npeM/Mw/tmOmPvmSAIKCgogL+/PxwcLNdTxi5abhwcHNCmTZs6vYaHhwd/qU3Ee2Y63jPT8Z6ZjvfMPLxvpjPmnlmyxaYKOxQTERGRXWG4ISIiIrvCcCPD2dkZUVFRcHZ2tnZVbAbvmel4z0zHe2Y63jPz8L6Zztr3zC46FBMRERFVYcsNERER2RWGGyIiIrIrDDdERERkVxhuiIiIyK4w3MhYunQpAgMD4eLigtDQUCQkJFi7SnVi586dGDt2LPz9/aFQKLBhwwat/YIgYPHixWjVqhVcXV0RHh6Oc+fOaZW5fv06Jk2aBA8PD3h5eeGJJ57AzZs3tcocPXoUQ4YMgYuLCwICAvD+++/r1OWXX35B165d4eLigp49e2Lz5s0W/7y1FR0djf79+8Pd3R0tW7bEuHHjcObMGa0yxcXFmDVrFlq0aIGmTZvigQceQFZWllaZ1NRUjBkzBm5ubmjZsiVefvlllJeXa5WJi4tD37594ezsjI4dO2LVqlU69bGV39Nly5ahV69e4sReYWFh+Pvvv8X9vGeGvfvuu1AoFHj++efFbbxnul577TUoFAqtV9euXcX9vGfS0tPT8eijj6JFixZwdXVFz549cfDgQXG/TX0PCKTXmjVrBJVKJaxcuVI4ceKEMH36dMHLy0vIysqydtUsbvPmzcKrr74qrF+/XgAg/P7771r73333XcHT01PYsGGDcOTIEeG+++4T2rdvLxQVFYllRo4cKQQHBwv79u0Tdu3aJXTs2FGYOHGiuD8vL0/w9fUVJk2aJBw/flz4+eefBVdXV+Grr74Sy+zZs0dQKpXC+++/L5w8eVJYuHCh4OTkJBw7dqzO74EpIiIihO+++044fvy4kJSUJIwePVpo27atcPPmTbHMjBkzhICAACE2NlY4ePCgcOeddwoDBw4U95eXlws9evQQwsPDhcOHDwubN28WvL29hfnz54tlkpOTBTc3NyEyMlI4efKk8PnnnwtKpVKIiYkRy9jS7+kff/whbNq0STh79qxw5swZYcGCBYKTk5Nw/PhxQRB4zwxJSEgQAgMDhV69eglz5swRt/Oe6YqKihK6d+8uZGRkiK+rV6+K+3nPdF2/fl1o166dMHXqVGH//v1CcnKysGXLFuH8+fNiGVv6HmC4MWDAgAHCrFmzxPcVFRWCv7+/EB0dbcVa1b2a4UatVgt+fn7CBx98IG7Lzc0VnJ2dhZ9//lkQBEE4efKkAEA4cOCAWObvv/8WFAqFkJ6eLgiCIHz55ZdCs2bNhJKSErHMK6+8InTp0kV8/9BDDwljxozRqk9oaKjw9NNPW/QzWlp2drYAQNixY4cgCJX3x8nJSfjll1/EMqdOnRIACPHx8YIgVAZKBwcHITMzUyyzbNkywcPDQ7xHc+fOFbp37651rQkTJggRERHie1v/PW3WrJnw7bff8p4ZUFBQIHTq1EnYunWrcPfdd4vhhvdMWlRUlBAcHCy5j/dM2iuvvCIMHjxY735b+x7gYyk9SktLkZiYiPDwcHGbg4MDwsPDER8fb8Wa1b+UlBRkZmZq3QtPT0+EhoaK9yI+Ph5eXl7o16+fWCY8PBwODg7Yv3+/WOauu+6CSqUSy0RERODMmTO4ceOGWEbzOlVlGvo9z8vLAwA0b94cAJCYmIiysjKtz9K1a1e0bdtW65717NkTvr6+YpmIiAjk5+fjxIkTYhlD98OWf08rKiqwZs0aFBYWIiwsjPfMgFmzZmHMmDE6n4v3TL9z587B398fQUFBmDRpElJTUwHwnunzxx9/oF+/fnjwwQfRsmVL9OnTB998842439a+Bxhu9MjJyUFFRYXWLzcA+Pr6IjMz00q1so6qz2voXmRmZqJly5Za+x0dHdG8eXOtMlLn0LyGvjIN+Z6r1Wo8//zzGDRoEHr06AGg8nOoVCp4eXlpla15z8y9H/n5+SgqKrLJ39Njx46hadOmcHZ2xowZM/D777+jW7duvGd6rFmzBocOHUJ0dLTOPt4zaaGhoVi1ahViYmKwbNkypKSkYMiQISgoKOA90yM5ORnLli1Dp06dsGXLFsycORPPPfccVq9eDcD2vgfsYlVwImuaNWsWjh8/jt27d1u7KjahS5cuSEpKQl5eHn799VdMmTIFO3bssHa1GqS0tDTMmTMHW7duhYuLi7WrYzNGjRol/tyrVy+EhoaiXbt2WLduHVxdXa1Ys4ZLrVajX79+eOeddwAAffr0wfHjx7F8+XJMmTLFyrUzHVtu9PD29oZSqdTpQZ+VlQU/Pz8r1co6qj6voXvh5+eH7Oxsrf3l5eW4fv26Vhmpc2heQ1+ZhnrPZ8+ejb/++gvbt29HmzZtxO1+fn4oLS1Fbm6uVvma98zc++Hh4QFXV1eb/D1VqVTo2LEjQkJCEB0djeDgYHz66ae8ZxISExORnZ2Nvn37wtHREY6OjtixYwc+++wzODo6wtfXl/fMCF5eXujcuTPOnz/P3zM9WrVqhW7dumltu+OOO8THebb2PcBwo4dKpUJISAhiY2PFbWq1GrGxsQgLC7Nizepf+/bt4efnp3Uv8vPzsX//fvFehIWFITc3F4mJiWKZbdu2Qa1WIzQ0VCyzc+dOlJWViWW2bt2KLl26oFmzZmIZzetUlWlo91wQBMyePRu///47tm3bhvbt22vtDwkJgZOTk9ZnOXPmDFJTU7Xu2bFjx7T+Mdi6dSs8PDzEf2Tk7oc9/J6q1WqUlJTwnkkYPnw4jh07hqSkJPHVr18/TJo0SfyZ90zezZs3ceHCBbRq1Yq/Z3oMGjRIZzqLs2fPol27dgBs8HvA6K7HjdCaNWsEZ2dnYdWqVcLJkyeFp556SvDy8tLqQW8vCgoKhMOHDwuHDx8WAAhLliwRDh8+LFy6dEkQhMohgF5eXsLGjRuFo0ePCvfff7/kEMA+ffoI+/fvF3bv3i106tRJawhgbm6u4OvrKzz22GPC8ePHhTVr1ghubm46QwAdHR2FDz/8UDh16pQQFRXVIIeCz5w5U/D09BTi4uK0hpveunVLLDNjxgyhbdu2wrZt24SDBw8KYWFhQlhYmLi/arjpiBEjhKSkJCEmJkbw8fGRHG768ssvC6dOnRKWLl0qOdzUVn5P582bJ+zYsUNISUkRjh49KsybN09QKBTCP//8IwgC75kxNEdLCQLvmZQXX3xRiIuLE1JSUoQ9e/YI4eHhgre3t5CdnS0IAu+ZlISEBMHR0VF4++23hXPnzgk//vij4ObmJvzwww9iGVv6HmC4kfH5558Lbdu2FVQqlTBgwABh37591q5Sndi+fbsAQOc1ZcoUQRAqhwEuWrRI8PX1FZydnYXhw4cLZ86c0TrHtWvXhIkTJwpNmzYVPDw8hGnTpgkFBQVaZY4cOSIMHjxYcHZ2Flq3bi28++67OnVZt26d0LlzZ0GlUgndu3cXNm3aVGef21xS9wqA8N1334llioqKhGeeeUZo1qyZ4ObmJowfP17IyMjQOs/FixeFUaNGCa6uroK3t7fw4osvCmVlZVpltm/fLvTu3VtQqVRCUFCQ1jWq2Mrv6eOPPy60a9dOUKlUgo+PjzB8+HAx2AgC75kxaoYb3jNdEyZMEFq1aiWoVCqhdevWwoQJE7Tma+E9k/bnn38KPXr0EJydnYWuXbsKX3/9tdZ+W/oeUAiCIBjfzkNERETUsLHPDREREdkVhhsiIiKyKww3REREZFcYboiIiMiuMNwQERGRXWG4ISIiIrvCcENERER2heGGiIiI7ArDDREREdkVhhsiIiKyKww3REREZFcYboiIiMiu/D86zS5zev7rMAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fY3YqGX2TMmv"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample the model"
      ],
      "metadata": {
        "id": "7kTWwn5Q8x4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_gpu = torch.Generator(device=device).manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "# 0, which is new line char, is a reasonable start (seed) char\n",
        "idx = torch.tensor([[0]]).to(device)\n",
        "new_idx = net.generate(idx, BLOCK_SIZE)\n",
        "\n",
        "print(decode(new_idx.view(-1).tolist()))"
      ],
      "metadata": {
        "id": "CGiFf5b280CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d7bbd0-a7eb-4dad-e70d-8858d24f5d7c"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! With head of CNN Collegeer explains to Workers, are clear words beyond no music group, Others gas, horror deline Name’s newitory and possibly accelerating, he until the first<|endoftext|>Ch elector charge, loss of the real time, H8, Ladding, began to continue back to remove his voice or happy to anyone who Root but then plan that could actually take on the approach against her husband for her big very easy to his party since avoiding a overtime on his new\n"
          ]
        }
      ]
    }
  ]
}